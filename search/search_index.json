{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started The examples in this document use various IBM Cloud SDKs to interact with the various cloud services. You can install the modules as needed for each example, or you can install them all at once using pip : pip install --upgrade ibm-platform-services pip install --upgrade ibm-cloud-networking-services pip install --upgrade ibm-cloud-databases pip install --upgrade ibm-cloud-sdk-core pip install --upgrade ibm-schematics pip install --upgrade ibm-vpc Authentication IBM Cloud services support a variety of authentication options but for these guides focus on the token-based Identity and Access Management ( IAM ) authentication. With IAM authentication, you supply an API key and the authenticator will exchange that API key for an access token (a bearer token) by interacting with the IAM token service. The access token is then added (via the Authorization header) to each outbound request to provide the required authentication information. Access tokens are valid only for a limited amount of time and must be periodically refreshed. The IAM authenticator will automatically detect the need to refresh the access token and will interact with the IAM token service as needed to obtain a fresh access token, relieving the SDK user from that burden. For instance if we were interacting with the IAM Access Group service our authentication and client set up would look like this: from ibm_cloud_sdk_core.authenticators import IAMAuthenticator authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) accessGroupService = IamAccessGroupsV2 ( authenticator = authenticator ) For more details about the construction of the IAM authentication flow see IBM Cloud SDK Common documentation.","title":"Getting Started"},{"location":"#getting-started","text":"The examples in this document use various IBM Cloud SDKs to interact with the various cloud services. You can install the modules as needed for each example, or you can install them all at once using pip : pip install --upgrade ibm-platform-services pip install --upgrade ibm-cloud-networking-services pip install --upgrade ibm-cloud-databases pip install --upgrade ibm-cloud-sdk-core pip install --upgrade ibm-schematics pip install --upgrade ibm-vpc","title":"Getting Started"},{"location":"#authentication","text":"IBM Cloud services support a variety of authentication options but for these guides focus on the token-based Identity and Access Management ( IAM ) authentication. With IAM authentication, you supply an API key and the authenticator will exchange that API key for an access token (a bearer token) by interacting with the IAM token service. The access token is then added (via the Authorization header) to each outbound request to provide the required authentication information. Access tokens are valid only for a limited amount of time and must be periodically refreshed. The IAM authenticator will automatically detect the need to refresh the access token and will interact with the IAM token service as needed to obtain a fresh access token, relieving the SDK user from that burden. For instance if we were interacting with the IAM Access Group service our authentication and client set up would look like this: from ibm_cloud_sdk_core.authenticators import IAMAuthenticator authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) accessGroupService = IamAccessGroupsV2 ( authenticator = authenticator ) For more details about the construction of the IAM authentication flow see IBM Cloud SDK Common documentation.","title":"Authentication"},{"location":"code-engine/","text":"layout: default title: IBM Code Engine nav_order: 4 has_children: true","title":"Index"},{"location":"code-engine/container-image-build-ce/","text":"Overview This guide will show you how to use the experimental IBM Code Engine to build a container image from a Source control repository. Behind the scenes Code Engine will use Tekton pipelines to pull our source code from a Github repository and then create a container image using the supplied Docker file. After the build is complete Code Engine will push the new container image in to the IBM Cloud Container Registry . NOTE : Code Engine is currently an experimental offering and all resources are deleted every 7 days. Start a session in IBM Cloud Shell In the IBM Cloud console, click the IBM Cloud Shell icon . A session starts and automatically logs you in through the IBM Cloud CLI. Target Resource Group In oder to interact with the Code Engine CLI we first need to target the Resource Group where the Code Engine project will be created: $ ibmcloud target -g <Your Resource Group> Create a Code Engine Project The first step is to create a Code Engine project. Keep in mind during the Beta phase you are limited to one Code Engine project per region. If you already have a Code Engine project you can simply target that project using the command ibmcloud ce project target -n <name of project> We'll specify the --target option to automatically have the Code Engine cli target our new project: $ ibmcloud ce project create -n <Project Name> --target The project creation can take a few minutes, but when it completes you should see something like this: $ ibmcloud ce project create -n ce-demo-project --target Creating project 'ce-demo-project' ... Waiting for project 'ce-demo-project' to be in ready state... Now selecting project 'ce-demo-project' . OK Create an API Key for Code Engine for Registry Access As part of our Build process we are going be pulling a public Github repo but then pushing the built container in to IBM Cloud Container Registry . In order for our Code Engine project to be able to push to the registry we'll need to create an API key. $ ibmcloud iam api-key-create <Project Name>-cliapikey -d \"API Key for talking to Image registry from Code Engine\" --file key_file Create Code Engine Registry Secret With our API key created, we will now add the IBM Cloud Container Registry to Code Engine. When using the IBM Container Registry the username will always be iamapikey . If you would like to push to an alternate IBM Container Registry endpoint update the --server flag accordingly. $ export CR_API_KEY = ` jq -r '.apikey' < key_file ` $ ibmcloud ce registry create --name ibmcr --server us.icr.io --username iamapikey --password \" ${ CR_API_KEY } \" You can view all of your registry secrets by running the command: ibmcloud ce registry list . $ ibmcloud ce registry list Project 'demo-rt' and all its contents will be automatically deleted 7 days from now. Listing image registry access secrets... OK Name Age ibmcr 11s Create Code Engine Build Definition With the Registry access added we can now create our Build definition. If you do not already have a Container namespace to push images to, please follow [this guide to create one. $ ibmcloud ce build create --name go-app-example-build --source https://github.com/greyhoundforty/ce-build-example-go --strategy kaniko --size medium --image us.icr.io/<namespace>/go-app-example --registry-secret ibmcr The breakdown of the command: - name: The name of the build definition - source: The Source control repository where our code lives - strategy: The build strategy we will use to build the image. In this case since our repository has a Dockerfile we will use kaniko - size: The size of the build defines how CPU cores, memory, and disk space are assigned to the build - image: The Container Registry namespace and image name to push our built container image - registry-secret: The Container Registry secret that allows Code Engine to push and pull images Submit the Build Job Before the Build run is submitted (the actual process of building the container image), we\u2019ll want to target the underlying Kubernetes cluster that powers Code Engine. This will allow us to see the pods that are spun up for the build as well as track it\u2019s progress. To have kubctl within Cloud Shell target our cluster run the following command: ibmcloud ce project target -n <Name of Project> -k You should see output similar to this: $ ibmcloud ce project target -n demo-rt -k Selecting project 'demo-rt' ... Added context for 'demo-rt' to the current kubeconfig file. OK With kubectl properly configured we can now launch the actual build of our container image using the buildrun command. We specify the build definition we created previously with the --build flag: $ ibmcloud ce buildrun submit --name go-app-buildrun-v1 --build go-app-example-build You can check the status of the build run using the command ibmcloud ce buildrun get --name <Name of build run> $ ibmcloud ce buildrun get --name go-app-buildrun-v1 Project 'demo-rt' and all its contents will be automatically deleted 7 days from now. Getting build run 'go-app-buildrun-v1' ... OK Name: go-app-buildrun-v1 ID: d378e865-ecf4-4e26-932d-acb437eef0ef Project Name: demo-rt Project ID: ab07a001-9a77-4fd8-82e8-d4f8395ad735 Age: 36s Created: 2020 -09-23 09 :13:33 -0500 CDT Status: Reason: Running Registered: Unknown Instances: Name Running Status Restarts Age go-app-buildrun-v1-xpqfq-pod-hqchd 2 /4 Running 0 34s You can also check on the status of the Kubernetes pods by running kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE go-app-buildrun-v1-xpqfq-pod-hqchd 2 /4 Running 0 41s If the build completes successfully the pods will show Completed and the build run will show Succeeded $ kubectl get pods NAME READY STATUS RESTARTS AGE go-app-buildrun-v1-xpqfq-pod-hqchd 0 /4 Completed 0 4m10s $ ibmcloud ce buildrun get --name go-app-buildrun-v1 Project 'demo-rt' and all its contents will be automatically deleted 7 days from now. Getting build run 'go-app-buildrun-v1' ... OK Name: go-app-buildrun-v1 ID: d378e865-ecf4-4e26-932d-acb437eef0ef Project Name: demo-rt Project ID: ab07a001-9a77-4fd8-82e8-d4f8395ad735 Age: 4m26s Created: 2020 -09-23 09 :13:33 -0500 CDT Status: Reason: Succeeded Registered: True Instances: Name Running Status Restarts Age go-app-buildrun-v1-xpqfq-pod-hqchd 0 /4 Succeeded 0 4m24s","title":"Build a container image from source control using Code Engine"},{"location":"code-engine/container-image-build-ce/#overview","text":"This guide will show you how to use the experimental IBM Code Engine to build a container image from a Source control repository. Behind the scenes Code Engine will use Tekton pipelines to pull our source code from a Github repository and then create a container image using the supplied Docker file. After the build is complete Code Engine will push the new container image in to the IBM Cloud Container Registry . NOTE : Code Engine is currently an experimental offering and all resources are deleted every 7 days.","title":"Overview"},{"location":"code-engine/container-image-build-ce/#start-a-session-in-ibm-cloud-shell","text":"In the IBM Cloud console, click the IBM Cloud Shell icon . A session starts and automatically logs you in through the IBM Cloud CLI.","title":"Start a session in IBM Cloud Shell"},{"location":"code-engine/container-image-build-ce/#target-resource-group","text":"In oder to interact with the Code Engine CLI we first need to target the Resource Group where the Code Engine project will be created: $ ibmcloud target -g <Your Resource Group>","title":"Target Resource Group"},{"location":"code-engine/container-image-build-ce/#create-a-code-engine-project","text":"The first step is to create a Code Engine project. Keep in mind during the Beta phase you are limited to one Code Engine project per region. If you already have a Code Engine project you can simply target that project using the command ibmcloud ce project target -n <name of project> We'll specify the --target option to automatically have the Code Engine cli target our new project: $ ibmcloud ce project create -n <Project Name> --target The project creation can take a few minutes, but when it completes you should see something like this: $ ibmcloud ce project create -n ce-demo-project --target Creating project 'ce-demo-project' ... Waiting for project 'ce-demo-project' to be in ready state... Now selecting project 'ce-demo-project' . OK","title":"Create a Code Engine Project"},{"location":"code-engine/container-image-build-ce/#create-an-api-key-for-code-engine-for-registry-access","text":"As part of our Build process we are going be pulling a public Github repo but then pushing the built container in to IBM Cloud Container Registry . In order for our Code Engine project to be able to push to the registry we'll need to create an API key. $ ibmcloud iam api-key-create <Project Name>-cliapikey -d \"API Key for talking to Image registry from Code Engine\" --file key_file","title":"Create an API Key for Code Engine for Registry Access"},{"location":"code-engine/container-image-build-ce/#create-code-engine-registry-secret","text":"With our API key created, we will now add the IBM Cloud Container Registry to Code Engine. When using the IBM Container Registry the username will always be iamapikey . If you would like to push to an alternate IBM Container Registry endpoint update the --server flag accordingly. $ export CR_API_KEY = ` jq -r '.apikey' < key_file ` $ ibmcloud ce registry create --name ibmcr --server us.icr.io --username iamapikey --password \" ${ CR_API_KEY } \" You can view all of your registry secrets by running the command: ibmcloud ce registry list . $ ibmcloud ce registry list Project 'demo-rt' and all its contents will be automatically deleted 7 days from now. Listing image registry access secrets... OK Name Age ibmcr 11s","title":"Create Code Engine Registry Secret"},{"location":"code-engine/container-image-build-ce/#create-code-engine-build-definition","text":"With the Registry access added we can now create our Build definition. If you do not already have a Container namespace to push images to, please follow [this guide to create one. $ ibmcloud ce build create --name go-app-example-build --source https://github.com/greyhoundforty/ce-build-example-go --strategy kaniko --size medium --image us.icr.io/<namespace>/go-app-example --registry-secret ibmcr The breakdown of the command: - name: The name of the build definition - source: The Source control repository where our code lives - strategy: The build strategy we will use to build the image. In this case since our repository has a Dockerfile we will use kaniko - size: The size of the build defines how CPU cores, memory, and disk space are assigned to the build - image: The Container Registry namespace and image name to push our built container image - registry-secret: The Container Registry secret that allows Code Engine to push and pull images","title":"Create Code Engine Build Definition"},{"location":"code-engine/container-image-build-ce/#submit-the-build-job","text":"Before the Build run is submitted (the actual process of building the container image), we\u2019ll want to target the underlying Kubernetes cluster that powers Code Engine. This will allow us to see the pods that are spun up for the build as well as track it\u2019s progress. To have kubctl within Cloud Shell target our cluster run the following command: ibmcloud ce project target -n <Name of Project> -k You should see output similar to this: $ ibmcloud ce project target -n demo-rt -k Selecting project 'demo-rt' ... Added context for 'demo-rt' to the current kubeconfig file. OK With kubectl properly configured we can now launch the actual build of our container image using the buildrun command. We specify the build definition we created previously with the --build flag: $ ibmcloud ce buildrun submit --name go-app-buildrun-v1 --build go-app-example-build You can check the status of the build run using the command ibmcloud ce buildrun get --name <Name of build run> $ ibmcloud ce buildrun get --name go-app-buildrun-v1 Project 'demo-rt' and all its contents will be automatically deleted 7 days from now. Getting build run 'go-app-buildrun-v1' ... OK Name: go-app-buildrun-v1 ID: d378e865-ecf4-4e26-932d-acb437eef0ef Project Name: demo-rt Project ID: ab07a001-9a77-4fd8-82e8-d4f8395ad735 Age: 36s Created: 2020 -09-23 09 :13:33 -0500 CDT Status: Reason: Running Registered: Unknown Instances: Name Running Status Restarts Age go-app-buildrun-v1-xpqfq-pod-hqchd 2 /4 Running 0 34s You can also check on the status of the Kubernetes pods by running kubectl get pods $ kubectl get pods NAME READY STATUS RESTARTS AGE go-app-buildrun-v1-xpqfq-pod-hqchd 2 /4 Running 0 41s If the build completes successfully the pods will show Completed and the build run will show Succeeded $ kubectl get pods NAME READY STATUS RESTARTS AGE go-app-buildrun-v1-xpqfq-pod-hqchd 0 /4 Completed 0 4m10s $ ibmcloud ce buildrun get --name go-app-buildrun-v1 Project 'demo-rt' and all its contents will be automatically deleted 7 days from now. Getting build run 'go-app-buildrun-v1' ... OK Name: go-app-buildrun-v1 ID: d378e865-ecf4-4e26-932d-acb437eef0ef Project Name: demo-rt Project ID: ab07a001-9a77-4fd8-82e8-d4f8395ad735 Age: 4m26s Created: 2020 -09-23 09 :13:33 -0500 CDT Status: Reason: Succeeded Registered: True Instances: Name Running Status Restarts Age go-app-buildrun-v1-xpqfq-pod-hqchd 0 /4 Succeeded 0 4m24s","title":"Submit the Build Job"},{"location":"code-engine/cross-account-sync-ce/","text":"Overview Preparing Accounts Source Account Destination Account Create Code Engine Project (Optional) Create Docker container via Code Engine Deploy Sync Environment Overview In this guide I will show you how to sync IBM Cloud Object Storage buckets between accounts using Code Engine . Code Engine provides a platform to unify the deployment of all of your container-based applications on a Kubernetes-based infrastructure. The Code Engine experience is designed so that you can focus on writing code without the need for you to learn, or even know about, Kubernetes. Preparing Accounts We will be using Cloud Shell to generate Service IDs and Object Storage credentials for both the source and destination accounts. We will also be creating a Service ID on the both accounts. A service ID identifies a service or application similar to how a user ID identifies a user. We can assign specific access policies to the service ID that restrict permissions for using specific services: in this case it gets read-only access to an Object Storage bucket on the Source Account and write access an Object Storage bucket on the Destination Account. Source Account Launch a Cloud Shell session on the Source Account to begin generating our Object Storage access credentials. Create Service ID $ ibmcloud iam service-id-create SERVICE_ID_NAME --description \"Service ID for read-only access to bucket\" Inputs: SERVICE_ID_NAME : The name for the Service ID on the source account. Create Reader access policy for newly created Service ID: Now we will limit the scope of this service ID to have read only access to our source Object Storage bucket. $ ibmcloud iam service-policy-create SERVICE_ID --roles Reader --service-name cloud-object-storage \\ --service-instance ICOS_SERVICE_INSTANCE_ID --resource-type bucket --resource SOURCE_BUCKET_NAME Inputs: SERVICE_ID : The ID of the Service ID created in the previous step. ICOS_SERVICE_INSTANCE_ID : The GUID of the Cloud Object Storage instance on the source account. You can retrieve this with the command: ibmcloud resource service-instance <name of icos instance> SOURCE_BUCKET_NAME : The name of the source account bucket that we will sync with the destination bucket. Generate HMAC credentials tied to our service ID: In order for the Minio client to talk to each Object Storage instance it will need HMAC credentials (Access Key and Secret Key in S3 parlance). $ ibmcloud resource service-key-create SERVICE_ID_KEY_NAME Reader --instance-id ICOS_SERVICE_INSTANCE_ID \\ --service-id SERVICE_ID --parameters '{\"HMAC\":true}' Inputs: SERVICE_ID : The ID of the Service ID created in the previous step. ICOS_SERVICE_INSTANCE_ID : The GUID of the Cloud Object Storage instance on the source account. SERVICE_ID_KEY_NAME : The name of the Service ID credentials to create. Important Outputs: Take note of the output from the Serice Key output. These values will be used when creating our Code Engine Job. access_key_id will be used as the variable SOURCE_ACCESS_KEY secret_access_key will be used as the variable SOURCE_SECRET_KEY Destination Account Launch a Cloud Shell session on the Source Account to begin generating our Object Storage access credentials. Create Service ID $ ibmcloud iam service-id-create SERVICE_ID_NAME --description \"Service ID for write access to destination account bucket\" Inputs: SERVICE_ID_NAME : The name for the Service ID on the source account. Create Reader access policy for newly created service ID: Now we will add a Writer policy to our destination bucket bound to the Service ID. $ ibmcloud iam service-policy-create SERVICE_ID --roles Writer --service-name cloud-object-storage \\ --service-instance ICOS_SERVICE_INSTANCE_ID --resource-type bucket --resource DESTINATION_BUCKET_NAME Inputs: SERVICE_ID : The ID of the Service ID created in the previous step. ICOS_SERVICE_INSTANCE_ID : The GUID of the Cloud Object Storage instance on the source account. You can retrieve this with the command: ibmcloud resource service-instance <name of icos instance> DESTINATION_BUCKET_NAME : The name of the source account bucket that we will sync with the destination bucket. Generate HMAC credentials tied to our service ID: We'll follow the same procedure as last time to generate the HMAC credentials, but this time on the destination account. $ ibmcloud resource service-key-create SERVICE_ID_KEY_NAME Reader --instance-id ICOS_SERVICE_INSTANCE_ID \\ --service-id SERVICE_ID --parameters '{\"HMAC\":true}' Inputs: SERVICE_ID : The ID of the Service ID created in the previous step. ICOS_SERVICE_INSTANCE_ID : The GUID of the Cloud Object Storage instance on the source account. SERVICE_ID_KEY_NAME : Important Outputs: Take note of the output from the Serice Key output. These values will be used when creating our Code Engine Job. access_key_id will be used as the variable DESTINATION_ACCESS_KEY secret_access_key will be used as the variable DESTINATION_SECRET_KEY Create Code Engine Project Target Resource Group: On the account where you will deploy and run the Code Engine job to sync the buckets jump back in to Cloud Shell. In order to create our Code Engine project we need to make sure that our cloud shell session is targeting the correct resource group. $ ibmcloud target -g RESOURCE_GROUP Inputs: RESOURCE_GROUP : Name of the Resource Group to assign to Code Engine Project. Create Code Engine Project: With the correct Resource Group set, we can now create our Code Engine project. We add the --target flag to ensure that future Code Engine commands are targeting the correct project. $ ibmcloud ce project create -n PROJECT_NAME --target Inputs: PROJECT_NAME : Name of the Code Engine Project. (Optional) Create Docker container via Code Engine The default image used to sync the buckets is greyhoundforty/mcsync:latest . If you would like to build the container yourself and stick it in to IBM Cloud Container Registry fork this repository , update the Dockerfile if needed, and then use Code Engine to build the image as outlined below. Create Code Engine Repository Secret: In order to push our container image in to IBM Cloud Container Registry we need to first set up a Code Engine registry secret . ibmcloud ce registry create --name REGISTRY_SECRET_NAME --username iamapikey \\ --password IBMCLOUD_API_KEY --email YOUR_IBM_ACCOUNT_EMAIL --server ICR_ENDPOINT Inputs: REGISTRY_SECRET_NAME : The name of the Code Engine Registry Secret. IBMCLOUD_API_KEY : The IBM Cloud API Key for your account. YOUR_IBM_ACCOUNT_EMAIL : The email associated with your IBM Account. ICR_ENDPOINT : The IBM Container Registry Endpoint to use. See full list Create Container Build: ic ce build create --name BUILD_NAME --image us.icr.io/NAMESPACE/CONTAINER_NAME:1 --source FORKED_REPO_URL \\ --rs REGISTRY_SECRET_NAME --size small Inputs: BUILD_NAME : The name of the build job. NAMESPACE : The IBM Container Registry Namespace where the image will be stored. See this guide if you need to create a namespace. CONTAINER_NAME : The name of the container. FORKED_REPO_URL : The Github URL for the forked version of the sync container. REGISTRY_SECRET_NAME: The name of the Container Registry Secreate created in the previous step. Run container build: ibmcloud ce buildrun submit --build BUILD_NAME Inputs: BUILD_NAME : The name of the build job created in the previous step. Deploy Sync Environment Clone this repository: git clone https://github.com/cloud-design-dev/code-engine-minio-sync.git cd code-engine-minio-sync Copy variables.example to .env : cp variables.example .env Edit .env to match your environment: See inputs for available options. Once updated source the file for use in our session: source .env Create Code Engine Secret: ibmcloud ce secret create --name CODE_ENGINE_SECRET --from-literal SOURCE_ACCESS_KEY=\"${SOURCE_ACCESS_KEY}\" \\ --from-literal SOURCE_SECRET_KEY=\"${SOURCE_SECRET_KEY}\" --from-literal SOURCE_REGION=\"${SOURCE_REGION}\" \\ --from-literal SOURCE_BUCKET=\"${SOURCE_BUCKET}\" --from-literal DESTINATION_REGION=\"${DESTINATION_REGION}\" \\ --from-literal DESTINATION_ACCESS_KEY=\"${DESTINATION_ACCESS_KEY}\" --from-literal DESTINATION_SECRET_KEY=\"${DESTINATION_SECRET_KEY}\" \\ --from-literal DESTINATION_BUCKET=\"${DESTINATION_BUCKET}\" Inputs: CODE_ENGINE_SECRET : Name of the Code Engine Secret. All other variables are picked up from our .env file. Create Code Engine Job: If you created your own version of the container image as outlined above you will need to update the command and replace greyhoundforty/mcsync:latest with your image. ibmcloud ce job create --name JOB_NAME --image greyhoundforty/mcsync:latest --env-from-secret CODE_ENGINE_SECRET Inputs: JOB_NAME : The name of the Code Engine job. CODE_ENGINE_SECRET : Name of the Code Engine Secret. All other variables are picked up from our .env file. Submit Code Engine Job: ibmcloud ce jobrun submit --job JOB_NAME Inputs: JOB_NAME : The name of the Code Engine job. Check the status of the job: Depending on the size and number of objects that you are syncing the job could take a bit of time. You can check on the status of the job run by issuing the command: ibmcloud ce jobrun get --name JOB_NAME","title":"Cross account Object Storage bucket sync with Code Engine"},{"location":"code-engine/cross-account-sync-ce/#overview","text":"In this guide I will show you how to sync IBM Cloud Object Storage buckets between accounts using Code Engine . Code Engine provides a platform to unify the deployment of all of your container-based applications on a Kubernetes-based infrastructure. The Code Engine experience is designed so that you can focus on writing code without the need for you to learn, or even know about, Kubernetes.","title":"Overview"},{"location":"code-engine/cross-account-sync-ce/#preparing-accounts","text":"We will be using Cloud Shell to generate Service IDs and Object Storage credentials for both the source and destination accounts. We will also be creating a Service ID on the both accounts. A service ID identifies a service or application similar to how a user ID identifies a user. We can assign specific access policies to the service ID that restrict permissions for using specific services: in this case it gets read-only access to an Object Storage bucket on the Source Account and write access an Object Storage bucket on the Destination Account.","title":"Preparing Accounts"},{"location":"code-engine/cross-account-sync-ce/#source-account","text":"Launch a Cloud Shell session on the Source Account to begin generating our Object Storage access credentials. Create Service ID $ ibmcloud iam service-id-create SERVICE_ID_NAME --description \"Service ID for read-only access to bucket\" Inputs: SERVICE_ID_NAME : The name for the Service ID on the source account. Create Reader access policy for newly created Service ID: Now we will limit the scope of this service ID to have read only access to our source Object Storage bucket. $ ibmcloud iam service-policy-create SERVICE_ID --roles Reader --service-name cloud-object-storage \\ --service-instance ICOS_SERVICE_INSTANCE_ID --resource-type bucket --resource SOURCE_BUCKET_NAME Inputs: SERVICE_ID : The ID of the Service ID created in the previous step. ICOS_SERVICE_INSTANCE_ID : The GUID of the Cloud Object Storage instance on the source account. You can retrieve this with the command: ibmcloud resource service-instance <name of icos instance> SOURCE_BUCKET_NAME : The name of the source account bucket that we will sync with the destination bucket. Generate HMAC credentials tied to our service ID: In order for the Minio client to talk to each Object Storage instance it will need HMAC credentials (Access Key and Secret Key in S3 parlance). $ ibmcloud resource service-key-create SERVICE_ID_KEY_NAME Reader --instance-id ICOS_SERVICE_INSTANCE_ID \\ --service-id SERVICE_ID --parameters '{\"HMAC\":true}' Inputs: SERVICE_ID : The ID of the Service ID created in the previous step. ICOS_SERVICE_INSTANCE_ID : The GUID of the Cloud Object Storage instance on the source account. SERVICE_ID_KEY_NAME : The name of the Service ID credentials to create. Important Outputs: Take note of the output from the Serice Key output. These values will be used when creating our Code Engine Job. access_key_id will be used as the variable SOURCE_ACCESS_KEY secret_access_key will be used as the variable SOURCE_SECRET_KEY","title":"Source Account"},{"location":"code-engine/cross-account-sync-ce/#destination-account","text":"Launch a Cloud Shell session on the Source Account to begin generating our Object Storage access credentials. Create Service ID $ ibmcloud iam service-id-create SERVICE_ID_NAME --description \"Service ID for write access to destination account bucket\" Inputs: SERVICE_ID_NAME : The name for the Service ID on the source account. Create Reader access policy for newly created service ID: Now we will add a Writer policy to our destination bucket bound to the Service ID. $ ibmcloud iam service-policy-create SERVICE_ID --roles Writer --service-name cloud-object-storage \\ --service-instance ICOS_SERVICE_INSTANCE_ID --resource-type bucket --resource DESTINATION_BUCKET_NAME Inputs: SERVICE_ID : The ID of the Service ID created in the previous step. ICOS_SERVICE_INSTANCE_ID : The GUID of the Cloud Object Storage instance on the source account. You can retrieve this with the command: ibmcloud resource service-instance <name of icos instance> DESTINATION_BUCKET_NAME : The name of the source account bucket that we will sync with the destination bucket. Generate HMAC credentials tied to our service ID: We'll follow the same procedure as last time to generate the HMAC credentials, but this time on the destination account. $ ibmcloud resource service-key-create SERVICE_ID_KEY_NAME Reader --instance-id ICOS_SERVICE_INSTANCE_ID \\ --service-id SERVICE_ID --parameters '{\"HMAC\":true}' Inputs: SERVICE_ID : The ID of the Service ID created in the previous step. ICOS_SERVICE_INSTANCE_ID : The GUID of the Cloud Object Storage instance on the source account. SERVICE_ID_KEY_NAME : Important Outputs: Take note of the output from the Serice Key output. These values will be used when creating our Code Engine Job. access_key_id will be used as the variable DESTINATION_ACCESS_KEY secret_access_key will be used as the variable DESTINATION_SECRET_KEY","title":"Destination Account"},{"location":"code-engine/cross-account-sync-ce/#create-code-engine-project","text":"Target Resource Group: On the account where you will deploy and run the Code Engine job to sync the buckets jump back in to Cloud Shell. In order to create our Code Engine project we need to make sure that our cloud shell session is targeting the correct resource group. $ ibmcloud target -g RESOURCE_GROUP Inputs: RESOURCE_GROUP : Name of the Resource Group to assign to Code Engine Project. Create Code Engine Project: With the correct Resource Group set, we can now create our Code Engine project. We add the --target flag to ensure that future Code Engine commands are targeting the correct project. $ ibmcloud ce project create -n PROJECT_NAME --target Inputs: PROJECT_NAME : Name of the Code Engine Project.","title":"Create Code Engine Project"},{"location":"code-engine/cross-account-sync-ce/#optional-create-docker-container-via-code-engine","text":"The default image used to sync the buckets is greyhoundforty/mcsync:latest . If you would like to build the container yourself and stick it in to IBM Cloud Container Registry fork this repository , update the Dockerfile if needed, and then use Code Engine to build the image as outlined below. Create Code Engine Repository Secret: In order to push our container image in to IBM Cloud Container Registry we need to first set up a Code Engine registry secret . ibmcloud ce registry create --name REGISTRY_SECRET_NAME --username iamapikey \\ --password IBMCLOUD_API_KEY --email YOUR_IBM_ACCOUNT_EMAIL --server ICR_ENDPOINT Inputs: REGISTRY_SECRET_NAME : The name of the Code Engine Registry Secret. IBMCLOUD_API_KEY : The IBM Cloud API Key for your account. YOUR_IBM_ACCOUNT_EMAIL : The email associated with your IBM Account. ICR_ENDPOINT : The IBM Container Registry Endpoint to use. See full list Create Container Build: ic ce build create --name BUILD_NAME --image us.icr.io/NAMESPACE/CONTAINER_NAME:1 --source FORKED_REPO_URL \\ --rs REGISTRY_SECRET_NAME --size small Inputs: BUILD_NAME : The name of the build job. NAMESPACE : The IBM Container Registry Namespace where the image will be stored. See this guide if you need to create a namespace. CONTAINER_NAME : The name of the container. FORKED_REPO_URL : The Github URL for the forked version of the sync container. REGISTRY_SECRET_NAME: The name of the Container Registry Secreate created in the previous step. Run container build: ibmcloud ce buildrun submit --build BUILD_NAME Inputs: BUILD_NAME : The name of the build job created in the previous step.","title":"(Optional) Create Docker container via Code Engine"},{"location":"code-engine/cross-account-sync-ce/#deploy-sync-environment","text":"Clone this repository: git clone https://github.com/cloud-design-dev/code-engine-minio-sync.git cd code-engine-minio-sync Copy variables.example to .env : cp variables.example .env Edit .env to match your environment: See inputs for available options. Once updated source the file for use in our session: source .env Create Code Engine Secret: ibmcloud ce secret create --name CODE_ENGINE_SECRET --from-literal SOURCE_ACCESS_KEY=\"${SOURCE_ACCESS_KEY}\" \\ --from-literal SOURCE_SECRET_KEY=\"${SOURCE_SECRET_KEY}\" --from-literal SOURCE_REGION=\"${SOURCE_REGION}\" \\ --from-literal SOURCE_BUCKET=\"${SOURCE_BUCKET}\" --from-literal DESTINATION_REGION=\"${DESTINATION_REGION}\" \\ --from-literal DESTINATION_ACCESS_KEY=\"${DESTINATION_ACCESS_KEY}\" --from-literal DESTINATION_SECRET_KEY=\"${DESTINATION_SECRET_KEY}\" \\ --from-literal DESTINATION_BUCKET=\"${DESTINATION_BUCKET}\" Inputs: CODE_ENGINE_SECRET : Name of the Code Engine Secret. All other variables are picked up from our .env file. Create Code Engine Job: If you created your own version of the container image as outlined above you will need to update the command and replace greyhoundforty/mcsync:latest with your image. ibmcloud ce job create --name JOB_NAME --image greyhoundforty/mcsync:latest --env-from-secret CODE_ENGINE_SECRET Inputs: JOB_NAME : The name of the Code Engine job. CODE_ENGINE_SECRET : Name of the Code Engine Secret. All other variables are picked up from our .env file. Submit Code Engine Job: ibmcloud ce jobrun submit --job JOB_NAME Inputs: JOB_NAME : The name of the Code Engine job. Check the status of the job: Depending on the size and number of objects that you are syncing the job could take a bit of time. You can check on the status of the job run by issuing the command: ibmcloud ce jobrun get --name JOB_NAME","title":"Deploy Sync Environment"},{"location":"diagrams/","text":"","title":"IBM Cloud Diagrams"},{"location":"diagrams/basic-dl/","text":"Basic Direct Link Diagram Edit As New Basic Direct Link with Ipsec Tunnel Edit As New","title":"Basic Classic Direct Link Diagram"},{"location":"diagrams/basic-dl/#basic-direct-link-diagram","text":"Edit As New","title":"Basic Direct Link Diagram"},{"location":"diagrams/basic-dl/#basic-direct-link-with-ipsec-tunnel","text":"Edit As New","title":"Basic Direct Link with Ipsec Tunnel"},{"location":"diagrams/basic-vpc/","text":"Basic VPC Edit As New","title":"Basic VPC Diagram"},{"location":"diagrams/basic-vpc/#basic-vpc","text":"Edit As New","title":"Basic VPC"},{"location":"ibm-api/","text":"Prerequisites IBM Cloud API Key IAM Token generated from API Key jq installed (Not strictly required but useful for parsing API response) Export IBM Cloud API Key You can generate an API Key by navigating to the API Keys page in the IBM Cloud Portal and clicking Create an IBM Cloud API Key . Copy your newly created API Key and export it to your shell session. export IBMCLOUD_API_KEY = <Your IBM Cloud API Key> With the API key exported we now need to generate an IAM token in order to interact with the various APIs. The following command will generate a new IAM token from your API Key and store it as the variable iam_token . iam_token = ` curl -s -k -X POST -H \"Content-Type: application/x-www-form-urlencoded\" \\ -H \"Accept: application/json\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" \\ --data-urlencode \"apikey= ${ IBMCLOUD_API_KEY } \" \"https://iam.cloud.ibm.com/identity/token\" \\ | jq -r '(.token_type + \" \" + .access_token)' `","title":"IBM Cloud Rest API"},{"location":"ibm-api/#prerequisites","text":"IBM Cloud API Key IAM Token generated from API Key jq installed (Not strictly required but useful for parsing API response)","title":"Prerequisites"},{"location":"ibm-api/#export-ibm-cloud-api-key","text":"You can generate an API Key by navigating to the API Keys page in the IBM Cloud Portal and clicking Create an IBM Cloud API Key . Copy your newly created API Key and export it to your shell session. export IBMCLOUD_API_KEY = <Your IBM Cloud API Key> With the API key exported we now need to generate an IAM token in order to interact with the various APIs. The following command will generate a new IAM token from your API Key and store it as the variable iam_token . iam_token = ` curl -s -k -X POST -H \"Content-Type: application/x-www-form-urlencoded\" \\ -H \"Accept: application/json\" --data-urlencode \"grant_type=urn:ibm:params:oauth:grant-type:apikey\" \\ --data-urlencode \"apikey= ${ IBMCLOUD_API_KEY } \" \"https://iam.cloud.ibm.com/identity/token\" \\ | jq -r '(.token_type + \" \" + .access_token)' `","title":"Export IBM Cloud API Key"},{"location":"ibm-api/private-dns/","text":"Overview This page shows examples of interacting with the IBM Cloud Private DNS API. Table of Contents: - Overview * Prerequisites + Authentication * Create a Private DNS Instance * Create a Private DNS Zone Prerequisites You have a valid IAM Token as outlined here Authentication Access to DNS Services instances for users in your account is controlled by IBM Cloud Identity and Access Management (IAM). Every user that accesses the DNS Services in your account must be assigned an access policy with an IAM role defined. Pass a bearer token in an authorization header. Tokens support authenticated requests without embedding service credentials in every call. For more information, see Managing access with IAM for DNS Services . Create Private DNS Instance We will need to use the Resource Controller API in order to provision an instance of the Private DNS offering. RESOURCE_GROUP_ID: The ID of the resource group the VPC will be deployed in to. RESOURCE_PLAN_ID: The ID of the specific catatlog plan ID for the service. To find available options run ibmcloud catalog service dns-svcs . curl -X POST \"https://resource-controller.cloud.ibm.com/v2/resource_instances\" \\ -H \"Authorization: ${ iam_token } \" -H \"Content-Type: application/json\" \\ -d ' { \"name\" : \"pdns-rest-test\" , \"target\" : \"bluemix-global\" , \"resource_group\" : \"RESOURCE_GROUP\" , \"resource_plan_id\" : \"RESOURCE_PLAN_ID\" , \"tags\" : [ \"service:dns-svcs\" ] } Create a Private DNS Zone The following example will create a new DNS Zone in our Private DNS instance. DNS_INSTANCE: The Private Instance ID. You can get this by running the command ibmcloud dns instances curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: ${ iam_token } \" \\ \"https://api.dns-svcs.cloud.ibm.com/v1/instances/DNS_INSTANCE/dnszones\" \\ -d '{ \"name\": \"pdns-rest.test\", \"description\": \"Example Private DNS Zone\", \"label\": \"us-east\" }' Add a Permitted Network to a DNS zone DNS_INSTANCE: The Private Instance ID. You can get this by running the command ibmcloud dns instances . DNS_ZONE_ID: The DNS Zone ID. VPC_CRN: The CRN of the VPC to add as a permitted network. curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: ${ iam_token } \" \\ \"https://api.dns-svcs.cloud.ibm.com/v1/instances/DNS_INSTANCE/dnszones/DNS_ZONE_ID/permitted_networks\" \\ -d '{ \"permitted_network\": { \"vpc_crn\": \"VPC_CRN\" }, \"type\": \"vpc\" }' Links Private DNS API Docs Private DNS Cloud Docs","title":"Private DNS API"},{"location":"ibm-api/private-dns/#overview","text":"This page shows examples of interacting with the IBM Cloud Private DNS API. Table of Contents: - Overview * Prerequisites + Authentication * Create a Private DNS Instance * Create a Private DNS Zone","title":"Overview"},{"location":"ibm-api/private-dns/#prerequisites","text":"You have a valid IAM Token as outlined here","title":"Prerequisites"},{"location":"ibm-api/private-dns/#authentication","text":"Access to DNS Services instances for users in your account is controlled by IBM Cloud Identity and Access Management (IAM). Every user that accesses the DNS Services in your account must be assigned an access policy with an IAM role defined. Pass a bearer token in an authorization header. Tokens support authenticated requests without embedding service credentials in every call. For more information, see Managing access with IAM for DNS Services .","title":"Authentication"},{"location":"ibm-api/private-dns/#create-private-dns-instance","text":"We will need to use the Resource Controller API in order to provision an instance of the Private DNS offering. RESOURCE_GROUP_ID: The ID of the resource group the VPC will be deployed in to. RESOURCE_PLAN_ID: The ID of the specific catatlog plan ID for the service. To find available options run ibmcloud catalog service dns-svcs . curl -X POST \"https://resource-controller.cloud.ibm.com/v2/resource_instances\" \\ -H \"Authorization: ${ iam_token } \" -H \"Content-Type: application/json\" \\ -d ' { \"name\" : \"pdns-rest-test\" , \"target\" : \"bluemix-global\" , \"resource_group\" : \"RESOURCE_GROUP\" , \"resource_plan_id\" : \"RESOURCE_PLAN_ID\" , \"tags\" : [ \"service:dns-svcs\" ] }","title":"Create Private DNS Instance"},{"location":"ibm-api/private-dns/#create-a-private-dns-zone","text":"The following example will create a new DNS Zone in our Private DNS instance. DNS_INSTANCE: The Private Instance ID. You can get this by running the command ibmcloud dns instances curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: ${ iam_token } \" \\ \"https://api.dns-svcs.cloud.ibm.com/v1/instances/DNS_INSTANCE/dnszones\" \\ -d '{ \"name\": \"pdns-rest.test\", \"description\": \"Example Private DNS Zone\", \"label\": \"us-east\" }'","title":"Create a Private DNS Zone"},{"location":"ibm-api/private-dns/#add-a-permitted-network-to-a-dns-zone","text":"DNS_INSTANCE: The Private Instance ID. You can get this by running the command ibmcloud dns instances . DNS_ZONE_ID: The DNS Zone ID. VPC_CRN: The CRN of the VPC to add as a permitted network. curl -X POST -H \"Content-Type: application/json\" -H \"Authorization: ${ iam_token } \" \\ \"https://api.dns-svcs.cloud.ibm.com/v1/instances/DNS_INSTANCE/dnszones/DNS_ZONE_ID/permitted_networks\" \\ -d '{ \"permitted_network\": { \"vpc_crn\": \"VPC_CRN\" }, \"type\": \"vpc\" }'","title":"Add a Permitted Network to a DNS zone"},{"location":"ibm-api/private-dns/#links","text":"Private DNS API Docs Private DNS Cloud Docs","title":"Links"},{"location":"ibm-api/search-rest-api-examples/","text":"Overview This page shows examples of interacting with the IBM Cloud REST Search API . Prerequisites You have a valid IAM Token as outlined here List all supported resource types Retrieves a list of all the resource types supported by the Cloud and Classic infrastructure global catalog. curl -s -X GET https://api.global-search-tagging.cloud.ibm.com/v2/resources/supported_types \\ -H \"authorization: ${ iam_token } \" | jq -r { \"supported_types\" : [ \"resource-instance\" , \"resource-instance-provision-behind\" , \"resource-group\" , \"resource-binding\" , \"resource-alias\" , \"k8-cluster\" , \"k8-location\" , \"is-vpc\" , \"is-instance\" , \"is-key\" , \"is-volume\" , \"is-load-balancer\" , \"is-vpn\" , \"is-image\" , \"is-security-group\" , \"is-subnet\" , \"is-public-gateway\" , \"is-floating-ip\" , \"is-network-acl\" , \"is-endpoint-gateway\" , \"is-flow-log-collector\" , \"is-instance-group\" , \"is-dedicated-host\" , \"is-family\" , \"cf-user-provided-service-instance\" , \"cf-space\" , \"cf-service-instance\" , \"cf-service-binding\" , \"cf-organization\" , \"cf-application\" , \"vmware-solutions\" , \"ims-block-storage\" , \"ims-cloud-backup\" , \"ims-cloud-object-storage-infrastructure\" , \"ims-file-storage\" , \"ims-cdn-powered-by-akamai\" , \"ims-direct-link-cloud-connect\" , \"ims-direct-link-cloud-exchange\" , \"ims-direct-link-colocation\" , \"ims-direct-link-network-service-provider\" , \"ims-hardware-firewall\" , \"ims-hardware-firewall-dedicated\" , \"ims-fortigate-security-appliance-1gb\" , \"ims-fortigate-security-appliance-10gb\" , \"ims-network-gateway-juniper-vsrx\" , \"ims-network-gateway-byoa\" , \"ims-virtual-router-appliance-copy\" , \"ims-ibm-cloud-load-balancer\" , \"ims-bare-metal\" , \"ims-virtual-server\" , \"ims-dedicated-host\" , \"ims-image\" , \"activity-insights\" , \"network-insights\" , \"security-advisor\" ] } Searching IBM Cloud Classic Resources The following examples show how to search against Classic Infrastructure (IaaS, SoftLayer) resources on your IBM Cloud account. Find Classic IaaS virtual instances by tag: In order to search for resources on your account you need to supply your Account ID. From a terminal you can run the command ibmcloud account show to grab the Account ID. $ export ACCOUNT_ID = <Your IBM Account ID> In this example I am searching for all IaaS instances with the tag consul $ curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"content-type: application/json\" -H \"accept: application/json\" -H \\ \"Authorization: ${ iam_token } \" -d '{\"query\":\"(type:virtual-server AND family:ims) AND (tags:consul)\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"consul-server3.cdetesting.com\" , \"type\" : \"virtual-server\" , \"family\" : \"ims\" , \"crn\" : \"crn:v1:bluemix:public:virtual-server:wdc06:a/xxxxxxxxxxxxxxxxxxxxxx:90725760::\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"consul-server2.cdetesting.com\" , \"type\" : \"virtual-server\" , \"family\" : \"ims\" , \"crn\" : \"crn:v1:bluemix:public:virtual-server:wdc06:a/xxxxxxxxxxxxxxxxxxxxxx:90725768::\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"consul-server1.cdetesting.com\" , \"type\" : \"virtual-server\" , \"family\" : \"ims\" , \"crn\" : \"crn:v1:bluemix:public:virtual-server:wdc06:a/xxxxxxxxxxxxxxxxxxxxxx:90725770::\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxx\" } Find Block storage volumes with specific note The Classic File and Block volumes do not support tagging via the Cloud Portal. In their place their is a field to store notes about the volume. We will run a search for all block volumes with the note ryantiffany and just return their names. You can tag these volumes via the Tagging API. For an example of how to tag Block / File storage see the following example . $ curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?limit=50\" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"(type:block-storage AND service_name:block-storage) AND (doc.notes:ryantiffany)\" , \"fields\": [ \"name\", \"tags\" ]}' | jq -r Example Output { \"items\" : [ { \"name\" : \"SL02SEL78003-13\" , \"crn\" : \"crn:v1:bluemix:public:block-storage:wdc07:a/xxxxxxxxxxxxxxxxxxxxxx:53093855::\" , \"tags\" : [] } ] , \"limit\" : 50 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" } Find all VMware Solutions deployments The following call with return all the VMware Dedicated and VMware Shared deployments on the account. curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"type:vmware-solutions\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"vcs-rt\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/xxxxxxxxxxxxxxxxxxxxxx:55c17e7d-eaed-4b4f-xxxx-xxxxxxxx::\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"primary-rt\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/xxxxxxxxxxxxxxxxxxxxxx:707319d9-2f6b-4b48-xxxx-xxxxxxxx::\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"jb-test-1\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/xxxxxxxxxxxxxxxxxxxxxx:824912d4-10be-4421-xxxx-xxxxxxxx::\" } , { \"account_id\" : \"6c27214690345bfb75bb1f2b28a20504\" , \"name\" : \"centerpoint\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/6c27214690345bfb75bb1f2b28a20504:900d7d94-8236-4382-xxxx-xxxxxxxx:\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"vcd-rt\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/xxxxxxxxxxxxxxxxxxxxxx:9ac98b37-cd31-4800-xxxx-xxxxxxxx::\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" } Find all Juniper Network Gateway appliances in the Washington 7 Datacenter curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"(family:ims AND type:network-gateway-juniper-vsrx) AND (region:washington-7)\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"ha-srx\" , \"type\" : \"network-gateway-juniper-vsrx\" , \"family\" : \"ims\" , \"crn\" : \"crn:v1:bluemix:public:network-gateway-juniper-vsrx:washington-7:a/xxxxxxxxxxxxxxxxxxxxxx:538282::\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" } Cloud Resources Find all Kubernetes Clusters and just return their name and associated tags: Note : the CRN is always returned even if not part of the field return parameters. curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"content-type: application/json\" -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" \\ -d '{\"query\": \"type:k8-cluster\", \"fields\": [ \"name\", \"tags\", \"service_instance\" ]}' | jq -r Example Output { \"items\" : [ { \"name\" : \"devcluster\" , \"service_instance\" : \"bpdhxxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:bpdhxxxx::\" , \"tags\" : [ \"ryantiffany\" ] } , { \"name\" : \"mycluster-dal10-b3c.4x16\" , \"service_instance\" : \"bsdexxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:bsdexxxx::\" , \"tags\" : [ \"jack\" ] } , { \"name\" : \"aamir-testcluster-sjc04-b3c.4x16\" , \"service_instance\" : \"bslaxxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:bslaxxxx::\" , \"tags\" : [] } , { \"name\" : \"cluster2\" , \"service_instance\" : \"btckxxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:btckxxxx::\" , \"tags\" : [] } , { \"name\" : \"Russell_Cluster\" , \"service_instance\" : \"bthrxxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:bthrxxxx::\" , \"tags\" : [] } , { \"name\" : \"rt-us-south\" , \"service_instance\" : \"btr2xxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:btr2xxxx::\" , \"tags\" : [ \"region:us-south\" , \"ryantiffany\" ] } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" } Find all Kubernetes clusters in a given region In this example we are searching for all Kubernetes Clusters in the Tokyo region: $ curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"type:k8-cluster AND region:jp-tok\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"subredOpen\" , \"type\" : \"k8-cluster\" , \"family\" : \"containers\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:jp-tok:a/xxxxxxxxxxxxxxxxxxxxxx:blr1xxxxxx:\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" } Find all VPC instances in a specific region with specific tags Since VPC instances stretch across multiple zones in a region we use the wildcard search term region:us-east* in this case to show all instances within the US-East VPC region. $ curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"(family:is AND type:instance AND region:us-east*)\"}' Example output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"wgtest-z1-dev\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-1:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0757_2834694d-ad33-4b81-xxxx-xxxxxxxx\" }, { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"wgtest-wg-dev\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-1:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0757_ebdfe595-c96b-4924-xxxx-xxxxxxxx\" }, { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"vpntest\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-1:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0757_f89fa662-71ab-45e1-xxxx-xxxxxxxx\" }, { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"wgtest-z2-dev\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-2:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0767_65809d72-adca-4c5d-xxxx-xxxxxxxx\" }, { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"wgtest-z3-dev\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-3:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0777_e7321492-179b-4f26-xxxx-xxxxxxxx\" } ], \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" } Find all Database for Etcd instances with a specific tag curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"(type:resource-instance AND service_name:databases-for-etcd) AND (tags:ryantiffany)\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"rt-1978903720bf\" , \"type\" : \"resource-instance\" , \"family\" : \"resource_controller\" , \"crn\" : \"crn:v1:bluemix:public:databases-for-etcd:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:41625c25-f836-491e-xxx-xxxxxx::\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" }","title":"Search API Examples"},{"location":"ibm-api/search-rest-api-examples/#overview","text":"This page shows examples of interacting with the IBM Cloud REST Search API .","title":"Overview"},{"location":"ibm-api/search-rest-api-examples/#prerequisites","text":"You have a valid IAM Token as outlined here","title":"Prerequisites"},{"location":"ibm-api/search-rest-api-examples/#list-all-supported-resource-types","text":"Retrieves a list of all the resource types supported by the Cloud and Classic infrastructure global catalog. curl -s -X GET https://api.global-search-tagging.cloud.ibm.com/v2/resources/supported_types \\ -H \"authorization: ${ iam_token } \" | jq -r { \"supported_types\" : [ \"resource-instance\" , \"resource-instance-provision-behind\" , \"resource-group\" , \"resource-binding\" , \"resource-alias\" , \"k8-cluster\" , \"k8-location\" , \"is-vpc\" , \"is-instance\" , \"is-key\" , \"is-volume\" , \"is-load-balancer\" , \"is-vpn\" , \"is-image\" , \"is-security-group\" , \"is-subnet\" , \"is-public-gateway\" , \"is-floating-ip\" , \"is-network-acl\" , \"is-endpoint-gateway\" , \"is-flow-log-collector\" , \"is-instance-group\" , \"is-dedicated-host\" , \"is-family\" , \"cf-user-provided-service-instance\" , \"cf-space\" , \"cf-service-instance\" , \"cf-service-binding\" , \"cf-organization\" , \"cf-application\" , \"vmware-solutions\" , \"ims-block-storage\" , \"ims-cloud-backup\" , \"ims-cloud-object-storage-infrastructure\" , \"ims-file-storage\" , \"ims-cdn-powered-by-akamai\" , \"ims-direct-link-cloud-connect\" , \"ims-direct-link-cloud-exchange\" , \"ims-direct-link-colocation\" , \"ims-direct-link-network-service-provider\" , \"ims-hardware-firewall\" , \"ims-hardware-firewall-dedicated\" , \"ims-fortigate-security-appliance-1gb\" , \"ims-fortigate-security-appliance-10gb\" , \"ims-network-gateway-juniper-vsrx\" , \"ims-network-gateway-byoa\" , \"ims-virtual-router-appliance-copy\" , \"ims-ibm-cloud-load-balancer\" , \"ims-bare-metal\" , \"ims-virtual-server\" , \"ims-dedicated-host\" , \"ims-image\" , \"activity-insights\" , \"network-insights\" , \"security-advisor\" ] }","title":"List all supported resource types"},{"location":"ibm-api/search-rest-api-examples/#searching-ibm-cloud-classic-resources","text":"The following examples show how to search against Classic Infrastructure (IaaS, SoftLayer) resources on your IBM Cloud account.","title":"Searching IBM Cloud Classic Resources"},{"location":"ibm-api/search-rest-api-examples/#find-classic-iaas-virtual-instances-by-tag","text":"In order to search for resources on your account you need to supply your Account ID. From a terminal you can run the command ibmcloud account show to grab the Account ID. $ export ACCOUNT_ID = <Your IBM Account ID> In this example I am searching for all IaaS instances with the tag consul $ curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"content-type: application/json\" -H \"accept: application/json\" -H \\ \"Authorization: ${ iam_token } \" -d '{\"query\":\"(type:virtual-server AND family:ims) AND (tags:consul)\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"consul-server3.cdetesting.com\" , \"type\" : \"virtual-server\" , \"family\" : \"ims\" , \"crn\" : \"crn:v1:bluemix:public:virtual-server:wdc06:a/xxxxxxxxxxxxxxxxxxxxxx:90725760::\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"consul-server2.cdetesting.com\" , \"type\" : \"virtual-server\" , \"family\" : \"ims\" , \"crn\" : \"crn:v1:bluemix:public:virtual-server:wdc06:a/xxxxxxxxxxxxxxxxxxxxxx:90725768::\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"consul-server1.cdetesting.com\" , \"type\" : \"virtual-server\" , \"family\" : \"ims\" , \"crn\" : \"crn:v1:bluemix:public:virtual-server:wdc06:a/xxxxxxxxxxxxxxxxxxxxxx:90725770::\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxx\" }","title":"Find Classic IaaS virtual instances by tag:"},{"location":"ibm-api/search-rest-api-examples/#find-block-storage-volumes-with-specific-note","text":"The Classic File and Block volumes do not support tagging via the Cloud Portal. In their place their is a field to store notes about the volume. We will run a search for all block volumes with the note ryantiffany and just return their names. You can tag these volumes via the Tagging API. For an example of how to tag Block / File storage see the following example . $ curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?limit=50\" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"(type:block-storage AND service_name:block-storage) AND (doc.notes:ryantiffany)\" , \"fields\": [ \"name\", \"tags\" ]}' | jq -r Example Output { \"items\" : [ { \"name\" : \"SL02SEL78003-13\" , \"crn\" : \"crn:v1:bluemix:public:block-storage:wdc07:a/xxxxxxxxxxxxxxxxxxxxxx:53093855::\" , \"tags\" : [] } ] , \"limit\" : 50 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" }","title":"Find Block storage volumes with specific note"},{"location":"ibm-api/search-rest-api-examples/#find-all-vmware-solutions-deployments","text":"The following call with return all the VMware Dedicated and VMware Shared deployments on the account. curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"type:vmware-solutions\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"vcs-rt\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/xxxxxxxxxxxxxxxxxxxxxx:55c17e7d-eaed-4b4f-xxxx-xxxxxxxx::\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"primary-rt\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/xxxxxxxxxxxxxxxxxxxxxx:707319d9-2f6b-4b48-xxxx-xxxxxxxx::\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"jb-test-1\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/xxxxxxxxxxxxxxxxxxxxxx:824912d4-10be-4421-xxxx-xxxxxxxx::\" } , { \"account_id\" : \"6c27214690345bfb75bb1f2b28a20504\" , \"name\" : \"centerpoint\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/6c27214690345bfb75bb1f2b28a20504:900d7d94-8236-4382-xxxx-xxxxxxxx:\" } , { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"vcd-rt\" , \"type\" : \"vmware-solutions\" , \"family\" : \"vmware\" , \"crn\" : \"crn:v1:bluemix:public:vmware-solutions:global:a/xxxxxxxxxxxxxxxxxxxxxx:9ac98b37-cd31-4800-xxxx-xxxxxxxx::\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" }","title":"Find all VMware Solutions deployments"},{"location":"ibm-api/search-rest-api-examples/#find-all-juniper-network-gateway-appliances-in-the-washington-7-datacenter","text":"curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"(family:ims AND type:network-gateway-juniper-vsrx) AND (region:washington-7)\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"ha-srx\" , \"type\" : \"network-gateway-juniper-vsrx\" , \"family\" : \"ims\" , \"crn\" : \"crn:v1:bluemix:public:network-gateway-juniper-vsrx:washington-7:a/xxxxxxxxxxxxxxxxxxxxxx:538282::\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" }","title":"Find all Juniper Network Gateway appliances in the Washington 7 Datacenter"},{"location":"ibm-api/search-rest-api-examples/#cloud-resources","text":"","title":"Cloud Resources"},{"location":"ibm-api/search-rest-api-examples/#find-all-kubernetes-clusters-and-just-return-their-name-and-associated-tags","text":"Note : the CRN is always returned even if not part of the field return parameters. curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"content-type: application/json\" -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" \\ -d '{\"query\": \"type:k8-cluster\", \"fields\": [ \"name\", \"tags\", \"service_instance\" ]}' | jq -r Example Output { \"items\" : [ { \"name\" : \"devcluster\" , \"service_instance\" : \"bpdhxxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:bpdhxxxx::\" , \"tags\" : [ \"ryantiffany\" ] } , { \"name\" : \"mycluster-dal10-b3c.4x16\" , \"service_instance\" : \"bsdexxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:bsdexxxx::\" , \"tags\" : [ \"jack\" ] } , { \"name\" : \"aamir-testcluster-sjc04-b3c.4x16\" , \"service_instance\" : \"bslaxxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:bslaxxxx::\" , \"tags\" : [] } , { \"name\" : \"cluster2\" , \"service_instance\" : \"btckxxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:btckxxxx::\" , \"tags\" : [] } , { \"name\" : \"Russell_Cluster\" , \"service_instance\" : \"bthrxxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:bthrxxxx::\" , \"tags\" : [] } , { \"name\" : \"rt-us-south\" , \"service_instance\" : \"btr2xxxx\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:btr2xxxx::\" , \"tags\" : [ \"region:us-south\" , \"ryantiffany\" ] } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" }","title":"Find all Kubernetes Clusters and just return their name and associated tags:"},{"location":"ibm-api/search-rest-api-examples/#find-all-kubernetes-clusters-in-a-given-region","text":"In this example we are searching for all Kubernetes Clusters in the Tokyo region: $ curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"type:k8-cluster AND region:jp-tok\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"subredOpen\" , \"type\" : \"k8-cluster\" , \"family\" : \"containers\" , \"crn\" : \"crn:v1:bluemix:public:containers-kubernetes:jp-tok:a/xxxxxxxxxxxxxxxxxxxxxx:blr1xxxxxx:\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" }","title":"Find all Kubernetes clusters in a given region"},{"location":"ibm-api/search-rest-api-examples/#find-all-vpc-instances-in-a-specific-region-with-specific-tags","text":"Since VPC instances stretch across multiple zones in a region we use the wildcard search term region:us-east* in this case to show all instances within the US-East VPC region. $ curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"(family:is AND type:instance AND region:us-east*)\"}' Example output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"wgtest-z1-dev\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-1:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0757_2834694d-ad33-4b81-xxxx-xxxxxxxx\" }, { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"wgtest-wg-dev\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-1:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0757_ebdfe595-c96b-4924-xxxx-xxxxxxxx\" }, { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"vpntest\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-1:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0757_f89fa662-71ab-45e1-xxxx-xxxxxxxx\" }, { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"wgtest-z2-dev\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-2:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0767_65809d72-adca-4c5d-xxxx-xxxxxxxx\" }, { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"wgtest-z3-dev\" , \"type\" : \"instance\" , \"family\" : \"is\" , \"crn\" : \"crn:v1:bluemix:public:is:us-east-3:a/xxxxxxxxxxxxxxxxxxxxxx::instance:0777_e7321492-179b-4f26-xxxx-xxxxxxxx\" } ], \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" }","title":"Find all VPC instances in a specific region with specific tags"},{"location":"ibm-api/search-rest-api-examples/#find-all-database-for-etcd-instances-with-a-specific-tag","text":"curl -s -X POST \"https://api.global-search-tagging.cloud.ibm.com/v3/resources/search?account_id= ${ ACCOUNT_ID } \" \\ -H \"accept: application/json\" -H \"Authorization: ${ iam_token } \" -H \"content-type: application/json\" \\ -d '{\"query\":\"(type:resource-instance AND service_name:databases-for-etcd) AND (tags:ryantiffany)\"}' | jq -r Example Output { \"items\" : [ { \"account_id\" : \"xxxxxxxxxxxxxxxxxxxxxx\" , \"name\" : \"rt-1978903720bf\" , \"type\" : \"resource-instance\" , \"family\" : \"resource_controller\" , \"crn\" : \"crn:v1:bluemix:public:databases-for-etcd:us-south:a/xxxxxxxxxxxxxxxxxxxxxx:41625c25-f836-491e-xxx-xxxxxx::\" } ] , \"limit\" : 10 , \"search_cursor\" : \"xxxxxxxxxxxxxxxxxxxxxx\" }","title":"Find all Database for Etcd instances with a specific tag"},{"location":"ibm-api/vpc-api-examples/","text":"Overview This page shows examples of interacting with the IBM Cloud VPC API . Table of Contents: - Overview * Prerequisites + Endpoints + Versioning * Create a VPC * Create a Public Gateway * Create a Subnet Attached to a Public Gateway (Address Count) * Create a Subnet Attached to a Public Gateway (CIDR) * Create a Security Group with Rules * Create a Block Volume * Attach a Block Volume to a Compute Instance Prerequisites You have a valid IAM Token as outlined here Endpoints The endpoint is based on the region of the service and follows the convention https://REGION.iaas.cloud.ibm.com . The examples on this page use us-south by default so my base API endpoint is https://us-south.iaas.cloud.ibm.com . - VPC Region Endpoints Versioning API requests require a major version in the path (/v1/) and a date-based version as a query parameter in the format version= YYYY-MM-DD . You can use any date-based version up to the current date. Start development of new applications with the current date as a fixed value. See the VPC API Change Log for API changes. Create a VPC VPC is a virtual network in IBM Cloud. It gives you cloud security, with the ability to scale dynamically, by providing fine-grained control over your virtual infrastructure and your network traffic segmentation. RESOURCE_GROUP_ID: The ID of the resource group the VPC will be deployed in to. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/vpcs?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"address_prefix_management\": \"auto\", \"name\": \"rest-demo-vpc\", \"resource_group\": { \"id\": \"RESOURCE_GROUP_ID\" } }' Create a Public Gateway This example will create a VPC Public Gateway . A Public Gateway enables a subnet (or subnets) and all the attached virtual server instances to connect to the internet. Public gateways use Many-to-1 NAT. VPC_ID: The ID of the VPC where the subnet will be created. RESOURCE_GROUP_ID: The ID of the resource group the VPC will be deployed in to. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/public_gateways?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"rest-demo-public-gateway\", \"vpc\": { \"id\": \"VPC_ID\" }, \"resource_group\": { \"id\": \"RESOURCE_GROUP_ID\" }, \"zone\": { \"name\": \"us-south-1\" } }' Create a Subnet Attached to a Public Gateway (Address Count) This example uses total_ipv4_address_count to carve out a set number of IPs from the default Address Prefix . If you need to provision a specific CIDR block use the example under this one. NETWORK_ACL_ID: The network ACL ID to use for this subnet. If unspecified, the default network ACL for the VPC is used. VPC_ID: The ID of the VPC where the subnet will be created. PUBLIC_GATEWAY_ID: The Public Gateway ID to use for this subnet. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/subnets?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"rest-demo-subnet-1\", \"total_ipv4_address_count\": 32, \"ip_version\": \"ipv4\", \"zone\": { \"name\": \"us-south-1\" }, \"vpc\": { \"id\": \"VPC_ID\" }, \"public_gateway\": { \"id\": \"PUBLIC_GATEWAY_ID\" }, \"network_acl\": { \"id\": \"NETWORK_ACL_ID\" } }' Create a Subnet Attached to a Public Gateway (CIDR) This example uses ipv4_cidr_block to carve out a specific CIDR block from the default Address Prefix . NETWORK_ACL_ID: The network ACL ID to use for this subnet. If unspecified, the default network ACL for the VPC is used. VPC_ID: The ID of the VPC where the subnet will be created. PUBLIC_GATEWAY_ID: The Public Gateway ID to use for this subnet. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/subnets?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"rest-demo-subnet-2\", \"ipv4_cidr_block\": \"10.240.0.0/24\", \"ip_version\": \"ipv4\", \"zone\": { \"name\": \"us-south-1\" }, \"vpc\": { \"id\": \"VPC_ID\" }, \"public_gateway\": { \"id\": \"PUBLIC_GATEWAY_ID\" }, \"network_acl\": { \"id\": \"NETWORK_ACL_ID\" } }' Create a Security Group with Rules A security group acts as a Stateful virtual firewall with a collection of rules that specify whether to allow or deny traffic for an associated compute instance. In this example we are allowing inbound ICMP and SSH and allowing all for outbound traffic. VPC_ID: The ID of the VPC where the security group will be created. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/security_groups?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"my-security-group\", \"rules\": [{ \"direction\": \"inbound\", \"ip_version\": \"ipv4\", \"protocol\": \"tcp\", \"port_min\": 22, \"port_max\": 22, \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"outbound\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"ip_version\": \"ipv4\", \"protocol\": \"icmp\", \"type\": 8, \"code\": 0, \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } } ], \"vpc\": { \"id\": \"VPC_ID\" } }' Create a Block Volume Block Storage for VPC offers block-level volumes that are attached to an instance as a boot volume when the instance is created, or attached as secondary data volumes. PROFILE_NAME: The name of the volumes IOP profile. Default is general-purpose . See IOP Tiers for the full list. RESOURCE_GROUP_ID: The ID of the resource group the VPC will be deployed in to. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/volumes?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"my-volume-1\", \"capacity\": 500, \"zone\": { \"name\": \"us-south-2\" }, \"profile\": { \"name\": \"PROFILE_NAME\" }, \"resource_group\": { \"id\": \"RESOURCE_GROUP_ID\" } }' Create a Compute Instance This example will create a new VPC compute instance. - VPC_ID: The ID of the VPC where the subnet will be created. - SUBNET_ID: The ID of the Subnet to use for the compute instance. - IMAGE_ID: The ID of OS image to use. - SSH_KEY_ID: The ID of the SSH key that will be added to the compute instance. curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/instances?version=2020-12-01&generation=2\" \\ -d '{ \"boot_volume_attachment\": { \"volume\": { \"name\": \"rest-api-test-boot-volume\", \"profile\": { \"name\": \"general-purpose\" } } }, \"primary_network_interface\": { \"name\": \"rest-api-test-nic\", \"subnet\": { \"id\": \"SUBNET_ID\" } }, \"name\": \"rest-api-test-instance\", \"zone\": { \"name\": \"us-south-1\" }, \"vpc\": { \"id\": \"VPC_ID\" }, \"profile\": { \"name\": \"cx2-2x4\" }, \"image\": { \"id\": \"IMAGE_ID\" }, \"keys\": [ { \"id\": \"SSH_KEY_ID\" } ] }' Attach a Block Volume to a Compute Instance This example will attach a block volume to a running compute instance. If you want the volume to remain after the instance is deleted change \"delete_volume_on_instance_delete\": true to \"delete_volume_on_instance_delete\": false when making the call. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/instances/INSTANCE_ID/volume_attachments?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"delete_volume_on_instance_delete\": true, \"name\": \"my-volume-attachment-data-5iops\", \"volume\": { \"id\": \"VOLUME_ID\" } }'","title":"Virtual Private Cloud API"},{"location":"ibm-api/vpc-api-examples/#overview","text":"This page shows examples of interacting with the IBM Cloud VPC API . Table of Contents: - Overview * Prerequisites + Endpoints + Versioning * Create a VPC * Create a Public Gateway * Create a Subnet Attached to a Public Gateway (Address Count) * Create a Subnet Attached to a Public Gateway (CIDR) * Create a Security Group with Rules * Create a Block Volume * Attach a Block Volume to a Compute Instance","title":"Overview"},{"location":"ibm-api/vpc-api-examples/#prerequisites","text":"You have a valid IAM Token as outlined here","title":"Prerequisites"},{"location":"ibm-api/vpc-api-examples/#endpoints","text":"The endpoint is based on the region of the service and follows the convention https://REGION.iaas.cloud.ibm.com . The examples on this page use us-south by default so my base API endpoint is https://us-south.iaas.cloud.ibm.com . - VPC Region Endpoints","title":"Endpoints"},{"location":"ibm-api/vpc-api-examples/#versioning","text":"API requests require a major version in the path (/v1/) and a date-based version as a query parameter in the format version= YYYY-MM-DD . You can use any date-based version up to the current date. Start development of new applications with the current date as a fixed value. See the VPC API Change Log for API changes.","title":"Versioning"},{"location":"ibm-api/vpc-api-examples/#create-a-vpc","text":"VPC is a virtual network in IBM Cloud. It gives you cloud security, with the ability to scale dynamically, by providing fine-grained control over your virtual infrastructure and your network traffic segmentation. RESOURCE_GROUP_ID: The ID of the resource group the VPC will be deployed in to. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/vpcs?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"address_prefix_management\": \"auto\", \"name\": \"rest-demo-vpc\", \"resource_group\": { \"id\": \"RESOURCE_GROUP_ID\" } }'","title":"Create a VPC"},{"location":"ibm-api/vpc-api-examples/#create-a-public-gateway","text":"This example will create a VPC Public Gateway . A Public Gateway enables a subnet (or subnets) and all the attached virtual server instances to connect to the internet. Public gateways use Many-to-1 NAT. VPC_ID: The ID of the VPC where the subnet will be created. RESOURCE_GROUP_ID: The ID of the resource group the VPC will be deployed in to. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/public_gateways?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"rest-demo-public-gateway\", \"vpc\": { \"id\": \"VPC_ID\" }, \"resource_group\": { \"id\": \"RESOURCE_GROUP_ID\" }, \"zone\": { \"name\": \"us-south-1\" } }'","title":"Create a Public Gateway"},{"location":"ibm-api/vpc-api-examples/#create-a-subnet-attached-to-a-public-gateway-address-count","text":"This example uses total_ipv4_address_count to carve out a set number of IPs from the default Address Prefix . If you need to provision a specific CIDR block use the example under this one. NETWORK_ACL_ID: The network ACL ID to use for this subnet. If unspecified, the default network ACL for the VPC is used. VPC_ID: The ID of the VPC where the subnet will be created. PUBLIC_GATEWAY_ID: The Public Gateway ID to use for this subnet. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/subnets?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"rest-demo-subnet-1\", \"total_ipv4_address_count\": 32, \"ip_version\": \"ipv4\", \"zone\": { \"name\": \"us-south-1\" }, \"vpc\": { \"id\": \"VPC_ID\" }, \"public_gateway\": { \"id\": \"PUBLIC_GATEWAY_ID\" }, \"network_acl\": { \"id\": \"NETWORK_ACL_ID\" } }'","title":"Create a Subnet Attached to a Public Gateway (Address Count)"},{"location":"ibm-api/vpc-api-examples/#create-a-subnet-attached-to-a-public-gateway-cidr","text":"This example uses ipv4_cidr_block to carve out a specific CIDR block from the default Address Prefix . NETWORK_ACL_ID: The network ACL ID to use for this subnet. If unspecified, the default network ACL for the VPC is used. VPC_ID: The ID of the VPC where the subnet will be created. PUBLIC_GATEWAY_ID: The Public Gateway ID to use for this subnet. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/subnets?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"rest-demo-subnet-2\", \"ipv4_cidr_block\": \"10.240.0.0/24\", \"ip_version\": \"ipv4\", \"zone\": { \"name\": \"us-south-1\" }, \"vpc\": { \"id\": \"VPC_ID\" }, \"public_gateway\": { \"id\": \"PUBLIC_GATEWAY_ID\" }, \"network_acl\": { \"id\": \"NETWORK_ACL_ID\" } }'","title":"Create a Subnet Attached to a Public Gateway (CIDR)"},{"location":"ibm-api/vpc-api-examples/#create-a-security-group-with-rules","text":"A security group acts as a Stateful virtual firewall with a collection of rules that specify whether to allow or deny traffic for an associated compute instance. In this example we are allowing inbound ICMP and SSH and allowing all for outbound traffic. VPC_ID: The ID of the VPC where the security group will be created. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/security_groups?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"my-security-group\", \"rules\": [{ \"direction\": \"inbound\", \"ip_version\": \"ipv4\", \"protocol\": \"tcp\", \"port_min\": 22, \"port_max\": 22, \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"outbound\", \"ip_version\": \"ipv4\", \"protocol\": \"all\", \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } }, { \"direction\": \"inbound\", \"ip_version\": \"ipv4\", \"protocol\": \"icmp\", \"type\": 8, \"code\": 0, \"remote\": { \"cidr_block\": \"0.0.0.0/0\" } } ], \"vpc\": { \"id\": \"VPC_ID\" } }'","title":"Create a Security Group with Rules"},{"location":"ibm-api/vpc-api-examples/#create-a-block-volume","text":"Block Storage for VPC offers block-level volumes that are attached to an instance as a boot volume when the instance is created, or attached as secondary data volumes. PROFILE_NAME: The name of the volumes IOP profile. Default is general-purpose . See IOP Tiers for the full list. RESOURCE_GROUP_ID: The ID of the resource group the VPC will be deployed in to. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/volumes?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"name\": \"my-volume-1\", \"capacity\": 500, \"zone\": { \"name\": \"us-south-2\" }, \"profile\": { \"name\": \"PROFILE_NAME\" }, \"resource_group\": { \"id\": \"RESOURCE_GROUP_ID\" } }'","title":"Create a Block Volume"},{"location":"ibm-api/vpc-api-examples/#create-a-compute-instance","text":"This example will create a new VPC compute instance. - VPC_ID: The ID of the VPC where the subnet will be created. - SUBNET_ID: The ID of the Subnet to use for the compute instance. - IMAGE_ID: The ID of OS image to use. - SSH_KEY_ID: The ID of the SSH key that will be added to the compute instance. curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/instances?version=2020-12-01&generation=2\" \\ -d '{ \"boot_volume_attachment\": { \"volume\": { \"name\": \"rest-api-test-boot-volume\", \"profile\": { \"name\": \"general-purpose\" } } }, \"primary_network_interface\": { \"name\": \"rest-api-test-nic\", \"subnet\": { \"id\": \"SUBNET_ID\" } }, \"name\": \"rest-api-test-instance\", \"zone\": { \"name\": \"us-south-1\" }, \"vpc\": { \"id\": \"VPC_ID\" }, \"profile\": { \"name\": \"cx2-2x4\" }, \"image\": { \"id\": \"IMAGE_ID\" }, \"keys\": [ { \"id\": \"SSH_KEY_ID\" } ] }'","title":"Create a Compute Instance"},{"location":"ibm-api/vpc-api-examples/#attach-a-block-volume-to-a-compute-instance","text":"This example will attach a block volume to a running compute instance. If you want the volume to remain after the instance is deleted change \"delete_volume_on_instance_delete\": true to \"delete_volume_on_instance_delete\": false when making the call. $ curl -X POST -H \"Authorization: ${ iam_token } \" \\ \"https://us-south.iaas.cloud.ibm.com/v1/instances/INSTANCE_ID/volume_attachments?version=YYYY-MM-DD&generation=2\" \\ -d '{ \"delete_volume_on_instance_delete\": true, \"name\": \"my-volume-attachment-data-5iops\", \"volume\": { \"id\": \"VOLUME_ID\" } }'","title":"Attach a Block Volume to a Compute Instance"},{"location":"ibm-cli/","text":"Examples of interacting with the IBM Cloud CLI A collection of examples for interacting with the IBM Cloud CLI. Where are you running the commands? If you plan to use your desktop/laptop, see the Local Prerequisites section. If you cannot install tools on your desktop/laptop, follow the Cloud Shell instructions. Local Prerequisites IBM Cloud CLI installed jq installed OR bxshell installed. This is a docker image with IBM and cloud-native tools baked in. Cloud Shell IBM Cloud Shell is a free service that gives you complete control of your cloud resources, applications, and infrastructure, from any web browser. It\u2019s instantly accessible from your free IBM Cloud account\u2014no other installation is needed. Cloud Shell setup Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window.","title":"IBM Cloud CLI"},{"location":"ibm-cli/#examples-of-interacting-with-the-ibm-cloud-cli","text":"A collection of examples for interacting with the IBM Cloud CLI.","title":"Examples of interacting with the IBM Cloud CLI"},{"location":"ibm-cli/#where-are-you-running-the-commands","text":"If you plan to use your desktop/laptop, see the Local Prerequisites section. If you cannot install tools on your desktop/laptop, follow the Cloud Shell instructions.","title":"Where are you running the commands?"},{"location":"ibm-cli/#local-prerequisites","text":"IBM Cloud CLI installed jq installed OR bxshell installed. This is a docker image with IBM and cloud-native tools baked in.","title":"Local Prerequisites"},{"location":"ibm-cli/#cloud-shell","text":"IBM Cloud Shell is a free service that gives you complete control of your cloud resources, applications, and infrastructure, from any web browser. It\u2019s instantly accessible from your free IBM Cloud account\u2014no other installation is needed.","title":"Cloud Shell"},{"location":"ibm-cli/#cloud-shell-setup","text":"Open the IBM Cloud console (cloud.ibm.com) in your browser and log in if needed. Invoke Cloud Shell by clicking on the button at the top, right-hand corner of the browser window.","title":"Cloud Shell setup"},{"location":"ibm-cli/icos/","text":"Overview Examples of interacting with the IBM Cloud Object Storage CLI Plugin. Prerequisites Requires the cos plugin to be installed. Configure cos plugin to use your object storage instance NAME_OF_COS_INSTANCE: The name of the existing Object Storage instance to interact with ibmcloud cos config crn --crn $( ibmcloud resource service-instance NAME_OF_COS_INSTANCE --output json | jq -r '.[].id' ) Upload object to object storage bucket ibmcloud cos upload --bucket NAME_OF_BUCKET --key NAME_FOR_OBJECT \\ --file /path/to/object --region ICOS_REGION Create HMAC credentials for S3 clients ibmcloud resource service-key-create NAME_OF_SERVICE_KEY Writer --instance-name NAME_OF_COS_INSTANCE --parameters '{\"HMAC\":true}'","title":"Using the Object Storage CLI Plugin"},{"location":"ibm-cli/icos/#overview","text":"Examples of interacting with the IBM Cloud Object Storage CLI Plugin.","title":"Overview"},{"location":"ibm-cli/icos/#prerequisites","text":"Requires the cos plugin to be installed.","title":"Prerequisites"},{"location":"ibm-cli/icos/#configure-cos-plugin-to-use-your-object-storage-instance","text":"NAME_OF_COS_INSTANCE: The name of the existing Object Storage instance to interact with ibmcloud cos config crn --crn $( ibmcloud resource service-instance NAME_OF_COS_INSTANCE --output json | jq -r '.[].id' )","title":"Configure cos plugin to use your object storage instance"},{"location":"ibm-cli/icos/#upload-object-to-object-storage-bucket","text":"ibmcloud cos upload --bucket NAME_OF_BUCKET --key NAME_FOR_OBJECT \\ --file /path/to/object --region ICOS_REGION","title":"Upload object to object storage bucket"},{"location":"ibm-cli/icos/#create-hmac-credentials-for-s3-clients","text":"ibmcloud resource service-key-create NAME_OF_SERVICE_KEY Writer --instance-name NAME_OF_COS_INSTANCE --parameters '{\"HMAC\":true}'","title":"Create HMAC credentials for S3 clients"},{"location":"ibm-cli/resource-search/","text":"Command line examples for ibmcloud resource search IaaS = Classic Infrastructure (SoftLayer) Resources. Cloud = IBM Cloud Resources. Search Cloud Resources (Cloud) $ ibmcloud resource search 'name:devcluster' Search by resource name and return CRN (Cloud) $ ibmcloud resource search 'name:devcluster' --output json | jq -r '.items[].crn' Search by resource tag (Cloud) $ ibmcloud resource search 'tags:ryantiffany' --output json Return resource names (Cloud) $ ibmcloud resource search 'tags:ryantiffany' --output json | jq -r '.items[].name' Return resource CRNs (Cloud) $ ibmcloud resource search 'tags:ryantiffany' --output json | jq -r '.items[].crn' Return resource types (Cloud) $ ibmcloud resource search 'tags:ryantiffany' --output json | jq -r '.items[].type' Search classic infrastructure (IaaS) $ ibmcloud resource search -p classic-infrastructure --output json Search classic infrastructure by tag (IaaS) $ ibmcloud resource search \"tagReferences.tag.name:ryantiffany\" -p classic-infrastructure --output json Return resource types (IaaS) $ ibmcloud resource search \"tagReferences.tag.name:ryantiffany\" -p classic-infrastructure --output json | jq -r '.items[].resourceType' Search by tag and filter on virtual instances (IaaS) $ ibmcloud resource search \"tagReferences.tag.name:ryantiffany _objectType:SoftLayer_Virtual_Guest\" -p classic-infrastructure --output json Search IaaS Virtual instances by Tag and return FQDNs $ ibmcloud resource search \"tagReferences.tag.name:ryantiffany _objectType:SoftLayer_Virtual_Guest\" -p classic-infrastructure --output json | jq -r '.items[].resource.fullyQualifiedDomainName' Search IaaS Virtual instances by Tag and return instance ID's ```shell $ ibmcloud resource search \"tagReferences.tag.name: _objectType:SoftLayer_Virtual_Guest\" -p classic-infrastructure --output json | jq -r '.items[].resource.id'","title":"Searching with the IBM Cloud CLI"},{"location":"ibm-cli/resource-search/#command-line-examples-for-ibmcloud-resource-search","text":"IaaS = Classic Infrastructure (SoftLayer) Resources. Cloud = IBM Cloud Resources.","title":"Command line examples for ibmcloud resource search"},{"location":"ibm-cli/resource-search/#search-cloud-resources-cloud","text":"$ ibmcloud resource search 'name:devcluster'","title":"Search Cloud Resources (Cloud)"},{"location":"ibm-cli/resource-search/#search-by-resource-name-and-return-crn-cloud","text":"$ ibmcloud resource search 'name:devcluster' --output json | jq -r '.items[].crn'","title":"Search by resource name and return CRN  (Cloud)"},{"location":"ibm-cli/resource-search/#search-by-resource-tag-cloud","text":"$ ibmcloud resource search 'tags:ryantiffany' --output json","title":"Search by resource tag (Cloud)"},{"location":"ibm-cli/resource-search/#return-resource-names-cloud","text":"$ ibmcloud resource search 'tags:ryantiffany' --output json | jq -r '.items[].name'","title":"Return resource names (Cloud)"},{"location":"ibm-cli/resource-search/#return-resource-crns-cloud","text":"$ ibmcloud resource search 'tags:ryantiffany' --output json | jq -r '.items[].crn'","title":"Return resource CRNs (Cloud)"},{"location":"ibm-cli/resource-search/#return-resource-types-cloud","text":"$ ibmcloud resource search 'tags:ryantiffany' --output json | jq -r '.items[].type'","title":"Return resource types  (Cloud)"},{"location":"ibm-cli/resource-search/#search-classic-infrastructure-iaas","text":"$ ibmcloud resource search -p classic-infrastructure --output json","title":"Search classic infrastructure (IaaS)"},{"location":"ibm-cli/resource-search/#search-classic-infrastructure-by-tag-iaas","text":"$ ibmcloud resource search \"tagReferences.tag.name:ryantiffany\" -p classic-infrastructure --output json","title":"Search classic infrastructure by tag (IaaS)"},{"location":"ibm-cli/resource-search/#return-resource-types-iaas","text":"$ ibmcloud resource search \"tagReferences.tag.name:ryantiffany\" -p classic-infrastructure --output json | jq -r '.items[].resourceType'","title":"Return resource types (IaaS)"},{"location":"ibm-cli/resource-search/#search-by-tag-and-filter-on-virtual-instances-iaas","text":"$ ibmcloud resource search \"tagReferences.tag.name:ryantiffany _objectType:SoftLayer_Virtual_Guest\" -p classic-infrastructure --output json","title":"Search by tag and filter on virtual instances (IaaS)"},{"location":"ibm-cli/resource-search/#search-iaas-virtual-instances-by-tag-and-return-fqdns","text":"$ ibmcloud resource search \"tagReferences.tag.name:ryantiffany _objectType:SoftLayer_Virtual_Guest\" -p classic-infrastructure --output json | jq -r '.items[].resource.fullyQualifiedDomainName'","title":"Search IaaS Virtual instances by Tag and return FQDNs"},{"location":"ibm-cli/resource-search/#search-iaas-virtual-instances-by-tag-and-return-instance-ids","text":"```shell $ ibmcloud resource search \"tagReferences.tag.name: _objectType:SoftLayer_Virtual_Guest\" -p classic-infrastructure --output json | jq -r '.items[].resource.id'","title":"Search IaaS Virtual instances by Tag and return instance ID's"},{"location":"ibm-cli/vpc/","text":"Overview Examples of interacting with the IBM Cloud VPC CLI Plugin. VPC Subnet Get all Subnets in a VPC and return their ID and Name VPC_NAME: The name of the VPC where the subnets reside ic is subnets --output json | jq -r '.[] | select(.vpc.name==\"VPC_NAME\") | .name,.id' Images Import custom image in to VPC CUSTOM_IMAGE_NAME: The name assigned to the imported image IMAGE: The file name of the qcow2 image OS_NAME: The IBM Cloud equivalent OS Name. See here for supported options. RESOURCE_GROUP_ID: The resource group ID for the imported image ibmcloud is image-create CUSTOM_IMAGE_NAME --file cos://region/bucket/IMAGE --os-name OS_NAME --resource-group-id RESOURCE_GROUP_ID Instances Examples for interacting with VPC compute instances Get Windows instance password INSTANCE_ID: The compute instance ID ibmcloud is instance-initialization-values INSTANCE_ID --private-key @/path/to/private_key Get primary IP from instance INSTANCE_ID: The compute instance ID ibmcloud is instance INSTANCE_ID --json | jq -r '.primary_network_interface.primary_ipv4_address' Grab ID of compute instance based on name NAME_OF_INSTANCE: The name of the compute instance ibmcloud is instances --output json | jq -r '.[] | select(.name==\"NAME_OF_INSTANCE\") | .id' Find all networking interfaces attached to instance and return their name and ID INSTANCE_ID: The compute instance ID ibmcloud is in -nics INSTANCE_ID --output json | jq -r '.[] | .name,.id' Find the floating IP attached to a specific compute instance INSTANCE_ID: The compute instance ID ibmcloud is instance INSTANCE_ID --output json | jq -r '.network_interfaces[].floating_ips[].id' Finding Names and IDs Finding IBM Image OS Names You can run the following command to list the supported OS Names: ibmcloud is images --visibility public --json | jq -r '.[] | select(.status==\"available\") | .operating_system.name'","title":"Using the VPC CLI Plugin"},{"location":"ibm-cli/vpc/#overview","text":"Examples of interacting with the IBM Cloud VPC CLI Plugin.","title":"Overview"},{"location":"ibm-cli/vpc/#vpc","text":"","title":"VPC"},{"location":"ibm-cli/vpc/#subnet","text":"","title":"Subnet"},{"location":"ibm-cli/vpc/#get-all-subnets-in-a-vpc-and-return-their-id-and-name","text":"VPC_NAME: The name of the VPC where the subnets reside ic is subnets --output json | jq -r '.[] | select(.vpc.name==\"VPC_NAME\") | .name,.id'","title":"Get all Subnets in a VPC and return their ID and Name"},{"location":"ibm-cli/vpc/#images","text":"","title":"Images"},{"location":"ibm-cli/vpc/#import-custom-image-in-to-vpc","text":"CUSTOM_IMAGE_NAME: The name assigned to the imported image IMAGE: The file name of the qcow2 image OS_NAME: The IBM Cloud equivalent OS Name. See here for supported options. RESOURCE_GROUP_ID: The resource group ID for the imported image ibmcloud is image-create CUSTOM_IMAGE_NAME --file cos://region/bucket/IMAGE --os-name OS_NAME --resource-group-id RESOURCE_GROUP_ID","title":"Import custom image in to VPC"},{"location":"ibm-cli/vpc/#instances","text":"Examples for interacting with VPC compute instances","title":"Instances"},{"location":"ibm-cli/vpc/#get-windows-instance-password","text":"INSTANCE_ID: The compute instance ID ibmcloud is instance-initialization-values INSTANCE_ID --private-key @/path/to/private_key","title":"Get Windows instance password"},{"location":"ibm-cli/vpc/#get-primary-ip-from-instance","text":"INSTANCE_ID: The compute instance ID ibmcloud is instance INSTANCE_ID --json | jq -r '.primary_network_interface.primary_ipv4_address'","title":"Get primary IP from instance"},{"location":"ibm-cli/vpc/#grab-id-of-compute-instance-based-on-name","text":"NAME_OF_INSTANCE: The name of the compute instance ibmcloud is instances --output json | jq -r '.[] | select(.name==\"NAME_OF_INSTANCE\") | .id'","title":"Grab ID of compute instance based on name"},{"location":"ibm-cli/vpc/#find-all-networking-interfaces-attached-to-instance-and-return-their-name-and-id","text":"INSTANCE_ID: The compute instance ID ibmcloud is in -nics INSTANCE_ID --output json | jq -r '.[] | .name,.id'","title":"Find all networking interfaces attached to instance and return their name and ID"},{"location":"ibm-cli/vpc/#find-the-floating-ip-attached-to-a-specific-compute-instance","text":"INSTANCE_ID: The compute instance ID ibmcloud is instance INSTANCE_ID --output json | jq -r '.network_interfaces[].floating_ips[].id'","title":"Find the floating IP attached to a specific compute instance"},{"location":"ibm-cli/vpc/#finding-names-and-ids","text":"","title":"Finding Names and IDs"},{"location":"ibm-cli/vpc/#finding-ibm-image-os-names","text":"You can run the following command to list the supported OS Names: ibmcloud is images --visibility public --json | jq -r '.[] | select(.status==\"available\") | .operating_system.name'","title":"Finding IBM Image OS Names"},{"location":"ibm-cos/","text":"layout: default title: IBM Cloud Object Storage nav_order: 2 has_children: true","title":"Index"},{"location":"ibm-cos/veeam-scale-out/","text":"Scale Out Veeam Storage with IBM Cloud Object Storage Prerequisites IBM Cloud Object Storage bucket. See this guide if you need to create an Object Storage Bucket. Veeam Backup Server with an un-used local or Block volume drive. Steps IBM Cloud Pre-Work Prepare Windows server drive to be used with Veeam Create new Backup Repository in Veeam Add IBM Cloud Object Storage as a Scale-Out Repository in Veeam Links IBM Cloud Object Storage Veeam Scale Out Repository Veeam Backup and Replication IBM Cloud Pre-Work Launch IBM Cloud Shell Launch Cloud Shell to run through the following steps. The icon can be found in the upper right navigation bar in the cloud portal. Finding Your Cloud Object Storage Instance ID: We will use the resource service-instance operator to get our Cloud Object Storage instance id (GUID). Use the handy jq utility to only return the GUID. $ ibmcloud resource service-instance <name-of-icos-instance> --output json | jq -r '.[].guid' Note: You will use the value returned by the above command in another command. Please save it. Create a Service ID We will create a service ID to interact with our ICOS bucket. A service ID identifies a service or application similar to how a user ID identifies a user. We can assign specific access policies to the service ID that restrict permissions for using specific services. ibmcloud iam service-id-create <name-of-your-service-id> --description \"Service ID for Veeam Scale out repository\" --output json This command simply returns the ID of the service-id that was created. This will be used in place of in the next command. ibmcloud iam service-id <name-of-your-service-id> --output json | jq -r '.[].id' Note: You will use the value returned by the above command in another command. Please save it. Assign Service ID an access policy We will now create an access policy that gives the Service ID write access to a single IBM Cloud Object Storage bucket. ibmcloud iam service-policy-create <Service ID> --roles Writer --service-name cloud-object-storage --service-instance <Service Instance GUID> --resource-type bucket --resource <bucket-name> Create Service Credentials for the Service ID Veeam uses HMAC credentials (Secret Key/Access Key) so we will need to generate some from cloud shell. We will be binding these new credentials to our Service ID $ ibmcloud resource service-key-create <name of service key> Writer --instance-id <Service Instance GUID> --service-id <Service ID> --parameters '{\"HMAC\":true}' Note: Save the access_key_id and secret_access_key as we will need these when we configure the scale out repository in Veeam. Prepare Windows Server Drive Upon initial login to Windows server you should see Server Manager. If not then launch Server Manager and from the left hand navigation select: File and Storage Services Disks The drive we want to use as our new Veeam Backup Repository In the Volumes view click on Tasks and select New Volume . This will start the new Volume Wizard. Under Server and Disk select your local server and the drive where this volume will be created. Click Next For both the Size and Drive Letter selections you can simply click Next as we are leaving these as default. When it comes the filesystem we will want to select ReFS for the File System and 64K for the Allocation unit size and then click Next . After you have reviewed the volume creation details click Create to provision the volume. The drive is now ready to be used with Veeam. Create New Veeam Backup Repository From the Veeam Backup and Replication console click Backup Infrastructure and then right click on Backup Repositories and select Add backup repository to launch the Repository creation wizard. Select Direct Attached Storage in the first dialog box and Microsoft Windows in the second. Give the backup repository a name and click Next Select your local server and click Populate to list locally attached available drives. In my case I select F and select Next For both the Repository and Mount Server dialog boxes you can accept the defaults and just click Next On the review dialog box make sure to leave Import existing backups automatically unchecked since we're starting off with a fresh set of backup job. Once you're satisfied everything looks correct click Apply . Note: When you see a dialog box asking Change the configuration backup location to the newly created repository? select No . If you select Yes the backup repository becomes un-usable for Scale out ICOS storage. Add Scale Out Storage to Veeam From the Veeam Backup and Replication console click on Backup Infrastructure and then right click on Add Scale out repository : Give the new Scale out repository a name and click Next . Under Performance Tier click Add and then select the newly created backup repository. Select Extend scale-out with object storage and click the Add button. Select IBM Cloud Object Storage Add the Cloud Object Storage endpoint for your bucket as well as the HMAC credentials we created earlier. The bucket that I am going to be offloading backups to was created as a us-south regional bucket. As such I am adding the private us-south endpoint s3.private.us-south.cloud-object-storage.appdomain.cloud . To see all available endpoints see Object Storage Endpoints . Next to credentials click Add. This is where you will add the HMAC keys that we generated previously. With those credentials added click Next. You will now select the COS bucket to use. Under Folder selection click Browse and click Add Folder to create a new directory within the bucket. Click Next to get to the Review page. If everything looks good click Finish . You are now dropped back in to the Scale out repository wizard. You can now set the age out policy for backups in this backup repository. Make sure the Move backups to object storage checkbox is checked and set your policy. The default is 30 days. Click Apply and review the creation details. Click Finish . Make sure to select your newly created backup repository when creating new backup jobs to ensure that your backup roll-off operations run as expected.","title":"Scale Out Repository with Veeam and IBM COS"},{"location":"ibm-cos/veeam-scale-out/#scale-out-veeam-storage-with-ibm-cloud-object-storage","text":"","title":"Scale Out Veeam Storage with IBM Cloud Object Storage"},{"location":"ibm-cos/veeam-scale-out/#prerequisites","text":"IBM Cloud Object Storage bucket. See this guide if you need to create an Object Storage Bucket. Veeam Backup Server with an un-used local or Block volume drive.","title":"Prerequisites"},{"location":"ibm-cos/veeam-scale-out/#steps","text":"IBM Cloud Pre-Work Prepare Windows server drive to be used with Veeam Create new Backup Repository in Veeam Add IBM Cloud Object Storage as a Scale-Out Repository in Veeam","title":"Steps"},{"location":"ibm-cos/veeam-scale-out/#links","text":"IBM Cloud Object Storage Veeam Scale Out Repository Veeam Backup and Replication","title":"Links"},{"location":"ibm-cos/veeam-scale-out/#ibm-cloud-pre-work","text":"","title":"IBM Cloud Pre-Work"},{"location":"ibm-cos/veeam-scale-out/#launch-ibm-cloud-shell","text":"Launch Cloud Shell to run through the following steps. The icon can be found in the upper right navigation bar in the cloud portal.","title":"Launch IBM Cloud Shell"},{"location":"ibm-cos/veeam-scale-out/#finding-your-cloud-object-storage-instance-id","text":"We will use the resource service-instance operator to get our Cloud Object Storage instance id (GUID). Use the handy jq utility to only return the GUID. $ ibmcloud resource service-instance <name-of-icos-instance> --output json | jq -r '.[].guid' Note: You will use the value returned by the above command in another command. Please save it.","title":"Finding Your Cloud Object Storage Instance ID:"},{"location":"ibm-cos/veeam-scale-out/#create-a-service-id","text":"We will create a service ID to interact with our ICOS bucket. A service ID identifies a service or application similar to how a user ID identifies a user. We can assign specific access policies to the service ID that restrict permissions for using specific services. ibmcloud iam service-id-create <name-of-your-service-id> --description \"Service ID for Veeam Scale out repository\" --output json This command simply returns the ID of the service-id that was created. This will be used in place of in the next command. ibmcloud iam service-id <name-of-your-service-id> --output json | jq -r '.[].id' Note: You will use the value returned by the above command in another command. Please save it.","title":"Create a Service ID"},{"location":"ibm-cos/veeam-scale-out/#assign-service-id-an-access-policy","text":"We will now create an access policy that gives the Service ID write access to a single IBM Cloud Object Storage bucket. ibmcloud iam service-policy-create <Service ID> --roles Writer --service-name cloud-object-storage --service-instance <Service Instance GUID> --resource-type bucket --resource <bucket-name>","title":"Assign Service ID an access policy"},{"location":"ibm-cos/veeam-scale-out/#create-service-credentials-for-the-service-id","text":"Veeam uses HMAC credentials (Secret Key/Access Key) so we will need to generate some from cloud shell. We will be binding these new credentials to our Service ID $ ibmcloud resource service-key-create <name of service key> Writer --instance-id <Service Instance GUID> --service-id <Service ID> --parameters '{\"HMAC\":true}' Note: Save the access_key_id and secret_access_key as we will need these when we configure the scale out repository in Veeam.","title":"Create Service Credentials for the Service ID"},{"location":"ibm-cos/veeam-scale-out/#prepare-windows-server-drive","text":"Upon initial login to Windows server you should see Server Manager. If not then launch Server Manager and from the left hand navigation select: File and Storage Services Disks The drive we want to use as our new Veeam Backup Repository In the Volumes view click on Tasks and select New Volume . This will start the new Volume Wizard. Under Server and Disk select your local server and the drive where this volume will be created. Click Next For both the Size and Drive Letter selections you can simply click Next as we are leaving these as default. When it comes the filesystem we will want to select ReFS for the File System and 64K for the Allocation unit size and then click Next . After you have reviewed the volume creation details click Create to provision the volume. The drive is now ready to be used with Veeam.","title":"Prepare Windows Server Drive"},{"location":"ibm-cos/veeam-scale-out/#create-new-veeam-backup-repository","text":"From the Veeam Backup and Replication console click Backup Infrastructure and then right click on Backup Repositories and select Add backup repository to launch the Repository creation wizard. Select Direct Attached Storage in the first dialog box and Microsoft Windows in the second. Give the backup repository a name and click Next Select your local server and click Populate to list locally attached available drives. In my case I select F and select Next For both the Repository and Mount Server dialog boxes you can accept the defaults and just click Next On the review dialog box make sure to leave Import existing backups automatically unchecked since we're starting off with a fresh set of backup job. Once you're satisfied everything looks correct click Apply . Note: When you see a dialog box asking Change the configuration backup location to the newly created repository? select No . If you select Yes the backup repository becomes un-usable for Scale out ICOS storage.","title":"Create New Veeam Backup Repository"},{"location":"ibm-cos/veeam-scale-out/#add-scale-out-storage-to-veeam","text":"From the Veeam Backup and Replication console click on Backup Infrastructure and then right click on Add Scale out repository : Give the new Scale out repository a name and click Next . Under Performance Tier click Add and then select the newly created backup repository. Select Extend scale-out with object storage and click the Add button. Select IBM Cloud Object Storage Add the Cloud Object Storage endpoint for your bucket as well as the HMAC credentials we created earlier. The bucket that I am going to be offloading backups to was created as a us-south regional bucket. As such I am adding the private us-south endpoint s3.private.us-south.cloud-object-storage.appdomain.cloud . To see all available endpoints see Object Storage Endpoints . Next to credentials click Add. This is where you will add the HMAC keys that we generated previously. With those credentials added click Next. You will now select the COS bucket to use. Under Folder selection click Browse and click Add Folder to create a new directory within the bucket. Click Next to get to the Review page. If everything looks good click Finish . You are now dropped back in to the Scale out repository wizard. You can now set the age out policy for backups in this backup repository. Make sure the Move backups to object storage checkbox is checked and set your policy. The default is 30 days. Click Apply and review the creation details. Click Finish . Make sure to select your newly created backup repository when creating new backup jobs to ensure that your backup roll-off operations run as expected.","title":"Add Scale Out Storage to Veeam"},{"location":"ibm-direct-link/","text":"layout: default title: IBM Cloud Direct Link nav_order: 7 has_children: true","title":"Index"},{"location":"ibm-direct-link/classic-direct-link/","text":"Classic Direct Link Overview IBM Cloud Direct Link on Classic offerings provide connectivity from an external source into a customer's IBM Cloud private network. Direct Link on Classic can be viewed as an alternative to a traditional site-to-site VPN solution, which is designed for customers that need more consistent, higher-throughput connectivity between a remote network and their IBM Cloud environments. Exchange Direct Link Exchange on Classic allows customers to use an Exchange provider to deliver connectivity to their IBM Cloud. An Exchange provider is a colocation or data center provider that is already connected to the IBM Cloud network, by using multi-tenant, high capacity links (also known as a network-to-network interface, or NNI). Customers typically can purchase a virtual circuit at this provider, bringing connectivity at a reduced cost, because the physical connectivity from IBM Cloud to the Exchange provider is in place already, shared among other customers. Connect Direct Link Connect on Classic allows customers to use a connection through our Carrier partners who own and operate a facility-based network. A Connect provider is a network service provider (NSP) that is already connected to the IBM Cloud network, by using multi-tenant, high capacity links (also known as a network-to-network interface, or NNI). Customers typically can purchase a virtual circuit at this provider, bringing connectivity at a reduced cost, because the physical connectivity from IBM Cloud to the Connect provider is in place already, shared among other customers. Dedicated Direct Link Dedicated on Classic allows customers to terminate a single-tenant, fiber-based cross-connect into the IBM Cloud network. This offering can be used by customers with colocation premises that are next to IBM Cloud PoPs and data centers; as well as network service providers that deliver circuits to customer premises or other data centers. Dedicatated Hosting Direct Link Dedicated Hosting on Classic provides connectivity similar to Direct Link Dedicated, but the connection point is next to an IBM Cloud data center, which improves latency for higher-performance use cases. IBM Cloud offers various customizable colocation services with this solution. Find Locations and Providers via CLI From the customer portal, launch Cloud Shell and run the following commands to pull provider and location information: Get All Available Locations $ ibmcloud sl call-api Network_DirectLink_Location getAllObjects","title":"IBM Cloud Classic Direct Link Overview"},{"location":"ibm-direct-link/classic-direct-link/#classic-direct-link-overview","text":"IBM Cloud Direct Link on Classic offerings provide connectivity from an external source into a customer's IBM Cloud private network. Direct Link on Classic can be viewed as an alternative to a traditional site-to-site VPN solution, which is designed for customers that need more consistent, higher-throughput connectivity between a remote network and their IBM Cloud environments.","title":"Classic Direct Link Overview"},{"location":"ibm-direct-link/classic-direct-link/#exchange","text":"Direct Link Exchange on Classic allows customers to use an Exchange provider to deliver connectivity to their IBM Cloud. An Exchange provider is a colocation or data center provider that is already connected to the IBM Cloud network, by using multi-tenant, high capacity links (also known as a network-to-network interface, or NNI). Customers typically can purchase a virtual circuit at this provider, bringing connectivity at a reduced cost, because the physical connectivity from IBM Cloud to the Exchange provider is in place already, shared among other customers.","title":"Exchange"},{"location":"ibm-direct-link/classic-direct-link/#connect","text":"Direct Link Connect on Classic allows customers to use a connection through our Carrier partners who own and operate a facility-based network. A Connect provider is a network service provider (NSP) that is already connected to the IBM Cloud network, by using multi-tenant, high capacity links (also known as a network-to-network interface, or NNI). Customers typically can purchase a virtual circuit at this provider, bringing connectivity at a reduced cost, because the physical connectivity from IBM Cloud to the Connect provider is in place already, shared among other customers.","title":"Connect"},{"location":"ibm-direct-link/classic-direct-link/#dedicated","text":"Direct Link Dedicated on Classic allows customers to terminate a single-tenant, fiber-based cross-connect into the IBM Cloud network. This offering can be used by customers with colocation premises that are next to IBM Cloud PoPs and data centers; as well as network service providers that deliver circuits to customer premises or other data centers.","title":"Dedicated"},{"location":"ibm-direct-link/classic-direct-link/#dedicatated-hosting","text":"Direct Link Dedicated Hosting on Classic provides connectivity similar to Direct Link Dedicated, but the connection point is next to an IBM Cloud data center, which improves latency for higher-performance use cases. IBM Cloud offers various customizable colocation services with this solution.","title":"Dedicatated Hosting"},{"location":"ibm-direct-link/classic-direct-link/#find-locations-and-providers-via-cli","text":"From the customer portal, launch Cloud Shell and run the following commands to pull provider and location information: Get All Available Locations $ ibmcloud sl call-api Network_DirectLink_Location getAllObjects","title":"Find Locations and Providers via CLI"},{"location":"ibm-guides-tutorials/","text":"This is a collection of tutorials, guides and code samples for IBM Cloud. VMware Shared Getting Started with IBM Cloud for VMware Solutions Shared IPSec Tunnel over IBM Private Network Endpoint Simple Deploy of a VM Set up Veeam Cloud Connect Replication VPC Deploy OpenVPN server in VPC using Terraform and Ansible Consul cluster in IBM Cloud VPC using Terraform and Ansible Deploy a Wireguard VPN Server in IBM Cloud VPC using Terraform Site to Site IPsec Tunnel to IBM Cloud VPC VPNaaS Securely access remote VPC instances with a bastion host Team based privacy using IAM, VPC, Transit Gateway and DNS Classic Infrastructure Isolating workloads with a secure private network Configure NAT for Internet access from a private network VPN into a secure private network Automate a Custom OS Deployment on IBM Cloud Bare Metal Servers Using Terraform and Ansible Kubernetes Deploy a starter kit app to a Kubernetes cluster Using Calico network policies to block traffic Continuous Deployment to Kubernetes Cross account Object Storage bucket sync with free IKS Cluster Analyze logs and monitor application health with LogDNA and Sysdig Planning your IKS cluster storage options Planning your IKS cluster network options Code Engine Text analysis with Code Engine Build a container image from source control using Code Engine Cross account Object Storage bucket sync with Code Engine Cloud Functions Serverless web application and API using Object Storage Build a database-driven Slackbot Terraform Deploying IBM Cloud infrastructure using Terraform and Gitlab Consul cluster in IBM Cloud VPC using Terraform and Ansible Deploy a Wireguard VPN Server in IBM Cloud VPC using Terraform Deploy a VPC, a Bastion host, and a set of webservers across all 3 zones in a VPC Region Schematics Deploy an IBM Cloud VPC Bastion host using Schematics Deploy an IKS Cluster in VPC using Schematics Utilities VPC Diagram Exporter - Export IBM Cloud VPC resources to an SVG or PNG diagram bxshell - A ton of IBM Cloud tools in one Docker image and a few shell scripts","title":"IBM Cloud Tutorials, Tools, and Guides"},{"location":"ibm-guides-tutorials/#vmware-shared","text":"Getting Started with IBM Cloud for VMware Solutions Shared IPSec Tunnel over IBM Private Network Endpoint Simple Deploy of a VM Set up Veeam Cloud Connect Replication","title":"VMware Shared"},{"location":"ibm-guides-tutorials/#vpc","text":"Deploy OpenVPN server in VPC using Terraform and Ansible Consul cluster in IBM Cloud VPC using Terraform and Ansible Deploy a Wireguard VPN Server in IBM Cloud VPC using Terraform Site to Site IPsec Tunnel to IBM Cloud VPC VPNaaS Securely access remote VPC instances with a bastion host Team based privacy using IAM, VPC, Transit Gateway and DNS","title":"VPC"},{"location":"ibm-guides-tutorials/#classic-infrastructure","text":"Isolating workloads with a secure private network Configure NAT for Internet access from a private network VPN into a secure private network Automate a Custom OS Deployment on IBM Cloud Bare Metal Servers Using Terraform and Ansible","title":"Classic Infrastructure"},{"location":"ibm-guides-tutorials/#kubernetes","text":"Deploy a starter kit app to a Kubernetes cluster Using Calico network policies to block traffic Continuous Deployment to Kubernetes Cross account Object Storage bucket sync with free IKS Cluster Analyze logs and monitor application health with LogDNA and Sysdig Planning your IKS cluster storage options Planning your IKS cluster network options","title":"Kubernetes"},{"location":"ibm-guides-tutorials/#code-engine","text":"Text analysis with Code Engine Build a container image from source control using Code Engine Cross account Object Storage bucket sync with Code Engine","title":"Code Engine"},{"location":"ibm-guides-tutorials/#cloud-functions","text":"Serverless web application and API using Object Storage Build a database-driven Slackbot","title":"Cloud Functions"},{"location":"ibm-guides-tutorials/#terraform","text":"Deploying IBM Cloud infrastructure using Terraform and Gitlab Consul cluster in IBM Cloud VPC using Terraform and Ansible Deploy a Wireguard VPN Server in IBM Cloud VPC using Terraform Deploy a VPC, a Bastion host, and a set of webservers across all 3 zones in a VPC Region","title":"Terraform"},{"location":"ibm-guides-tutorials/#schematics","text":"Deploy an IBM Cloud VPC Bastion host using Schematics Deploy an IKS Cluster in VPC using Schematics","title":"Schematics"},{"location":"ibm-guides-tutorials/#utilities","text":"VPC Diagram Exporter - Export IBM Cloud VPC resources to an SVG or PNG diagram bxshell - A ton of IBM Cloud tools in one Docker image and a few shell scripts","title":"Utilities"},{"location":"ibm-vpc/","text":"layout: default title: IBM Cloud VPC nav_order: 5 has_children: true","title":"Index"},{"location":"ibm-vpc/vpc-vpnaas/","text":"Overview This article will walk you through the process of connecting on on-prem IPsec tunnel to the IBM Cloud VPC VPN as a service offering. This will allow you to communicate from your local machine to the private IP addresses assigned to your VPC compute instances. In this guide you will walk through the following steps: Provisioning an instance of VPC VPN as a Service Adding a Peer connection to VPC VPN to connect to your local network Install strongSwan on a local machine/VM Configuring the local IPsec peer Bringing up the local IPsec tunnel and pinging VPC resources Provision and configure the VPC VPNaaS We\u2019ll start by deploying an instance of VPNaaS. From the main VPC landing page click on VPN Gateways on the left hand navigation bar: Make sure you select the region where your VPC resides and then click Create VPN. At the top of the screen give the VPN a name, select the VPC where you would like the VPN deployed, and then select the subnet to use with the VPN. Note : Only the resources in the same zone as the subnet you choose can connect through this VPN gateway. With the subnet selected scroll down and make sure the New VPN Connection for VPC option is enabled. Give the new connection a name and provide the local and peer subnets along with the pre-shared key. Note : If you need to generate a pre-shared key, launch Cloud Shell by clicking the terminal icon in the upper right of the IBM Cloud portal. Once your Cloud Shell session starts run the following command to generate a 32 character pre-shared key tr -dc \"[:alpha:][:alnum:]\" < /dev/urandom | head -c 32 In this example my VPC utilizes the 10.240.0.0/24 subnet and my local network uses 172.16 IPs. The Peer gateway address is the public IP on your local network. If you are unsure of what this is pull up a browser and head to IP Chicken . With all the details added click Create VPN Gateway in the right hand navigation bar to deploy the VPN. We'll give the new VPN a few moments to deploy and then copy down the Peer address that we'll need for the local tunnel configuration. Install strongSwan on local machine In my example I have a local Ubuntu 18 VM that I will be using as the local IPsec peer. The first step is to install strongSwan . $ apt-get update && apt-get install strongswan -y With strongSwan installed we need to add IP forwarding to our kernel parameters: $ sudo cat >> /etc/sysctl.conf << EOF net.ipv4.ip_forward = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 EOF $ sysctl -p /etc/sysctl.conf Configure local ipsec peer Next we'll update the /etc/ipsec.secrets file. The syntax for the file is: LOCAL_PEER_IP VPC_VPN_PEER_IP : PSK \"Pre-shared key\" For example if your local public IP was 192.168.20.2, the VPC VPN Peer address was 192.168.30.5, and the pre-shared key was XtemrMYFfmmMCpxgdCwSYoRBKdjQ1ndb the file would look like this: 192.168.20.2 192.168.30.5 : PSK \"XtemrMYFfmmMCpxgdCwSYoRBKdjQ1ndb\" With the secrets file updated we'll now move on to updating the strongSwan configuration file: # ipsec.conf - strongSwan IPsec configuration file # basic configuration config setup # strictcrlpolicy=yes # uniqueids = no charondebug=\"all\" uniqueids=yes strictcrlpolicy=no # connection to us-east-vpc conn home-to-vpc authby=secret left=%defaultroute leftid=<Local Server Public IP> leftsubnet=<Local Internal Subnet range> right=<VPC VPN Endpoint IP> rightsubnet=<VPC Subnet range>,166.8.0.0/14,161.26.0.0/16 ike=aes256-sha2_256-modp1024! esp=aes256-sha2_256! keyingtries=0 ikelifetime=1h lifetime=8h dpddelay=30 dpdtimeout=120 dpdaction=restart auto=start We add in the ranges 166.8.0.0/14 and 161.26.0.0/16 so that we can communicate with IBM Cloud services over their private IP address space. With the ipsec configuration updated, add an iptables rule for post-routing. Again for my tunnel the VPC Subnet is 10.240.0.0/24 and my local internal subnet is 172.16.0.0/24 so adjust the following command to meet your needs. $ iptables -t nat -A POSTROUTING -s 10.240.0.0/24 -d 172.16.0.0/24 -J MASQUERADE Now restart the ipsec service and check the status of the tunnel: $ sudo ipsec restart Stopping strongSwan IPsec... Starting strongSwan 5.6.2 IPsec [starter]... $ sudo ipsec status Security Associations (1 up, 0 connecting): home-to-vpc[1]: ESTABLISHED 16 seconds ago, 10.0.0.67[x.x.x.x]...52.y.y.y[52.y.y.y] home-to-vpc{1}: INSTALLED, TUNNEL, reqid 1, ESP in UDP SPIs: c2225f47_i ccc3d826_o home-to-vpc{1}: 10.0.0.0/18 === 161.26.0.0/16 166.8.0.0/14 192.168.0.0/18 Test connectivity to VPC instance In my VPC I have an instance with a private IP of 10.240.0.6: $ ibmcloud is instances --output json | jq -r '.[] | select(.vpc.name==\"us-south-vpc-rt\") | .network_interfaces[].primary_ipv4_address' 10.240.0.6 $ ping -c2 -q 10.240.0.6 PING 10.240.0.6 (10.240.0.6) 56(84) bytes of data. --- 10.240.0.6 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 40.836/41.497/42.159/0.692 ms","title":"Site to Site IPsec Tunnel to IBM Cloud VPC VPNaaS"},{"location":"ibm-vpc/vpc-vpnaas/#overview","text":"This article will walk you through the process of connecting on on-prem IPsec tunnel to the IBM Cloud VPC VPN as a service offering. This will allow you to communicate from your local machine to the private IP addresses assigned to your VPC compute instances. In this guide you will walk through the following steps: Provisioning an instance of VPC VPN as a Service Adding a Peer connection to VPC VPN to connect to your local network Install strongSwan on a local machine/VM Configuring the local IPsec peer Bringing up the local IPsec tunnel and pinging VPC resources","title":"Overview"},{"location":"ibm-vpc/vpc-vpnaas/#provision-and-configure-the-vpc-vpnaas","text":"We\u2019ll start by deploying an instance of VPNaaS. From the main VPC landing page click on VPN Gateways on the left hand navigation bar: Make sure you select the region where your VPC resides and then click Create VPN. At the top of the screen give the VPN a name, select the VPC where you would like the VPN deployed, and then select the subnet to use with the VPN. Note : Only the resources in the same zone as the subnet you choose can connect through this VPN gateway. With the subnet selected scroll down and make sure the New VPN Connection for VPC option is enabled. Give the new connection a name and provide the local and peer subnets along with the pre-shared key. Note : If you need to generate a pre-shared key, launch Cloud Shell by clicking the terminal icon in the upper right of the IBM Cloud portal. Once your Cloud Shell session starts run the following command to generate a 32 character pre-shared key tr -dc \"[:alpha:][:alnum:]\" < /dev/urandom | head -c 32 In this example my VPC utilizes the 10.240.0.0/24 subnet and my local network uses 172.16 IPs. The Peer gateway address is the public IP on your local network. If you are unsure of what this is pull up a browser and head to IP Chicken . With all the details added click Create VPN Gateway in the right hand navigation bar to deploy the VPN. We'll give the new VPN a few moments to deploy and then copy down the Peer address that we'll need for the local tunnel configuration.","title":"Provision and configure the VPC VPNaaS"},{"location":"ibm-vpc/vpc-vpnaas/#install-strongswan-on-local-machine","text":"In my example I have a local Ubuntu 18 VM that I will be using as the local IPsec peer. The first step is to install strongSwan . $ apt-get update && apt-get install strongswan -y With strongSwan installed we need to add IP forwarding to our kernel parameters: $ sudo cat >> /etc/sysctl.conf << EOF net.ipv4.ip_forward = 1 net.ipv4.conf.all.accept_redirects = 0 net.ipv4.conf.all.send_redirects = 0 EOF $ sysctl -p /etc/sysctl.conf","title":"Install strongSwan on local machine"},{"location":"ibm-vpc/vpc-vpnaas/#configure-local-ipsec-peer","text":"Next we'll update the /etc/ipsec.secrets file. The syntax for the file is: LOCAL_PEER_IP VPC_VPN_PEER_IP : PSK \"Pre-shared key\" For example if your local public IP was 192.168.20.2, the VPC VPN Peer address was 192.168.30.5, and the pre-shared key was XtemrMYFfmmMCpxgdCwSYoRBKdjQ1ndb the file would look like this: 192.168.20.2 192.168.30.5 : PSK \"XtemrMYFfmmMCpxgdCwSYoRBKdjQ1ndb\" With the secrets file updated we'll now move on to updating the strongSwan configuration file: # ipsec.conf - strongSwan IPsec configuration file # basic configuration config setup # strictcrlpolicy=yes # uniqueids = no charondebug=\"all\" uniqueids=yes strictcrlpolicy=no # connection to us-east-vpc conn home-to-vpc authby=secret left=%defaultroute leftid=<Local Server Public IP> leftsubnet=<Local Internal Subnet range> right=<VPC VPN Endpoint IP> rightsubnet=<VPC Subnet range>,166.8.0.0/14,161.26.0.0/16 ike=aes256-sha2_256-modp1024! esp=aes256-sha2_256! keyingtries=0 ikelifetime=1h lifetime=8h dpddelay=30 dpdtimeout=120 dpdaction=restart auto=start We add in the ranges 166.8.0.0/14 and 161.26.0.0/16 so that we can communicate with IBM Cloud services over their private IP address space. With the ipsec configuration updated, add an iptables rule for post-routing. Again for my tunnel the VPC Subnet is 10.240.0.0/24 and my local internal subnet is 172.16.0.0/24 so adjust the following command to meet your needs. $ iptables -t nat -A POSTROUTING -s 10.240.0.0/24 -d 172.16.0.0/24 -J MASQUERADE Now restart the ipsec service and check the status of the tunnel: $ sudo ipsec restart Stopping strongSwan IPsec... Starting strongSwan 5.6.2 IPsec [starter]... $ sudo ipsec status Security Associations (1 up, 0 connecting): home-to-vpc[1]: ESTABLISHED 16 seconds ago, 10.0.0.67[x.x.x.x]...52.y.y.y[52.y.y.y] home-to-vpc{1}: INSTALLED, TUNNEL, reqid 1, ESP in UDP SPIs: c2225f47_i ccc3d826_o home-to-vpc{1}: 10.0.0.0/18 === 161.26.0.0/16 166.8.0.0/14 192.168.0.0/18","title":"Configure local ipsec peer"},{"location":"ibm-vpc/vpc-vpnaas/#test-connectivity-to-vpc-instance","text":"In my VPC I have an instance with a private IP of 10.240.0.6: $ ibmcloud is instances --output json | jq -r '.[] | select(.vpc.name==\"us-south-vpc-rt\") | .network_interfaces[].primary_ipv4_address' 10.240.0.6 $ ping -c2 -q 10.240.0.6 PING 10.240.0.6 (10.240.0.6) 56(84) bytes of data. --- 10.240.0.6 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 40.836/41.497/42.159/0.692 ms","title":"Test connectivity to VPC instance"},{"location":"kubernetes/","text":"layout: default title: Kubernetes nav_order: 15 has_children: true","title":"Index"},{"location":"kubernetes/icos-sync-iks/","text":"Overview In this guide I will show you how to sync ICOS bucket objects between accounts using Kubernetes. You can spin up 1 free IKS cluster on your account which will be automatically deleted after 30 days. Steps Preparing Accounts Source Account Create Service ID Create Reader access policy for newly created service id Generate HMAC credentials tied to our service ID Destination Account Create Service ID Create Reader access policy for newly created service id Generate HMAC credentials tied to our service ID Create free IBM Cloud Kubernetes cluster Create Kubernetes secret for use with job Download Kubernetes Job file Preparing Accounts We will be using Cloud Shell to generate Service IDs and Object Storage credentials for both the source and destination accounts. Source Account We will create a service ID on the source account. A service ID identifies a service or application similar to how a user ID identifies a user. We can assign specific access policies to the service ID that restrict permissions for using specific services: in this case it gets read-only access to an IBM Cloud Object Storage bucket. Create Service ID $ ibmcloud iam service-id-create <name-of-your-service-id> --description \"Service ID for read-only access to bucket\" --output json Create Reader access policy for newly created service id Now we will limit the scope of this service ID to have read only access to our source Object Storage bucket. $ ibmcloud iam service-policy-create <Service ID> --roles Reader --service-name cloud-object-storage --service-instance <Service Instance GUID> --resource-type bucket --resource <bucket-name> Service Instance GUID - This is the GUID of the Cloud Object Storage instance. You can retrieve this with the command: ibmcloud resource service-instance <name of icos instance> Generate HMAC credentials tied to our service ID In order for the Minio client to talk to each Object Storage instance it will need HMAC credentials (Access Key and Secret Key in S3 parlance). $ ibmcloud resource service-key-create source-icos-service-creds Reader --instance-id <Service Instance GUID> --service-id <Service ID> --parameters '{\"HMAC\":true}' Save the access_key_id and secret_access_key as we will be using these with our Kubernetes job. Destination Account We will create a service ID on the destination account. A service ID identifies a service or application similar to how a user ID identifies a user. We can assign specific access policies to the service ID that restrict permissions for using specific services: in this case it gets write access to an IBM Cloud Object Storage bucket. Create Service ID $ ibmcloud iam service-id-create <name-of-your-service-id> --description \"Service ID for write access to bucket\" --output json Create Reader access policy for newly created service id Now we will limit the scope of this service ID to have read only access to our source Object Storage bucket. $ ibmcloud iam service-policy-create <Service ID> --roles Writer --service-name cloud-object-storage --service-instance <Service Instance GUID> --resource-type bucket --resource <bucket-name> Service Instance GUID - This is the GUID of the Cloud Object Storage instance. You can retrieve this with the command: ibmcloud resource service-instance <name of icos instance> Generate HMAC credentials tied to our service ID We'll follow the same procedure as last time to generate the HMAC credentials, but this time on the destination account. $ ibmcloud resource service-key-create destination-icos-service-creds Writer --instance-id <Service Instance GUID> --service-id <Service ID> --parameters '{\"HMAC\":true}' Save the access_key_id and secret_access_key as we will be using these with our Kubernetes job. Create free IBM Cloud Kubernetes cluster You can have 1 free cluster at a time in IBM Cloud Kubernetes Service, and each free cluster expires in 30 days. This will allow is to run the Minio sync as a Kubernetes Job. $ ibmcloud ks cluster create classic --name <name of cluster> The cluster should be ready in about 10-15 minutes. You can verify the status of the creation by running the ibmcloud ks clusters and seeing when State returns as normal : ryan@cloudshell:~$ ibmcloud ks clusters OK Name ID State Created Workers Location Version Resource Group Name Provider freeikscluster bstcs2nf0bkrt2968nm0 normal 14 minutes ago 1 par01 1 .17.9_1534 default classic Create Kubernetes secret for use with job In order for our container to speak to the Cloud Object storage buckets we'll need to specify some environmental variables to be stored as Kubernetes secrets . The first step is to target our free cluster: $ ibmcloud ks cluster config --cluster <name of cluster> With kubectl now configured to talk to our cluster we can create our Kubernetes secrets. $ kubectl create secret generic <name of secret> --from-literal SOURCE_ACCESS_KEY = VALUE --from-literal SOURCE_SECRET_KEY = VALUE --from-literal SOURCE_REGION = VALUE --from-literal SOURCE_BUCKET = VALUE --from-literal DESTINATION_ACCESS_KEY = VALUE --from-literal DESTINATION_SECRET_KEY = VALUE --from-literal DESTINATION_REGION = VALUE --from-literal DESTINATION_BUCKET = VALUE Download Kubernetes Job file We specify our Kubernetes job in a yaml file hosted here on Github. $ wget https://gist.githubusercontent.com/greyhoundforty/b26d66b33f97d8368e3dd7869b7bbc5e/raw/926e7088cff7c0a991310065832d646e513890f1/job.yaml With the Job spec downloaded we can now submit the job to Kubernetes $ kubectl create -f job.yaml In my testing I am only syncing a few times so by the time I check the Kubernetes pods, the job has already completed. Looking at the logs I am able to verify that the contents have been synced between the Object Storage buckets. ryan@cloudshell:~$ kubectl get pods NAME READY STATUS RESTARTS AGE mc-icos-sync-test-c9zkq 0 /1 Completed 0 102s","title":"Cross account Object Storage bucket sync with free IKS Cluster"},{"location":"kubernetes/icos-sync-iks/#overview","text":"In this guide I will show you how to sync ICOS bucket objects between accounts using Kubernetes. You can spin up 1 free IKS cluster on your account which will be automatically deleted after 30 days.","title":"Overview"},{"location":"kubernetes/icos-sync-iks/#steps","text":"Preparing Accounts Source Account Create Service ID Create Reader access policy for newly created service id Generate HMAC credentials tied to our service ID Destination Account Create Service ID Create Reader access policy for newly created service id Generate HMAC credentials tied to our service ID Create free IBM Cloud Kubernetes cluster Create Kubernetes secret for use with job Download Kubernetes Job file","title":"Steps"},{"location":"kubernetes/icos-sync-iks/#preparing-accounts","text":"We will be using Cloud Shell to generate Service IDs and Object Storage credentials for both the source and destination accounts.","title":"Preparing Accounts"},{"location":"kubernetes/icos-sync-iks/#source-account","text":"We will create a service ID on the source account. A service ID identifies a service or application similar to how a user ID identifies a user. We can assign specific access policies to the service ID that restrict permissions for using specific services: in this case it gets read-only access to an IBM Cloud Object Storage bucket.","title":"Source Account"},{"location":"kubernetes/icos-sync-iks/#create-service-id","text":"$ ibmcloud iam service-id-create <name-of-your-service-id> --description \"Service ID for read-only access to bucket\" --output json","title":"Create Service ID"},{"location":"kubernetes/icos-sync-iks/#create-reader-access-policy-for-newly-created-service-id","text":"Now we will limit the scope of this service ID to have read only access to our source Object Storage bucket. $ ibmcloud iam service-policy-create <Service ID> --roles Reader --service-name cloud-object-storage --service-instance <Service Instance GUID> --resource-type bucket --resource <bucket-name> Service Instance GUID - This is the GUID of the Cloud Object Storage instance. You can retrieve this with the command: ibmcloud resource service-instance <name of icos instance>","title":"Create Reader access policy for newly created service id"},{"location":"kubernetes/icos-sync-iks/#generate-hmac-credentials-tied-to-our-service-id","text":"In order for the Minio client to talk to each Object Storage instance it will need HMAC credentials (Access Key and Secret Key in S3 parlance). $ ibmcloud resource service-key-create source-icos-service-creds Reader --instance-id <Service Instance GUID> --service-id <Service ID> --parameters '{\"HMAC\":true}' Save the access_key_id and secret_access_key as we will be using these with our Kubernetes job.","title":"Generate HMAC credentials tied to our service ID"},{"location":"kubernetes/icos-sync-iks/#destination-account","text":"We will create a service ID on the destination account. A service ID identifies a service or application similar to how a user ID identifies a user. We can assign specific access policies to the service ID that restrict permissions for using specific services: in this case it gets write access to an IBM Cloud Object Storage bucket.","title":"Destination Account"},{"location":"kubernetes/icos-sync-iks/#create-service-id_1","text":"$ ibmcloud iam service-id-create <name-of-your-service-id> --description \"Service ID for write access to bucket\" --output json","title":"Create Service ID"},{"location":"kubernetes/icos-sync-iks/#create-reader-access-policy-for-newly-created-service-id_1","text":"Now we will limit the scope of this service ID to have read only access to our source Object Storage bucket. $ ibmcloud iam service-policy-create <Service ID> --roles Writer --service-name cloud-object-storage --service-instance <Service Instance GUID> --resource-type bucket --resource <bucket-name> Service Instance GUID - This is the GUID of the Cloud Object Storage instance. You can retrieve this with the command: ibmcloud resource service-instance <name of icos instance>","title":"Create Reader access policy for newly created service id"},{"location":"kubernetes/icos-sync-iks/#generate-hmac-credentials-tied-to-our-service-id_1","text":"We'll follow the same procedure as last time to generate the HMAC credentials, but this time on the destination account. $ ibmcloud resource service-key-create destination-icos-service-creds Writer --instance-id <Service Instance GUID> --service-id <Service ID> --parameters '{\"HMAC\":true}' Save the access_key_id and secret_access_key as we will be using these with our Kubernetes job.","title":"Generate HMAC credentials tied to our service ID"},{"location":"kubernetes/icos-sync-iks/#create-free-ibm-cloud-kubernetes-cluster","text":"You can have 1 free cluster at a time in IBM Cloud Kubernetes Service, and each free cluster expires in 30 days. This will allow is to run the Minio sync as a Kubernetes Job. $ ibmcloud ks cluster create classic --name <name of cluster> The cluster should be ready in about 10-15 minutes. You can verify the status of the creation by running the ibmcloud ks clusters and seeing when State returns as normal : ryan@cloudshell:~$ ibmcloud ks clusters OK Name ID State Created Workers Location Version Resource Group Name Provider freeikscluster bstcs2nf0bkrt2968nm0 normal 14 minutes ago 1 par01 1 .17.9_1534 default classic","title":"Create free IBM Cloud Kubernetes cluster"},{"location":"kubernetes/icos-sync-iks/#create-kubernetes-secret-for-use-with-job","text":"In order for our container to speak to the Cloud Object storage buckets we'll need to specify some environmental variables to be stored as Kubernetes secrets . The first step is to target our free cluster: $ ibmcloud ks cluster config --cluster <name of cluster> With kubectl now configured to talk to our cluster we can create our Kubernetes secrets. $ kubectl create secret generic <name of secret> --from-literal SOURCE_ACCESS_KEY = VALUE --from-literal SOURCE_SECRET_KEY = VALUE --from-literal SOURCE_REGION = VALUE --from-literal SOURCE_BUCKET = VALUE --from-literal DESTINATION_ACCESS_KEY = VALUE --from-literal DESTINATION_SECRET_KEY = VALUE --from-literal DESTINATION_REGION = VALUE --from-literal DESTINATION_BUCKET = VALUE","title":"Create Kubernetes secret for use with job"},{"location":"kubernetes/icos-sync-iks/#download-kubernetes-job-file","text":"We specify our Kubernetes job in a yaml file hosted here on Github. $ wget https://gist.githubusercontent.com/greyhoundforty/b26d66b33f97d8368e3dd7869b7bbc5e/raw/926e7088cff7c0a991310065832d646e513890f1/job.yaml With the Job spec downloaded we can now submit the job to Kubernetes $ kubectl create -f job.yaml In my testing I am only syncing a few times so by the time I check the Kubernetes pods, the job has already completed. Looking at the logs I am able to verify that the contents have been synced between the Object Storage buckets. ryan@cloudshell:~$ kubectl get pods NAME READY STATUS RESTARTS AGE mc-icos-sync-test-c9zkq 0 /1 Completed 0 102s","title":"Download Kubernetes Job file"},{"location":"kubernetes/overview/","text":"Overview Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model. Key Components of Kubernetes Pods A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application Creating a Kubernetes Pod Use the following command to launch a simple busybox container as a Kubernetes pod: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: busybox-sleep spec: containers: - name: busybox image: busybox args: - sleep - \"1000000\" EOF Describe Pod Resource Use the kubectl describe command to get more information about our running pod: kubectl describe pod/busybox-sleep While it is trivial to launch a pod within Kubernetes in order to scale I would need to manually create new pods everytime I need the service to be more elastic. This is where Deployments come in to play. Deployments A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. Creating a Kubernetes Deployment Use the following command to launch a set of 3 nginx pods: cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-container image: nginx ports: - containerPort: 80 EOF Describe Deployment Resource Use the kubectl describe command to get more information about our new deployment: kubectl get deployment.apps/nginx-deploy Now if K8s sees that my nginx deployment only has 2 running pods, it will create an additional pod to meet the deployment specification. So I've now got my nginx containers running within my cluster, but how do outside users and other cluster resources find them? Enter Services . Services A Service is an abstract way to expose an application running on a set of Pods as a network service. Services use a single DNS name for a set of Pods, and can load-balance across them. The set of Pods targeted by a Service is usually determined by a selector (in our case app=nginx). Deploying a service cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx spec: selector: app: nginx ports: - protocol: TCP port: 80 EOF Describe Service Resource Use the kubectl describe command to get more information about our new service: kubectl describe service/nginx-svc One thing you will notice when looking at the service config is Type: ClusterIP . Kubernetes comes with 3 primary ways to expose resources via Services. If you do not specifically set a type, Kubernetes will default to ClusterIP. ClusterIP Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType. NodePort Exposes the Service on each Node's IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You'll be able to contact the NodePort Service, from outside the cluster, by requesting : . Deploying a NodePort type service cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-np labels: app: nginx spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 80 EOF Pull NodePort You can pull the assigned port using the kubect get command and jq : kubectl get svc/nginx-svc-np -o json | jq -r '.spec.ports[]' LoadBalancer Exposes the Service externally using a cloud provider's load balancer. The NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created when using Type: LoadBalancer . Deploying a LoadBalancer type service cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb labels: app: nginx spec: type: LoadBalancer selector: app: nginx ports: - protocol: TCP port: 80 EOF Find external loadBalancer IP kubectl get svc/nginx-svc-lb -o json | jq -r '.status.loadBalancer.ingress[].ip' Use curl to verify connectivity from outside the cluster to your newly created Loadbalancer service. rtiffany@cloudshell:~$ kubectl get svc/nginx-svc-lb -o json | jq -r '.status.loadBalancer.ingress[].ip' 169 .48.252.133 rtiffany@cloudshell:~$ curl 169 .48.252.133 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href = \"http://nginx.org/\" >nginx.org</a>.<br/> Commercial support is available at <a href = \"http://nginx.com/\" >nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html> Ingress Exposes HTTP/S routes from outside the cluster to services within the cluster. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer . Deploy an ingress This will deploy a an ingress that says when traffic for test-ingress.cdetesting.com hits the cluster it will be routed to the service nginx-svc within the cluster. cat <<EOF | kubectl apply -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: test-ingress.cdetesting.com http: paths: - backend: serviceName: nginx-svc servicePort: 80 path: / EOF Pointing the domain at the cluster In order to get test-ingress.cdetesting.com to resolve to my Kubernetes cluster I created the CNAME test-ingress for the cdetesting.com domain and pointed it at my IKS ingress hostname. You can find your ingress hostname by running the following command: ibmcloud ks cluster get --cluster devcluster --json | jq -r .ingressHostname Persistent Storage Namespaces Secrets Kubernetes application deployment workflow User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods \u2013 including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact. Extending Kubernetes Sidecar container: a separate container that performs its own function distinct from the application container. Istio uses a Sidecar proxy to mediate inbound and outbound communication to the workload instance it is attached to. Custom Resource Definitions: IBM Cloud Databases use CRD's to deploy to the IBM Cloud . Videos / Tutorials / Labs Kubernetes Deconstructed (very good video) IBM Kube101 IBM Istio101 Flask Python app on IKS","title":"Kubernetes 101 Overview"},{"location":"kubernetes/overview/#overview","text":"Kubernetes is a container orchestrator to provision, manage, and scale applications. In other words, Kubernetes allows you to manage the lifecycle of containerized applications within a cluster of nodes (which are a collection of worker machines, for example, VMs, physical machines etc.). Kubernetes does not have the concept of an application. It has simple building blocks that you are required to compose. Kubernetes is a cloud native platform where the internal resource model is the same as the end user resource model.","title":"Overview"},{"location":"kubernetes/overview/#key-components-of-kubernetes","text":"","title":"Key Components of Kubernetes"},{"location":"kubernetes/overview/#pods","text":"A Pod is the smallest object model that you can create and run. You can add labels to a pod to identify a subset to run operations on. When you are ready to scale your application you can use the label to tell Kubernetes which Pod you need to scale. When we talk about a application, we usually refer to group of Pods. Although an entire application can be run in a single Pod, we usually build multiple Pods that talk to each other to make a useful application Creating a Kubernetes Pod Use the following command to launch a simple busybox container as a Kubernetes pod: cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Pod metadata: name: busybox-sleep spec: containers: - name: busybox image: busybox args: - sleep - \"1000000\" EOF Describe Pod Resource Use the kubectl describe command to get more information about our running pod: kubectl describe pod/busybox-sleep While it is trivial to launch a pod within Kubernetes in order to scale I would need to manually create new pods everytime I need the service to be more elastic. This is where Deployments come in to play.","title":"Pods"},{"location":"kubernetes/overview/#deployments","text":"A Deployment provides declarative updates for Pods and ReplicaSets. You describe a desired state in a Deployment, and the Deployment Controller changes the actual state to the desired state at a controlled rate. Creating a Kubernetes Deployment Use the following command to launch a set of 3 nginx pods: cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deploy spec: replicas: 3 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx-container image: nginx ports: - containerPort: 80 EOF Describe Deployment Resource Use the kubectl describe command to get more information about our new deployment: kubectl get deployment.apps/nginx-deploy Now if K8s sees that my nginx deployment only has 2 running pods, it will create an additional pod to meet the deployment specification. So I've now got my nginx containers running within my cluster, but how do outside users and other cluster resources find them? Enter Services .","title":"Deployments"},{"location":"kubernetes/overview/#services","text":"A Service is an abstract way to expose an application running on a set of Pods as a network service. Services use a single DNS name for a set of Pods, and can load-balance across them. The set of Pods targeted by a Service is usually determined by a selector (in our case app=nginx). Deploying a service cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: nginx-svc labels: app: nginx spec: selector: app: nginx ports: - protocol: TCP port: 80 EOF Describe Service Resource Use the kubectl describe command to get more information about our new service: kubectl describe service/nginx-svc One thing you will notice when looking at the service config is Type: ClusterIP . Kubernetes comes with 3 primary ways to expose resources via Services. If you do not specifically set a type, Kubernetes will default to ClusterIP.","title":"Services"},{"location":"kubernetes/overview/#clusterip","text":"Exposes the Service on a cluster-internal IP. Choosing this value makes the Service only reachable from within the cluster. This is the default ServiceType.","title":"ClusterIP"},{"location":"kubernetes/overview/#nodeport","text":"Exposes the Service on each Node's IP at a static port (the NodePort). A ClusterIP Service, to which the NodePort Service routes, is automatically created. You'll be able to contact the NodePort Service, from outside the cluster, by requesting : . Deploying a NodePort type service cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-np labels: app: nginx spec: type: NodePort selector: app: nginx ports: - protocol: TCP port: 80 EOF Pull NodePort You can pull the assigned port using the kubect get command and jq : kubectl get svc/nginx-svc-np -o json | jq -r '.spec.ports[]'","title":"NodePort"},{"location":"kubernetes/overview/#loadbalancer","text":"Exposes the Service externally using a cloud provider's load balancer. The NodePort and ClusterIP Services, to which the external load balancer routes, are automatically created when using Type: LoadBalancer . Deploying a LoadBalancer type service cat <<EOF | kubectl apply -f - apiVersion: v1 kind: Service metadata: name: nginx-svc-lb labels: app: nginx spec: type: LoadBalancer selector: app: nginx ports: - protocol: TCP port: 80 EOF Find external loadBalancer IP kubectl get svc/nginx-svc-lb -o json | jq -r '.status.loadBalancer.ingress[].ip' Use curl to verify connectivity from outside the cluster to your newly created Loadbalancer service. rtiffany@cloudshell:~$ kubectl get svc/nginx-svc-lb -o json | jq -r '.status.loadBalancer.ingress[].ip' 169 .48.252.133 rtiffany@cloudshell:~$ curl 169 .48.252.133 <!DOCTYPE html> <html> <head> <title>Welcome to nginx!</title> <style> body { width: 35em ; margin: 0 auto ; font-family: Tahoma, Verdana, Arial, sans-serif ; } </style> </head> <body> <h1>Welcome to nginx!</h1> <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p> <p>For online documentation and support please refer to <a href = \"http://nginx.org/\" >nginx.org</a>.<br/> Commercial support is available at <a href = \"http://nginx.com/\" >nginx.com</a>.</p> <p><em>Thank you for using nginx.</em></p> </body> </html>","title":"LoadBalancer"},{"location":"kubernetes/overview/#ingress","text":"Exposes HTTP/S routes from outside the cluster to services within the cluster. An Ingress does not expose arbitrary ports or protocols. Exposing services other than HTTP and HTTPS to the internet typically uses a service of type NodePort or LoadBalancer . Deploy an ingress This will deploy a an ingress that says when traffic for test-ingress.cdetesting.com hits the cluster it will be routed to the service nginx-svc within the cluster. cat <<EOF | kubectl apply -f - apiVersion: extensions/v1beta1 kind: Ingress metadata: name: nginx-ingress spec: rules: - host: test-ingress.cdetesting.com http: paths: - backend: serviceName: nginx-svc servicePort: 80 path: / EOF Pointing the domain at the cluster In order to get test-ingress.cdetesting.com to resolve to my Kubernetes cluster I created the CNAME test-ingress for the cdetesting.com domain and pointed it at my IKS ingress hostname. You can find your ingress hostname by running the following command: ibmcloud ks cluster get --cluster devcluster --json | jq -r .ingressHostname","title":"Ingress"},{"location":"kubernetes/overview/#persistent-storage","text":"","title":"Persistent Storage"},{"location":"kubernetes/overview/#namespaces","text":"","title":"Namespaces"},{"location":"kubernetes/overview/#secrets","text":"","title":"Secrets"},{"location":"kubernetes/overview/#kubernetes-application-deployment-workflow","text":"User via \"kubectl\" deploys a new application. Kubectl sends the request to the API Server. API server receives the request and stores it in the data store (etcd). Once the request is written to data store, the API server is done with the request. Watchers detects the resource changes and send a notification to controller to act upon it Controller detects the new app and creates new pods to match the desired number# of instances. Any changes to the stored model will be picked up to create or delete Pods. Scheduler assigns new pods to a Node based on a criteria. Scheduler makes decisions to run Pods on specific Nodes in the cluster. Scheduler modifies the model with the node information. Kubelet on a node detects a pod with an assignment to itself, and deploys the requested containers via the container runtime (e.g. Docker). Each Node watches the storage to see what pods it is assigned to run. It takes necessary actions on resource assigned to it like create/delete Pods. Kubeproxy manages network traffic for the pods \u2013 including service discovery and load-balancing. Kubeproxy is responsible for communication between Pods that want to interact.","title":"Kubernetes application deployment workflow"},{"location":"kubernetes/overview/#extending-kubernetes","text":"Sidecar container: a separate container that performs its own function distinct from the application container. Istio uses a Sidecar proxy to mediate inbound and outbound communication to the workload instance it is attached to. Custom Resource Definitions: IBM Cloud Databases use CRD's to deploy to the IBM Cloud .","title":"Extending Kubernetes"},{"location":"kubernetes/overview/#videos-tutorials-labs","text":"Kubernetes Deconstructed (very good video) IBM Kube101 IBM Istio101 Flask Python app on IKS","title":"Videos / Tutorials / Labs"},{"location":"kubernetes/qs-storage/","text":"Overview IBM Cloud Kubernetes Service (IKS) Single Zone Clusters Multizone Clusters QuantaStor Prerequisites Steps Use the slcli to get the IKS Worker Node IPs Create Storage Pool in QuantaStor Create QuantaStor Network Share Enable NFS access from IKS Worker node IPs Deploy NFS provisioner Helm chart Deploy example pod and write to mounted persistent volume Overview In this guide I will be showing you how to use QuantaStor as a persistent volume provider for Kubernetes on IBM Cloud. IBM Cloud Kubernetes Service (IKS) The IBM Cloud Kubernetes service (IKS) has several options for persistent storage but they can vary greatly depending on the type of cluster that you've provisioned. Within IBM Cloud you can deploy Kubernetes Singlezone clusters or Multizone clusters. Single Zone Clusters For single zone clusters (a cluster whose worker nodes all reside in the same Datacenter) the following options are supported out of the box: - IBM Cloud File Storage - IBM Cloud Block Storage - IBM Cloud Object Storage Multizone Clusters For multizone clusters (a cluster whose worker nodes span multiple Datacenters in the same region) your options are a little more limited. Both the IBM Cloud File and Block offerings are restricted to the Datacenters in which they are ordered. So if you have a File volume provisioned in Dallas 13, your servers and IKS worker nodes in say Dallas 10 cannot actually interact with that storage. For Multizone clusters the following options are supported out of the box: IBM Cloud Object Storage Portworx IBM Cloud Databases - These are fully managed IBM Cloud hosted databases: Etcd Mongodb Elasticsearch Redis PostgreSQL Portworx is a great option for supporing Multizone persistent storage but you have to also run an instance of Etcd and it is best used with Bare Metal servers which would increase the cost of your cluster and would likely be overkill for smaller dev/test environment. As an alternative we'll set up a QuantaStor server to act as an NFS Persistent Volume storage provider. Since the storage is from the local Quantastor disks and not the IBM Cloud the volumes can span multiple Datacenters. QuantaStor QuantaStor Software Defined Storage supports all major file, block, and object protocols including iSCSI/FC, NFS/SMB, and S3. It features the ability to enable encryption at rest as well as in flight. Prerequisites Kubernetes cluster QuantaStor server. You can download the Community edition for free. QuantaStor Community Edition keys are capacity limited to 40TB of raw capacity and 4x servers per storage grid. kubectl installed. Guide here . slcli installed. Guide here . helm installed on your cluster. Guide here Steps Here are the steps we will be going through in this guide: - Use the slcli to get the IKS Worker Node IPs - Create Storage Pool in QuantaStor - Create QuantaStor Network Share - Enable NFS access from IKS Worker node IPs - Deploy NFS provisioner Helm chart - Deploy example pod and write to mounted persistent volume Use the slcli to get the IKS Worker Node IP Ranges We can gather our worker node IPs using kubectl and then use the slcli to get the subnet range that will be whitelisted for our NFS share. $ kubectl get nodes -o json | jq -r '.items[].metadata.name' 10 .184.249.15 10 .184.249.2 10 .184.249.34 10 .221.28.10 10 .221.28.37 10 .221.28.5 $ slcli --format json subnet lookup 10 .184.249.15 | jq -r '.subnet.identifier' 10 .184.249.0/26 $ slcli --format json subnet lookup 10 .221.28.37 | jq -r '.subnet.identifier' 10 .221.28.0/25 Create Storage Pool in QuantaStor Create QuantaStor Network Share Enable NFS access from IKS Worker node IPs Deploy NFS provisioner Helm chart Enter the nfs-client-provisioner helm chart $ $ helm install stable/nfs-client-provisioner --set nfs.server = 10 .171.145.12 --set nfs.path = /export/PVShare --name nfs-client NAME: nfs-client LAST DEPLOYED: Fri Jan 10 09 :09:10 2020 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == > v1/ClusterRole NAME AGE nfs-client-nfs-client-provisioner-runner 1s == > v1/ClusterRoleBinding NAME AGE run-nfs-client-nfs-client-provisioner 1s == > v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE nfs-client-nfs-client-provisioner 0 /1 0 0 1s == > v1/Role NAME AGE leader-locking-nfs-client-nfs-client-provisioner 1s == > v1/RoleBinding NAME AGE leader-locking-nfs-client-nfs-client-provisioner 1s == > v1/ServiceAccount NAME SECRETS AGE nfs-client-nfs-client-provisioner 1 2s == > v1/StorageClass NAME PROVISIONER AGE nfs-client cluster.local/nfs-client-nfs-client-provisioner 2s Check that NFS is available as a storageclass. {% highlight shell %} $ kubectl get storageclasses | grep nfs-client nfs-client cluster.local/nfs-client-nfs-client-provisioner 104s {% endhighlight %} Deploy example pod and write to mounted persistent volume Here is an example Persistent Volume Claim yaml file. qs-test-pvc.yml {% highlight yaml %} apiVersion: v1 kind: PersistentVolumeClaim metadata: name: qspvc spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 2Gi {% endhighlight %} Create pvc $ kubectl create -f qs-test-pvc.yml persistentvolumeclaim/qspvc created $ kubectl get persistentvolumeclaim/qspvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE qspvc Bound pvc-3602d021-da6d-466e-8440-c79653339ee5 2Gi RWX nfs-client 14s Here is an example pod that will write data to our new PVC. nfs-test-pod.yml kind : Pod apiVersion : v1 metadata : name : pod-using-nfs spec : volumes : - name : nfs-volume persistentVolumeClaim : claimName : qspvc containers : - name : apptest image : alpine volumeMounts : - name : nfs-volume mountPath : /var/nfs # Write to a file inside our NFS command : [ \"/bin/sh\" ] args : [ \"-c\" , \"while true; do date >> /var/nfs/dates.txt; sleep 5; done\" ] Create test pod $ kubectl create -f nfs-test-pod.yml pod/pod-using-nfs created Read from storage $ kubectl exec -it pod/pod-using-nfs -- /bin/sh / # ls -l /var/nfs/dates.txt -rw-rw-rw- 1 nobody nobody 261 Jan 10 15 :24 /var/nfs/dates.txt / # cat /var/nfs/dates.txt Fri Jan 10 15 :23:49 UTC 2020 Fri Jan 10 15 :23:54 UTC 2020 Fri Jan 10 15 :23:59 UTC 2020 Fri Jan 10 15 :24:04 UTC 2020 Fri Jan 10 15 :24:09 UTC 2020 Fri Jan 10 15 :24:14 UTC 2020 Fri Jan 10 15 :24:19 UTC 2020 Fri Jan 10 15 :24:24 UTC 2020 Fri Jan 10 15 :24:29 UTC 2020 Fri Jan 10 15 :24:34 UTC 2020","title":"Using QuantaStor as a Persistent Volume Provider for Kubernetes"},{"location":"kubernetes/qs-storage/#overview","text":"In this guide I will be showing you how to use QuantaStor as a persistent volume provider for Kubernetes on IBM Cloud.","title":"Overview"},{"location":"kubernetes/qs-storage/#ibm-cloud-kubernetes-service-iks","text":"The IBM Cloud Kubernetes service (IKS) has several options for persistent storage but they can vary greatly depending on the type of cluster that you've provisioned. Within IBM Cloud you can deploy Kubernetes Singlezone clusters or Multizone clusters.","title":"IBM Cloud Kubernetes Service (IKS)"},{"location":"kubernetes/qs-storage/#single-zone-clusters","text":"For single zone clusters (a cluster whose worker nodes all reside in the same Datacenter) the following options are supported out of the box: - IBM Cloud File Storage - IBM Cloud Block Storage - IBM Cloud Object Storage","title":"Single Zone Clusters"},{"location":"kubernetes/qs-storage/#multizone-clusters","text":"For multizone clusters (a cluster whose worker nodes span multiple Datacenters in the same region) your options are a little more limited. Both the IBM Cloud File and Block offerings are restricted to the Datacenters in which they are ordered. So if you have a File volume provisioned in Dallas 13, your servers and IKS worker nodes in say Dallas 10 cannot actually interact with that storage. For Multizone clusters the following options are supported out of the box: IBM Cloud Object Storage Portworx IBM Cloud Databases - These are fully managed IBM Cloud hosted databases: Etcd Mongodb Elasticsearch Redis PostgreSQL Portworx is a great option for supporing Multizone persistent storage but you have to also run an instance of Etcd and it is best used with Bare Metal servers which would increase the cost of your cluster and would likely be overkill for smaller dev/test environment. As an alternative we'll set up a QuantaStor server to act as an NFS Persistent Volume storage provider. Since the storage is from the local Quantastor disks and not the IBM Cloud the volumes can span multiple Datacenters.","title":"Multizone Clusters"},{"location":"kubernetes/qs-storage/#quantastor","text":"QuantaStor Software Defined Storage supports all major file, block, and object protocols including iSCSI/FC, NFS/SMB, and S3. It features the ability to enable encryption at rest as well as in flight.","title":"QuantaStor"},{"location":"kubernetes/qs-storage/#prerequisites","text":"Kubernetes cluster QuantaStor server. You can download the Community edition for free. QuantaStor Community Edition keys are capacity limited to 40TB of raw capacity and 4x servers per storage grid. kubectl installed. Guide here . slcli installed. Guide here . helm installed on your cluster. Guide here","title":"Prerequisites"},{"location":"kubernetes/qs-storage/#steps","text":"Here are the steps we will be going through in this guide: - Use the slcli to get the IKS Worker Node IPs - Create Storage Pool in QuantaStor - Create QuantaStor Network Share - Enable NFS access from IKS Worker node IPs - Deploy NFS provisioner Helm chart - Deploy example pod and write to mounted persistent volume","title":"Steps"},{"location":"kubernetes/qs-storage/#use-the-slcli-to-get-the-iks-worker-node-ip-ranges","text":"We can gather our worker node IPs using kubectl and then use the slcli to get the subnet range that will be whitelisted for our NFS share. $ kubectl get nodes -o json | jq -r '.items[].metadata.name' 10 .184.249.15 10 .184.249.2 10 .184.249.34 10 .221.28.10 10 .221.28.37 10 .221.28.5 $ slcli --format json subnet lookup 10 .184.249.15 | jq -r '.subnet.identifier' 10 .184.249.0/26 $ slcli --format json subnet lookup 10 .221.28.37 | jq -r '.subnet.identifier' 10 .221.28.0/25","title":"Use the slcli to get the IKS Worker Node IP Ranges"},{"location":"kubernetes/qs-storage/#create-storage-pool-in-quantastor","text":"","title":"Create Storage Pool in QuantaStor"},{"location":"kubernetes/qs-storage/#create-quantastor-network-share","text":"","title":"Create QuantaStor Network Share"},{"location":"kubernetes/qs-storage/#enable-nfs-access-from-iks-worker-node-ips","text":"","title":"Enable NFS access from IKS Worker node IPs"},{"location":"kubernetes/qs-storage/#deploy-nfs-provisioner-helm-chart","text":"Enter the nfs-client-provisioner helm chart $ $ helm install stable/nfs-client-provisioner --set nfs.server = 10 .171.145.12 --set nfs.path = /export/PVShare --name nfs-client NAME: nfs-client LAST DEPLOYED: Fri Jan 10 09 :09:10 2020 NAMESPACE: default STATUS: DEPLOYED RESOURCES: == > v1/ClusterRole NAME AGE nfs-client-nfs-client-provisioner-runner 1s == > v1/ClusterRoleBinding NAME AGE run-nfs-client-nfs-client-provisioner 1s == > v1/Deployment NAME READY UP-TO-DATE AVAILABLE AGE nfs-client-nfs-client-provisioner 0 /1 0 0 1s == > v1/Role NAME AGE leader-locking-nfs-client-nfs-client-provisioner 1s == > v1/RoleBinding NAME AGE leader-locking-nfs-client-nfs-client-provisioner 1s == > v1/ServiceAccount NAME SECRETS AGE nfs-client-nfs-client-provisioner 1 2s == > v1/StorageClass NAME PROVISIONER AGE nfs-client cluster.local/nfs-client-nfs-client-provisioner 2s Check that NFS is available as a storageclass. {% highlight shell %} $ kubectl get storageclasses | grep nfs-client nfs-client cluster.local/nfs-client-nfs-client-provisioner 104s {% endhighlight %}","title":"Deploy NFS provisioner Helm chart"},{"location":"kubernetes/qs-storage/#deploy-example-pod-and-write-to-mounted-persistent-volume","text":"Here is an example Persistent Volume Claim yaml file. qs-test-pvc.yml {% highlight yaml %} apiVersion: v1 kind: PersistentVolumeClaim metadata: name: qspvc spec: storageClassName: nfs-client accessModes: - ReadWriteMany resources: requests: storage: 2Gi {% endhighlight %} Create pvc $ kubectl create -f qs-test-pvc.yml persistentvolumeclaim/qspvc created $ kubectl get persistentvolumeclaim/qspvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE qspvc Bound pvc-3602d021-da6d-466e-8440-c79653339ee5 2Gi RWX nfs-client 14s Here is an example pod that will write data to our new PVC. nfs-test-pod.yml kind : Pod apiVersion : v1 metadata : name : pod-using-nfs spec : volumes : - name : nfs-volume persistentVolumeClaim : claimName : qspvc containers : - name : apptest image : alpine volumeMounts : - name : nfs-volume mountPath : /var/nfs # Write to a file inside our NFS command : [ \"/bin/sh\" ] args : [ \"-c\" , \"while true; do date >> /var/nfs/dates.txt; sleep 5; done\" ] Create test pod $ kubectl create -f nfs-test-pod.yml pod/pod-using-nfs created Read from storage $ kubectl exec -it pod/pod-using-nfs -- /bin/sh / # ls -l /var/nfs/dates.txt -rw-rw-rw- 1 nobody nobody 261 Jan 10 15 :24 /var/nfs/dates.txt / # cat /var/nfs/dates.txt Fri Jan 10 15 :23:49 UTC 2020 Fri Jan 10 15 :23:54 UTC 2020 Fri Jan 10 15 :23:59 UTC 2020 Fri Jan 10 15 :24:04 UTC 2020 Fri Jan 10 15 :24:09 UTC 2020 Fri Jan 10 15 :24:14 UTC 2020 Fri Jan 10 15 :24:19 UTC 2020 Fri Jan 10 15 :24:24 UTC 2020 Fri Jan 10 15 :24:29 UTC 2020 Fri Jan 10 15 :24:34 UTC 2020","title":"Deploy example pod and write to mounted persistent volume"},{"location":"python-examples/","text":"","title":"IBM Cloud Python Examples"},{"location":"python-examples/Containers/","text":"","title":"Containers"},{"location":"python-examples/Containers/container-registry/","text":"Container Registry Examples","title":"Container Registry Examples"},{"location":"python-examples/Containers/container-registry/#container-registry-examples","text":"","title":"Container Registry Examples"},{"location":"python-examples/Containers/va/","text":"Vulnerability Advisor Examples List the Vulnerability Advisor status for all container images import os from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException from ibm_container_registry.vulnerability_advisor_v3 import * ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) account = os . environ . get ( 'ACCOUNT_ID' ) accept_language = 'en_US' vulnerabilityAdvisorService = VulnerabilityAdvisorV3 ( authenticator = authenticator , account = account ) def get_all_scan_results ( vulnerabilityAdvisorService ): scanreport = vulnerabilityAdvisorService . account_status_query_path () . get_result () image_report = scanreport [ 'images' ] for image in image_report : print ( \"Container Image: \" + image [ 'name' ] + \" was marked as \" + image [ 'status' ] + \" in the most recent VA scan.\" ) try : get_all_scan_results ( vulnerabilityAdvisorService ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ]) Example Output Image: us.icr.io/rtiffany/nodejscloudantyzruj20211019:1-master-d45c0dba-20211019112141 VA Status: FAIL Last Scan: 2022 -04-22 18 :27:05 Image: us.icr.io/rtiffany/rclone-sync:8 VA Status: FAIL Last Scan: 2022 -04-24 23 :04:13 Image: us.icr.io/rtiffany/pythonbase-invoice:1 VA Status: FAIL Last Scan: 2022 -04-24 03 :05:57 Image: us.icr.io/rtiffany/cde-mkdocs:1 VA Status: FAIL Last Scan: 2022 -04-22 16 :44:34 Get latest scan results for a container image import os import json from pprint import pprint from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException from ibm_container_registry.vulnerability_advisor_v3 import * ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) imageName = \"icr.io/YOUR-NAMESPACE/IMAGE_NAME:TAG\" account = os . environ . get ( 'ACCOUNT_ID' ) accept_language = 'en_US' vulnerabilityAdvisorService = VulnerabilityAdvisorV3 ( authenticator = authenticator , account = account ) def va_scan_report ( vulnerabilityAdvisorService , imageName ): scan_report = vulnerabilityAdvisorService . image_report_query_path ( name = imageName ) . get_result () print ( json . dumps ( scan_report , indent = 2 )) try : va_scan_report ( vulnerabilityAdvisorService , imageName = imageName ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ])","title":"Vulnerability Advisor Examples"},{"location":"python-examples/Containers/va/#vulnerability-advisor-examples","text":"","title":"Vulnerability Advisor Examples"},{"location":"python-examples/Containers/va/#list-the-vulnerability-advisor-status-for-all-container-images","text":"import os from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException from ibm_container_registry.vulnerability_advisor_v3 import * ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) account = os . environ . get ( 'ACCOUNT_ID' ) accept_language = 'en_US' vulnerabilityAdvisorService = VulnerabilityAdvisorV3 ( authenticator = authenticator , account = account ) def get_all_scan_results ( vulnerabilityAdvisorService ): scanreport = vulnerabilityAdvisorService . account_status_query_path () . get_result () image_report = scanreport [ 'images' ] for image in image_report : print ( \"Container Image: \" + image [ 'name' ] + \" was marked as \" + image [ 'status' ] + \" in the most recent VA scan.\" ) try : get_all_scan_results ( vulnerabilityAdvisorService ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ])","title":"List the Vulnerability Advisor status for all container images"},{"location":"python-examples/Containers/va/#example-output","text":"Image: us.icr.io/rtiffany/nodejscloudantyzruj20211019:1-master-d45c0dba-20211019112141 VA Status: FAIL Last Scan: 2022 -04-22 18 :27:05 Image: us.icr.io/rtiffany/rclone-sync:8 VA Status: FAIL Last Scan: 2022 -04-24 23 :04:13 Image: us.icr.io/rtiffany/pythonbase-invoice:1 VA Status: FAIL Last Scan: 2022 -04-24 03 :05:57 Image: us.icr.io/rtiffany/cde-mkdocs:1 VA Status: FAIL Last Scan: 2022 -04-22 16 :44:34","title":"Example Output"},{"location":"python-examples/Containers/va/#get-latest-scan-results-for-a-container-image","text":"import os import json from pprint import pprint from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException from ibm_container_registry.vulnerability_advisor_v3 import * ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) imageName = \"icr.io/YOUR-NAMESPACE/IMAGE_NAME:TAG\" account = os . environ . get ( 'ACCOUNT_ID' ) accept_language = 'en_US' vulnerabilityAdvisorService = VulnerabilityAdvisorV3 ( authenticator = authenticator , account = account ) def va_scan_report ( vulnerabilityAdvisorService , imageName ): scan_report = vulnerabilityAdvisorService . image_report_query_path ( name = imageName ) . get_result () print ( json . dumps ( scan_report , indent = 2 )) try : va_scan_report ( vulnerabilityAdvisorService , imageName = imageName ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ])","title":"Get latest scan results for a container image"},{"location":"python-examples/IAM/","text":"Configuring Environment The examples in this IAM section use the IBM Cloud Platform Services Python SDK which can be installed using the command: pip install --upgrade \"ibm-platform-services\" You will also need to export the following environment variables if they are not already set: export ACCOUNT_ID = 'YOUR ACCOUNT ID' export IBMCLOUD_API_KEY = 'YOUR IBM CLOUD API KEY ' APIs used in these examples IAM Access Groups Service IDs Endpoints The IAM APIs uses the following public global endpoint URL: https://iam.cloud.ibm.com If you enabled service endpoints in your account, you can send API requests over the IBM Cloud private network at the following base endpoint URLs. Private endpoint URL for VPC infrastructure: https://private.iam.cloud.ibm.com Private endpoint URLs for classic infrastructure: Dallas: https://private.us-south.iam.cloud.ibm.com Washington DC: https://private.us-east.iam.cloud.ibm.com","title":"Configuring Environment"},{"location":"python-examples/IAM/#configuring-environment","text":"The examples in this IAM section use the IBM Cloud Platform Services Python SDK which can be installed using the command: pip install --upgrade \"ibm-platform-services\" You will also need to export the following environment variables if they are not already set: export ACCOUNT_ID = 'YOUR ACCOUNT ID' export IBMCLOUD_API_KEY = 'YOUR IBM CLOUD API KEY '","title":"Configuring Environment"},{"location":"python-examples/IAM/#apis-used-in-these-examples","text":"IAM Access Groups Service IDs","title":"APIs used in these examples"},{"location":"python-examples/IAM/#endpoints","text":"The IAM APIs uses the following public global endpoint URL: https://iam.cloud.ibm.com If you enabled service endpoints in your account, you can send API requests over the IBM Cloud private network at the following base endpoint URLs. Private endpoint URL for VPC infrastructure: https://private.iam.cloud.ibm.com Private endpoint URLs for classic infrastructure: Dallas: https://private.us-south.iam.cloud.ibm.com Washington DC: https://private.us-east.iam.cloud.ibm.com","title":"Endpoints"},{"location":"python-examples/IAM/access-groups/","text":"Access Group Examples The examples in this document use the IAM Access Groups Python SDK. Create New Access Group The following example will create a new Access Group named python-test-access-group : import os import json from ibm_platform_services import IamAccessGroupsV2 from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException ## Pull Account ID from Environment variable account_id = os . environ . get ( 'ACCOUNT_ID' ) ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) accessGroupService = IamAccessGroupsV2 ( authenticator = authenticator ) def create_access_group ( accessGroupService ): print ( \"Creating new access group:\" ) acessGroup = accessGroupService . create_access_group ( name = 'python-test-access-group' , account_id = account_id , description = 'Example Access Group created using the Python SDK' ) . get_result () newAccessGroup = acessGroup print ( json . dumps ( newAccessGroup , indent = 2 )) try : create_access_group ( accessGroupService ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ]) Example Output Creating new access group: { \"id\" : \"AccessGroupId-xxxxxxx-2d2f-4e30-xxxxxxx-bca01772f49a\" , \"name\" : \"cool-project-access-group\" , \"description\" : \"Example Access Group created using the Python SDK for our cool new project\" , \"account_id\" : \"xxxxxxx\" , \"created_at\" : \"2022-04-25T21:07:07Z\" , \"created_by_id\" : \"IBMid-xxxxxxx\" , \"last_modified_at\" : \"2022-04-25T21:07:07Z\" , \"last_modified_by_id\" : \"IBMid-xxxxxxx\" } List All Access Groups List all of the Access Groups on the account: import os import json from ibm_platform_services import IamAccessGroupsV2 from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException ## Pull Account ID from Environment variable account_id = os . environ . get ( 'ACCOUNT_ID' ) ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( '' )) accessGroupService = IamAccessGroupsV2 ( authenticator = authenticator ) def list_access_groups ( accessGroupService ): groupsList = accessGroupService . list_access_groups ( account_id = account_id , limit = 100 ) . get_result () . get ( \"groups\" ) for group in groupsList : print ( json . dumps ( group , indent = 2 )) try : list_access_groups ( accessGroupService ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ]) Get Access Group You will need to set the Access Group ID in the script. Replace Your-Access-Group-ID-Here with the actual Access Group ID. import os import json from ibm_platform_services import IamAccessGroupsV2 from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException ## Pull Account ID from Environment variable account_id = os . environ . get ( 'ACCOUNT_ID' ) # Set access group used in the get_access_group function access_group_id = 'Your-Access-Group-ID-Here' ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) accessGroupService = IamAccessGroupsV2 ( authenticator = authenticator ) def get_access_group ( accessGroupService , access_group_id ): accessGroup = accessGroupService . get_access_group ( access_group_id ) . get_result () ag = accessGroup print ( json . dumps ( ag , indent = 2 )) try : get_access_group ( accessGroupService , access_group_id = access_group_id ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ]) Example Output { \"id\" : \"AccessGroupId-xxxxxxx-1471-440b-xxxxxxx-xxxxxxx\" , \"name\" : \"CDE VPC Infrastructure\" , \"description\" : \"Access to VPC related resources in the CDE Resource Group\" , \"account_id\" : \"xxxxxxx\" , \"created_at\" : \"2020-11-06T14:27:16Z\" , \"created_by_id\" : \"IBMid-xxxxxxx\" , \"last_modified_at\" : \"2021-07-23T16:54:39Z\" , \"last_modified_by_id\" : \"IBMid-xxxxxxx\" }","title":"Access Group Examples"},{"location":"python-examples/IAM/access-groups/#access-group-examples","text":"The examples in this document use the IAM Access Groups Python SDK.","title":"Access Group Examples"},{"location":"python-examples/IAM/access-groups/#create-new-access-group","text":"The following example will create a new Access Group named python-test-access-group : import os import json from ibm_platform_services import IamAccessGroupsV2 from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException ## Pull Account ID from Environment variable account_id = os . environ . get ( 'ACCOUNT_ID' ) ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) accessGroupService = IamAccessGroupsV2 ( authenticator = authenticator ) def create_access_group ( accessGroupService ): print ( \"Creating new access group:\" ) acessGroup = accessGroupService . create_access_group ( name = 'python-test-access-group' , account_id = account_id , description = 'Example Access Group created using the Python SDK' ) . get_result () newAccessGroup = acessGroup print ( json . dumps ( newAccessGroup , indent = 2 )) try : create_access_group ( accessGroupService ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ])","title":"Create New Access Group"},{"location":"python-examples/IAM/access-groups/#example-output","text":"Creating new access group: { \"id\" : \"AccessGroupId-xxxxxxx-2d2f-4e30-xxxxxxx-bca01772f49a\" , \"name\" : \"cool-project-access-group\" , \"description\" : \"Example Access Group created using the Python SDK for our cool new project\" , \"account_id\" : \"xxxxxxx\" , \"created_at\" : \"2022-04-25T21:07:07Z\" , \"created_by_id\" : \"IBMid-xxxxxxx\" , \"last_modified_at\" : \"2022-04-25T21:07:07Z\" , \"last_modified_by_id\" : \"IBMid-xxxxxxx\" }","title":"Example Output"},{"location":"python-examples/IAM/access-groups/#list-all-access-groups","text":"List all of the Access Groups on the account: import os import json from ibm_platform_services import IamAccessGroupsV2 from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException ## Pull Account ID from Environment variable account_id = os . environ . get ( 'ACCOUNT_ID' ) ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( '' )) accessGroupService = IamAccessGroupsV2 ( authenticator = authenticator ) def list_access_groups ( accessGroupService ): groupsList = accessGroupService . list_access_groups ( account_id = account_id , limit = 100 ) . get_result () . get ( \"groups\" ) for group in groupsList : print ( json . dumps ( group , indent = 2 )) try : list_access_groups ( accessGroupService ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ])","title":"List All Access Groups"},{"location":"python-examples/IAM/access-groups/#get-access-group","text":"You will need to set the Access Group ID in the script. Replace Your-Access-Group-ID-Here with the actual Access Group ID. import os import json from ibm_platform_services import IamAccessGroupsV2 from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException ## Pull Account ID from Environment variable account_id = os . environ . get ( 'ACCOUNT_ID' ) # Set access group used in the get_access_group function access_group_id = 'Your-Access-Group-ID-Here' ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) accessGroupService = IamAccessGroupsV2 ( authenticator = authenticator ) def get_access_group ( accessGroupService , access_group_id ): accessGroup = accessGroupService . get_access_group ( access_group_id ) . get_result () ag = accessGroup print ( json . dumps ( ag , indent = 2 )) try : get_access_group ( accessGroupService , access_group_id = access_group_id ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ])","title":"Get Access Group"},{"location":"python-examples/IAM/access-groups/#example-output_1","text":"{ \"id\" : \"AccessGroupId-xxxxxxx-1471-440b-xxxxxxx-xxxxxxx\" , \"name\" : \"CDE VPC Infrastructure\" , \"description\" : \"Access to VPC related resources in the CDE Resource Group\" , \"account_id\" : \"xxxxxxx\" , \"created_at\" : \"2020-11-06T14:27:16Z\" , \"created_by_id\" : \"IBMid-xxxxxxx\" , \"last_modified_at\" : \"2021-07-23T16:54:39Z\" , \"last_modified_by_id\" : \"IBMid-xxxxxxx\" }","title":"Example Output"},{"location":"python-examples/Interconnectivity/","text":"Interconnectivity Examples The Examples in this section use the IBM Cloud Networking Services module. This can be installed using the following command: pip install --upgrade \"ibm-cloud-networking-services\"","title":"Interconnectivity Examples"},{"location":"python-examples/Interconnectivity/#interconnectivity-examples","text":"The Examples in this section use the IBM Cloud Networking Services module. This can be installed using the following command: pip install --upgrade \"ibm-cloud-networking-services\"","title":"Interconnectivity Examples"},{"location":"python-examples/VPC/","text":"Configuring Environment The examples in this IAM section use the IBM Cloud Platform Services Python SDK which can be installed using the command: pip install --upgrade \"ibm-vpc\" You will also need to export the following environment variables if they are not already set: export VPC_REGION = 'REGION FOR YOUR VPC RESOURCES' export IBMCLOUD_API_KEY = 'YOUR IBM CLOUD API KEY' export RESOURCE_GROUP = ' NAME OF RESOURCE_GROUP TO USE FOR DEPLOYMENTS \" Versioning Calls to the VPC API require a major version as the first segment of the request path ( ex: /v1/ ) and a date-based version as a query parameter in the format version=YYYY-MM-DD . For safety I set this to one day behind the current date using the datetime module: from datetime import datetime , timedelta today = datetime . now () date = today + timedelta ( days = - 1 ) version_date = date . strftime ( \"%Y-%m- %d \" ) ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) ## Construct the VPC service and set the regional endpoint vpcService = VpcV1 ( authenticator = authenticator , version = version_date , ) vpcServiceRegion = 'https://' + os . environ . get ( 'VPC_REGION' ) + '.iaas.cloud.ibm.com/v1' vpcService . set_service_url ( vpcServiceRegion ) Endpoints VPC uses region specific API endpoints: US South ( us-south ): https://us-south.iaas.cloud.ibm.com/v1 US East ( us-east ): https://us-east.iaas.cloud.ibm.com/v1 Toronto ( ca-tor ): https://ca-tor.iaas.cloud.ibm.com/v1 United Kingdom ( eu-gb ): https://eu-gb.iaas.cloud.ibm.com/v1 Germany ( eu-de ): https://eu-de.iaas.cloud.ibm.com/v1 Tokyo ( jp-tok ): https://jp-tok.iaas.cloud.ibm.com/v1 Osaka ( jp-osa ): https://jp-osa.iaas.cloud.ibm.com/v1 Sydney ( au-syd ): https://au-syd.iaas.cloud.ibm.com/v1 S\u00e3o Paulo ( br-sao ): https://br-sao.iaas.cloud.ibm.com/v1","title":"Configuring Environment"},{"location":"python-examples/VPC/#configuring-environment","text":"The examples in this IAM section use the IBM Cloud Platform Services Python SDK which can be installed using the command: pip install --upgrade \"ibm-vpc\" You will also need to export the following environment variables if they are not already set: export VPC_REGION = 'REGION FOR YOUR VPC RESOURCES' export IBMCLOUD_API_KEY = 'YOUR IBM CLOUD API KEY' export RESOURCE_GROUP = ' NAME OF RESOURCE_GROUP TO USE FOR DEPLOYMENTS \"","title":"Configuring Environment"},{"location":"python-examples/VPC/#versioning","text":"Calls to the VPC API require a major version as the first segment of the request path ( ex: /v1/ ) and a date-based version as a query parameter in the format version=YYYY-MM-DD . For safety I set this to one day behind the current date using the datetime module: from datetime import datetime , timedelta today = datetime . now () date = today + timedelta ( days = - 1 ) version_date = date . strftime ( \"%Y-%m- %d \" ) ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) ## Construct the VPC service and set the regional endpoint vpcService = VpcV1 ( authenticator = authenticator , version = version_date , ) vpcServiceRegion = 'https://' + os . environ . get ( 'VPC_REGION' ) + '.iaas.cloud.ibm.com/v1' vpcService . set_service_url ( vpcServiceRegion )","title":"Versioning"},{"location":"python-examples/VPC/#endpoints","text":"VPC uses region specific API endpoints: US South ( us-south ): https://us-south.iaas.cloud.ibm.com/v1 US East ( us-east ): https://us-east.iaas.cloud.ibm.com/v1 Toronto ( ca-tor ): https://ca-tor.iaas.cloud.ibm.com/v1 United Kingdom ( eu-gb ): https://eu-gb.iaas.cloud.ibm.com/v1 Germany ( eu-de ): https://eu-de.iaas.cloud.ibm.com/v1 Tokyo ( jp-tok ): https://jp-tok.iaas.cloud.ibm.com/v1 Osaka ( jp-osa ): https://jp-osa.iaas.cloud.ibm.com/v1 Sydney ( au-syd ): https://au-syd.iaas.cloud.ibm.com/v1 S\u00e3o Paulo ( br-sao ): https://br-sao.iaas.cloud.ibm.com/v1","title":"Endpoints"},{"location":"python-examples/VPC/create-single-zone-vpc/","text":"Single Zone VPC The following example will deploy a new VPC as well as a Public Gateway and Subnet in a single zone within the region. import os import json from pprint import pprint from ibm_vpc import VpcV1 from ibm_platform_services import ResourceManagerV2 from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException from haikunator import Haikunator from datetime import datetime , timedelta today = datetime . now () date = today + timedelta ( days = - 1 ) version_date = date . strftime ( \"%Y-%m- %d \" ) ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) resourceService = ResourceManagerV2 ( authenticator = authenticator ) resource_group = ( os . environ . get ( 'RESOURCE_GROUP' )) resource_group_list = resourceService . list_resource_groups ( account_id = ( os . environ . get ( 'IBM_ACCOUNT' )), include_deleted = False , ) . get_result () rglist = resource_group_list [ 'resources' ] rg_id = rglist [ 'name' == resource_group ][ 'id' ] ## Construct the VPC service and set the regional endpoint vpcService = VpcV1 ( authenticator = authenticator ) vpcServiceRegion = 'https://' + os . environ . get ( 'VPC_REGION' ) + '.iaas.cloud.ibm.com/v1' vpcService . set_service_url ( vpcServiceRegion ) ## Pull zones based on region. Set deployment zone to first in region by default. ## Todo: based on length of zones returned, create that number of pubgws and subnets zones = vpcService . list_region_zones ( os . environ . get ( 'VPC_REGION' )) . get_result ()[ 'zones' ] deployment_zone = zones [ 0 ][ 'name' ] ## Use Haikunator to generate a unique heroku like base name for resources. ## Handy while testing haikunator = Haikunator () basename = haikunator . haikunate ( token_length = 0 , delimiter = '' ) ## Set up some parameters that will be used by multiple functions resource_group_identity_model = {} resource_group_identity_model [ 'id' ] = rg_id resource_group_id = resource_group_identity_model zone_identity_model = {} zone_identity_model [ 'name' ] = deployment_zone zone = zone_identity_model def create_vpc ( vpcService ): address_prefix_management = 'auto' classic_access = False name = ( basename + \"-vpc\" ) new_vpc = vpcService . create_vpc ( classic_access = classic_access , address_prefix_management = address_prefix_management , name = name , resource_group = resource_group_id ) . get_result () print ( \" \\n Creating IBM Cloud VPC in \" + os . environ . get ( 'VPC_REGION' ) + \" ---- \\n \" ) newVpc = create_vpc ( vpcService ) print ( \"Creation Complete. VPC Info: ---- \\n Name: \" + newVpc [ 'name' ] + \" \\n ID: \" + newVpc [ 'id' ] + \" \\n CRN: \" + newVpc [ 'crn' ] + \" ---- \\n ---- \\n \" ) print ( newVpc [ 'default_network_acl' ][ 'id' ]) return new_vpc def create_pubgw ( vpcService ): vpc_identity_model = { 'id' : newVpc [ 'id' ]} vpc = vpc_identity_model zone = zone_identity_model name = ( basename + \"-\" + deployment_zone + \"-gw\" ) new_pubgw = vpcService . create_public_gateway ( vpc , zone , name = name , floating_ip = {}, resource_group = resource_group_id ) . get_result () newPubGw = create_pubgw ( vpcService ) print ( \"Creation Complete. Pubgw Info: ---- \\n Name: \" + newPubGw [ 'name' ] + \" \\n ID: \" + newPubGw [ 'id' ] + \" \\n CRN: \" + newPubGw [ 'crn' ]) return new_pubgw def create_subnet ( vpcService ): vpc_identity_model = { 'id' : newVpc [ 'id' ]} vpc = vpc_identity_model zone = zone_identity_model name = ( basename + \"-\" + deployment_zone + \"-subnet\" ) network_acl_identity_model = {} network_acl_identity_model [ 'id' ] = newVpc [ 'default_network_acl' ][ 'id' ] public_gateway_identity_model = {} public_gateway_identity_model [ 'id' ] = newPubGw [ 'id' ] subnet_prototype_model = {} subnet_prototype_model [ 'ip_version' ] = 'ipv4' subnet_prototype_model [ 'name' ] = name subnet_prototype_model [ 'network_acl' ] = network_acl_identity_model subnet_prototype_model [ 'public_gateway' ] = public_gateway_identity_model subnet_prototype_model [ 'resource_group' ] = resource_group_identity_model subnet_prototype_model [ 'vpc' ] = vpc subnet_prototype_model [ 'total_ipv4_address_count' ] = 256 subnet_prototype_model [ 'zone' ] = zone subnet_prototype = subnet_prototype_model new_subnet = vpcService . create_subnet ( subnet_prototype ) . get_result () print ( \" \\n Creating VPC Subnet in \" + deployment_zone + \" ---- \\n \" ) newSubnet = create_subnet ( vpcService ) print ( \"Creation Complete. Subnet Info: ---- \\n Name: \" + newSubnet [ 'name' ] + \" \\n ID: \" + newSubnet [ 'id' ] + \" \\n CRN: \" + newSubnet [ 'crn' ]) return new_subnet try : create_vpc ( vpcService ) create_pubgw ( vpcService ) create_subnet ( vpcService ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ])","title":"Single Zone VPC"},{"location":"python-examples/VPC/create-single-zone-vpc/#single-zone-vpc","text":"The following example will deploy a new VPC as well as a Public Gateway and Subnet in a single zone within the region. import os import json from pprint import pprint from ibm_vpc import VpcV1 from ibm_platform_services import ResourceManagerV2 from ibm_cloud_sdk_core.authenticators import IAMAuthenticator from ibm_cloud_sdk_core import ApiException from haikunator import Haikunator from datetime import datetime , timedelta today = datetime . now () date = today + timedelta ( days = - 1 ) version_date = date . strftime ( \"%Y-%m- %d \" ) ## Construct IAM Authentication using IBMCLOUD_API_KEY Environment variable authenticator = IAMAuthenticator ( os . environ . get ( 'IBMCLOUD_API_KEY' )) resourceService = ResourceManagerV2 ( authenticator = authenticator ) resource_group = ( os . environ . get ( 'RESOURCE_GROUP' )) resource_group_list = resourceService . list_resource_groups ( account_id = ( os . environ . get ( 'IBM_ACCOUNT' )), include_deleted = False , ) . get_result () rglist = resource_group_list [ 'resources' ] rg_id = rglist [ 'name' == resource_group ][ 'id' ] ## Construct the VPC service and set the regional endpoint vpcService = VpcV1 ( authenticator = authenticator ) vpcServiceRegion = 'https://' + os . environ . get ( 'VPC_REGION' ) + '.iaas.cloud.ibm.com/v1' vpcService . set_service_url ( vpcServiceRegion ) ## Pull zones based on region. Set deployment zone to first in region by default. ## Todo: based on length of zones returned, create that number of pubgws and subnets zones = vpcService . list_region_zones ( os . environ . get ( 'VPC_REGION' )) . get_result ()[ 'zones' ] deployment_zone = zones [ 0 ][ 'name' ] ## Use Haikunator to generate a unique heroku like base name for resources. ## Handy while testing haikunator = Haikunator () basename = haikunator . haikunate ( token_length = 0 , delimiter = '' ) ## Set up some parameters that will be used by multiple functions resource_group_identity_model = {} resource_group_identity_model [ 'id' ] = rg_id resource_group_id = resource_group_identity_model zone_identity_model = {} zone_identity_model [ 'name' ] = deployment_zone zone = zone_identity_model def create_vpc ( vpcService ): address_prefix_management = 'auto' classic_access = False name = ( basename + \"-vpc\" ) new_vpc = vpcService . create_vpc ( classic_access = classic_access , address_prefix_management = address_prefix_management , name = name , resource_group = resource_group_id ) . get_result () print ( \" \\n Creating IBM Cloud VPC in \" + os . environ . get ( 'VPC_REGION' ) + \" ---- \\n \" ) newVpc = create_vpc ( vpcService ) print ( \"Creation Complete. VPC Info: ---- \\n Name: \" + newVpc [ 'name' ] + \" \\n ID: \" + newVpc [ 'id' ] + \" \\n CRN: \" + newVpc [ 'crn' ] + \" ---- \\n ---- \\n \" ) print ( newVpc [ 'default_network_acl' ][ 'id' ]) return new_vpc def create_pubgw ( vpcService ): vpc_identity_model = { 'id' : newVpc [ 'id' ]} vpc = vpc_identity_model zone = zone_identity_model name = ( basename + \"-\" + deployment_zone + \"-gw\" ) new_pubgw = vpcService . create_public_gateway ( vpc , zone , name = name , floating_ip = {}, resource_group = resource_group_id ) . get_result () newPubGw = create_pubgw ( vpcService ) print ( \"Creation Complete. Pubgw Info: ---- \\n Name: \" + newPubGw [ 'name' ] + \" \\n ID: \" + newPubGw [ 'id' ] + \" \\n CRN: \" + newPubGw [ 'crn' ]) return new_pubgw def create_subnet ( vpcService ): vpc_identity_model = { 'id' : newVpc [ 'id' ]} vpc = vpc_identity_model zone = zone_identity_model name = ( basename + \"-\" + deployment_zone + \"-subnet\" ) network_acl_identity_model = {} network_acl_identity_model [ 'id' ] = newVpc [ 'default_network_acl' ][ 'id' ] public_gateway_identity_model = {} public_gateway_identity_model [ 'id' ] = newPubGw [ 'id' ] subnet_prototype_model = {} subnet_prototype_model [ 'ip_version' ] = 'ipv4' subnet_prototype_model [ 'name' ] = name subnet_prototype_model [ 'network_acl' ] = network_acl_identity_model subnet_prototype_model [ 'public_gateway' ] = public_gateway_identity_model subnet_prototype_model [ 'resource_group' ] = resource_group_identity_model subnet_prototype_model [ 'vpc' ] = vpc subnet_prototype_model [ 'total_ipv4_address_count' ] = 256 subnet_prototype_model [ 'zone' ] = zone subnet_prototype = subnet_prototype_model new_subnet = vpcService . create_subnet ( subnet_prototype ) . get_result () print ( \" \\n Creating VPC Subnet in \" + deployment_zone + \" ---- \\n \" ) newSubnet = create_subnet ( vpcService ) print ( \"Creation Complete. Subnet Info: ---- \\n Name: \" + newSubnet [ 'name' ] + \" \\n ID: \" + newSubnet [ 'id' ] + \" \\n CRN: \" + newSubnet [ 'crn' ]) return new_subnet try : create_vpc ( vpcService ) create_pubgw ( vpcService ) create_subnet ( vpcService ) except ApiException as ae : print ( \"Method failed\" ) print ( \" - status code: \" + str ( ae . code )) print ( \" - error message: \" + ae . message ) if ( \"reason\" in ae . http_response . json ()): print ( \" - reason: \" + ae . http_response . json ()[ \"reason\" ])","title":"Single Zone VPC"},{"location":"terraform/","text":"layout: default title: Terraform Examples nav_order: 16 has_children: true","title":"Index"},{"location":"terraform/ibm-gitlab/","tags":"gitlab,ibmcloud,terraform","text":"Overview Today I will be walking you through how to set up Environmental Variables and a .gitlab-ci.yml file to deploy IBM Cloud resources using Terraform and the Gitlab CI/CD. Setting CI/CD Variables in Gitlab All of my automated IBM Cloud Terraform projects land under the same Gitlab group . There are many reasons to use Groups in Gitlab but for me it is mainly so that I don't have to set per-project Environmental Variables. Step 1: From the top navigation bar click on Groups and select Your Groups . Step 2: Go to your Gitlab Group page and from the left hand navigation click Settings > CI/CD. Step 3 Click Expand in the Variables section The variables you will want to set are: IC_API_KEY = Your IBM Cloud API Key SL_API_KEY = Your IBM Cloud IaaS (SoftLayer) API Key SL_USERNAME = Your IBM Cloud IaaS (SoftLayer) username Check the Masked option for the variables. Setting the masked variable option means that the value of the variables will be hidden in job logs during the CI/CD runs. When you're variables are set click Save . A Note About Remote States : It is highly recommended to use a remote state for your Terraform deployments. With remote state, Terraform writes the state data to a remote data store, which can then be shared between all members of a team. If you will be using a remote state for your Terraform backend make sure you set the appropriate environmental variables for the backend provider. For instance if you are using the Consul backend provider you would want to set the CONSUL_TOKEN and CONSUL_HTTP_ADDR environmental variables. Test Gitlab Automation In order to test our Gitlab automation let's deploy a single Ubuntu 18 Virtual Instance. The first step is to create new Project in Github. When creating the project click the Import project tab and click on Repo by URL . Under the Git Repository URL section enter https://git.cloud-design.dev/ryan/ibm-tf-gitlab-example.git . Give your newly imported project a name, set it's visibility level and the click Create Project . After a few moments Gitlab will create the new project. Once the project has been created you can dive in to the code to tweak the example deployment. You will need to do the following at the very least: Update the main.tf file with the name of your IaaS SSH Key in the data.ibm_compute_ssh_key resource. You will need to rename the example.gitlab-ci.yml file to .gitlab-ci.yml . With your changes complete go ahead and commit your code to start the Gitlab CI/CD Pipeline. You can watch the progress of the Pipeline by clicking on the CI/CD left hand navigation link and selecting Pipelines : From there you can view the progress of the Pipeline. Notes Remote States If your deployments will be using a remote state make sure to change terraform init to terraform init -backend-config=\"lock=true\" in the before_script section. Gitlab and SSL If you're using Let's Encrypt generated certificates you may see issues with the certificate not being trusted. To get around this you can add the following to the .gitlab-ci.yml file. variables : GIT_SSL_NO_VERIFY : \"1\" Targetting Gitlab Runners If you need to use specific Gitlab Runners for your deployments you will want to add a tag decleration. For instance is you are targetting runners with the docker tag you would want to add the following to all the Ci/CD stages: tags: - docker Alternative IBM Cloud has recently launched a hosted Terraform offering called Schematics . IBM Cloud Schematics supports all IBM Cloud resources that are provided by the IBM Cloud Provider plug-in for Terraform with the advantage that you don't have to install the Terraform CLI and the IBM Cloud Provider plug-in. You can find some good Schematics example templates here .","title":"Deploying IBM Cloud infrastructure using Terraform and Gitlab"},{"location":"terraform/ibm-gitlab/#overview","text":"Today I will be walking you through how to set up Environmental Variables and a .gitlab-ci.yml file to deploy IBM Cloud resources using Terraform and the Gitlab CI/CD.","title":"Overview"},{"location":"terraform/ibm-gitlab/#setting-cicd-variables-in-gitlab","text":"All of my automated IBM Cloud Terraform projects land under the same Gitlab group . There are many reasons to use Groups in Gitlab but for me it is mainly so that I don't have to set per-project Environmental Variables. Step 1: From the top navigation bar click on Groups and select Your Groups . Step 2: Go to your Gitlab Group page and from the left hand navigation click Settings > CI/CD. Step 3 Click Expand in the Variables section The variables you will want to set are: IC_API_KEY = Your IBM Cloud API Key SL_API_KEY = Your IBM Cloud IaaS (SoftLayer) API Key SL_USERNAME = Your IBM Cloud IaaS (SoftLayer) username Check the Masked option for the variables. Setting the masked variable option means that the value of the variables will be hidden in job logs during the CI/CD runs. When you're variables are set click Save . A Note About Remote States : It is highly recommended to use a remote state for your Terraform deployments. With remote state, Terraform writes the state data to a remote data store, which can then be shared between all members of a team. If you will be using a remote state for your Terraform backend make sure you set the appropriate environmental variables for the backend provider. For instance if you are using the Consul backend provider you would want to set the CONSUL_TOKEN and CONSUL_HTTP_ADDR environmental variables.","title":"Setting CI/CD Variables in Gitlab"},{"location":"terraform/ibm-gitlab/#test-gitlab-automation","text":"In order to test our Gitlab automation let's deploy a single Ubuntu 18 Virtual Instance. The first step is to create new Project in Github. When creating the project click the Import project tab and click on Repo by URL . Under the Git Repository URL section enter https://git.cloud-design.dev/ryan/ibm-tf-gitlab-example.git . Give your newly imported project a name, set it's visibility level and the click Create Project . After a few moments Gitlab will create the new project. Once the project has been created you can dive in to the code to tweak the example deployment. You will need to do the following at the very least: Update the main.tf file with the name of your IaaS SSH Key in the data.ibm_compute_ssh_key resource. You will need to rename the example.gitlab-ci.yml file to .gitlab-ci.yml . With your changes complete go ahead and commit your code to start the Gitlab CI/CD Pipeline. You can watch the progress of the Pipeline by clicking on the CI/CD left hand navigation link and selecting Pipelines : From there you can view the progress of the Pipeline.","title":"Test Gitlab Automation"},{"location":"terraform/ibm-gitlab/#notes","text":"Remote States If your deployments will be using a remote state make sure to change terraform init to terraform init -backend-config=\"lock=true\" in the before_script section. Gitlab and SSL If you're using Let's Encrypt generated certificates you may see issues with the certificate not being trusted. To get around this you can add the following to the .gitlab-ci.yml file. variables : GIT_SSL_NO_VERIFY : \"1\" Targetting Gitlab Runners If you need to use specific Gitlab Runners for your deployments you will want to add a tag decleration. For instance is you are targetting runners with the docker tag you would want to add the following to all the Ci/CD stages: tags: - docker","title":"Notes"},{"location":"terraform/ibm-gitlab/#alternative","text":"IBM Cloud has recently launched a hosted Terraform offering called Schematics . IBM Cloud Schematics supports all IBM Cloud resources that are provided by the IBM Cloud Provider plug-in for Terraform with the advantage that you don't have to install the Terraform CLI and the IBM Cloud Provider plug-in. You can find some good Schematics example templates here .","title":"Alternative"},{"location":"terraform/vpc-consul/","text":"Deploy a Consul cluster to an IBM Cloud VPC using Terraform and Ansible Prerequisites tfswitch installed ansible installed An IBM Cloud API Key Deploy all resources Clone repository: git clone https://github.com/cloud-design-dev/ibm-vpc-consul-terraform-ansible.git cd ibm-vpc-consul-terraform-ansible Copy terraform.tfvars.template to terraform.tfvars : cp terraform.tfvars.template terraform.tfvars Edit terraform.tfvars to match your environment. Run tfswitch to point to the right Terraform version for this solution: tfswitch Deploy all resources: terraform init terraform plan -out default.tfplan terraform apply default.tfplan After the plan completes we will move on to deploying Consul using Ansible. Run Ansible playbook to create the consul cluster cd ansible ansible-playbook -i inventory playbooks/consul-cluster.yml Verify that the cluster is running Since we bound the Consul agent to the main private IP of the VPC instances we first need to set the environmental variable for CONSUL_HTTP_ADDR. Take one of the consul instance IPs and run the following command: ansible -m shell -b -a \"CONSUL_HTTP_ADDR=\\\"http://CONSUL_INSTANCE_IP:8500\\\" consul members\" CONSUL_INSTANCE_NAME -i inventory Example output ansible -m shell -b -a \"CONSUL_HTTP_ADDR=\\\"http://10.241.0.36:8500\\\" consul members\" dev-011534-us-east-1-consul1 -i inventory dev-011534-us-east-1-consul1 | CHANGED | rc = 0 >> Node Address Status Type Build Protocol DC Segment dev-011534-us-east-1-consul1 10 .241.0.36:8301 alive server 1 .9.0 2 us-east <all> dev-011534-us-east-1-consul2 10 .241.0.38:8301 alive server 1 .9.0 2 us-east <all> dev-011534-us-east-1-consul3 10 .241.0.37:8301 alive server 1 .9.0 2 us-east <all> Asciinema recording Diagram","title":"Consul cluster in IBM Cloud VPC using Terraform and Ansible"},{"location":"terraform/vpc-consul/#deploy-a-consul-cluster-to-an-ibm-cloud-vpc-using-terraform-and-ansible","text":"","title":"Deploy a Consul cluster to an IBM Cloud VPC using Terraform and Ansible"},{"location":"terraform/vpc-consul/#prerequisites","text":"tfswitch installed ansible installed An IBM Cloud API Key","title":"Prerequisites"},{"location":"terraform/vpc-consul/#deploy-all-resources","text":"Clone repository: git clone https://github.com/cloud-design-dev/ibm-vpc-consul-terraform-ansible.git cd ibm-vpc-consul-terraform-ansible Copy terraform.tfvars.template to terraform.tfvars : cp terraform.tfvars.template terraform.tfvars Edit terraform.tfvars to match your environment. Run tfswitch to point to the right Terraform version for this solution: tfswitch Deploy all resources: terraform init terraform plan -out default.tfplan terraform apply default.tfplan After the plan completes we will move on to deploying Consul using Ansible.","title":"Deploy all resources"},{"location":"terraform/vpc-consul/#run-ansible-playbook-to-create-the-consul-cluster","text":"cd ansible ansible-playbook -i inventory playbooks/consul-cluster.yml","title":"Run Ansible playbook to create the consul cluster"},{"location":"terraform/vpc-consul/#verify-that-the-cluster-is-running","text":"Since we bound the Consul agent to the main private IP of the VPC instances we first need to set the environmental variable for CONSUL_HTTP_ADDR. Take one of the consul instance IPs and run the following command: ansible -m shell -b -a \"CONSUL_HTTP_ADDR=\\\"http://CONSUL_INSTANCE_IP:8500\\\" consul members\" CONSUL_INSTANCE_NAME -i inventory","title":"Verify that the cluster is running"},{"location":"terraform/vpc-consul/#example-output","text":"ansible -m shell -b -a \"CONSUL_HTTP_ADDR=\\\"http://10.241.0.36:8500\\\" consul members\" dev-011534-us-east-1-consul1 -i inventory dev-011534-us-east-1-consul1 | CHANGED | rc = 0 >> Node Address Status Type Build Protocol DC Segment dev-011534-us-east-1-consul1 10 .241.0.36:8301 alive server 1 .9.0 2 us-east <all> dev-011534-us-east-1-consul2 10 .241.0.38:8301 alive server 1 .9.0 2 us-east <all> dev-011534-us-east-1-consul3 10 .241.0.37:8301 alive server 1 .9.0 2 us-east <all>","title":"Example output"},{"location":"terraform/vpc-consul/#asciinema-recording","text":"","title":"Asciinema recording"},{"location":"terraform/vpc-consul/#diagram","text":"","title":"Diagram"},{"location":"terraform/vpc-wireguard/","text":"Wireguard Server on IBM Cloud VPC The repository will spin up a new VPC on IBM Cloud and configure a Wireguard VPN server as well as additional compute nodes spread across the VPC region. This will allow us to connect to each of the compute instances via our Wireguard server as well as hit the VPC Cloud Service endpoints. Step 1: Install Wireguard Tools If you are on macOS you can use brew to install the Wireguard tools used to generate our client keys. brew install wireguard-tools For most linux distributions you can install via the OS package manager: [ yum/apt-get ] install wireguard-tools Step 2: Generate Wireguard Client Keys and Preshared Key $ wg genkey | tee privatekey | wg pubkey | tee publickey $ wg genpsk | tee presharedkey Step 3: Clone/Fork the repository git clone https://github.com/cloud-design-dev/ibmcloud-vpc-wireguard.git cd ibmcloud-vpc-wireguard Step 4: Update Credentials File The credentials.tfvars will hold all of the sensetive variables that get passed to our installer script: Copy example file cp credentials-example credentials.tfvars The .gitignore file has been configured to ignore any .tfvars files to prevent you from accidently pushing your Wireguard secrets to this repository. Update credentials.tfvars file remote_ssh_ip = \"Your Local IP\" client_private_key = \"Client Private Key generated in Step 2\" client_public_key = \"Client Public Key generated in Step 2\" client_preshared_key = \"Client Preshared Key generated in Step 2\" resource_group = \"Resource Group where you will deploy VPC and resources\" region = \"The IBM Cloud region where you will deploy the VPC and resources\" ssh_key = \"SSH key to add to compute instances\" Step 5: Initialize and Validate Terraform $ terraform init $ terraform validate If validation passes you can now proceed to generating the Terraform plan Step 6: Generate Terraform Plan $ terraform plan -var-file = \"./credentials.tfvars\" -out \"default.tfplan\" If the plan generates successfully you can now run apply to deploy the resources. Step 7: Deploy resources $ terraform apply \"default.tfplan\" After a successful deployment Terraform will generate the local Wireguard configuration needed to connect to our Wireguard instance. The file will saved as ${vpc_name}-wireguard.conf . You will need to update it with the servers Public Key and Preshared Key. Once that is complete you can launch the macOS Wireguard app and import the tunnel. Click Activate to connect to your Wireguard VPC VPN server:","title":"Deploy a Wireguard VPN Server in IBM Cloud VPC using Terraform"},{"location":"terraform/vpc-wireguard/#wireguard-server-on-ibm-cloud-vpc","text":"The repository will spin up a new VPC on IBM Cloud and configure a Wireguard VPN server as well as additional compute nodes spread across the VPC region. This will allow us to connect to each of the compute instances via our Wireguard server as well as hit the VPC Cloud Service endpoints.","title":"Wireguard Server on IBM Cloud VPC"},{"location":"terraform/vpc-wireguard/#step-1-install-wireguard-tools","text":"If you are on macOS you can use brew to install the Wireguard tools used to generate our client keys. brew install wireguard-tools For most linux distributions you can install via the OS package manager: [ yum/apt-get ] install wireguard-tools","title":"Step 1: Install Wireguard Tools"},{"location":"terraform/vpc-wireguard/#step-2-generate-wireguard-client-keys-and-preshared-key","text":"$ wg genkey | tee privatekey | wg pubkey | tee publickey $ wg genpsk | tee presharedkey","title":"Step 2: Generate Wireguard Client Keys and Preshared Key"},{"location":"terraform/vpc-wireguard/#step-3-clonefork-the-repository","text":"git clone https://github.com/cloud-design-dev/ibmcloud-vpc-wireguard.git cd ibmcloud-vpc-wireguard","title":"Step 3: Clone/Fork the repository"},{"location":"terraform/vpc-wireguard/#step-4-update-credentials-file","text":"The credentials.tfvars will hold all of the sensetive variables that get passed to our installer script: Copy example file cp credentials-example credentials.tfvars The .gitignore file has been configured to ignore any .tfvars files to prevent you from accidently pushing your Wireguard secrets to this repository. Update credentials.tfvars file remote_ssh_ip = \"Your Local IP\" client_private_key = \"Client Private Key generated in Step 2\" client_public_key = \"Client Public Key generated in Step 2\" client_preshared_key = \"Client Preshared Key generated in Step 2\" resource_group = \"Resource Group where you will deploy VPC and resources\" region = \"The IBM Cloud region where you will deploy the VPC and resources\" ssh_key = \"SSH key to add to compute instances\"","title":"Step 4: Update Credentials File"},{"location":"terraform/vpc-wireguard/#step-5-initialize-and-validate-terraform","text":"$ terraform init $ terraform validate If validation passes you can now proceed to generating the Terraform plan","title":"Step 5: Initialize and Validate Terraform"},{"location":"terraform/vpc-wireguard/#step-6-generate-terraform-plan","text":"$ terraform plan -var-file = \"./credentials.tfvars\" -out \"default.tfplan\" If the plan generates successfully you can now run apply to deploy the resources.","title":"Step 6: Generate Terraform Plan"},{"location":"terraform/vpc-wireguard/#step-7-deploy-resources","text":"$ terraform apply \"default.tfplan\" After a successful deployment Terraform will generate the local Wireguard configuration needed to connect to our Wireguard instance. The file will saved as ${vpc_name}-wireguard.conf . You will need to update it with the servers Public Key and Preshared Key. Once that is complete you can launch the macOS Wireguard app and import the tunnel. Click Activate to connect to your Wireguard VPC VPN server:","title":"Step 7: Deploy resources"}]}